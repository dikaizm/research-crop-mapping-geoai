{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9289bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/geoai\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: albumentations in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (2.0.8)\n",
      "Requirement already satisfied: buildingregulariser in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.2.4)\n",
      "Requirement already satisfied: contextily in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (1.6.2)\n",
      "Requirement already satisfied: datasets>=3.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (4.2.0)\n",
      "Requirement already satisfied: ever-beta in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.5.3)\n",
      "Requirement already satisfied: geopandas in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (1.1.1)\n",
      "Requirement already satisfied: huggingface_hub in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.35.3)\n",
      "Requirement already satisfied: jupyter-server-proxy in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (4.4.0)\n",
      "Requirement already satisfied: leafmap in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.54.0)\n",
      "Requirement already satisfied: localtileserver in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.10.6)\n",
      "Requirement already satisfied: mapclassify in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (2.10.0)\n",
      "Requirement already satisfied: maplibre in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.3.5)\n",
      "Requirement already satisfied: opencv-python-headless in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (4.12.0.88)\n",
      "Requirement already satisfied: overturemaps in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.17.0)\n",
      "Requirement already satisfied: planetary-computer in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (1.0.0)\n",
      "Requirement already satisfied: psutil in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (7.1.0)\n",
      "Requirement already satisfied: pyarrow in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (21.0.0)\n",
      "Requirement already satisfied: pystac-client in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.9.0)\n",
      "Requirement already satisfied: rasterio in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (1.4.3)\n",
      "Requirement already satisfied: rioxarray in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.19.0)\n",
      "Requirement already satisfied: scikit-image in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.25.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (1.7.2)\n",
      "Requirement already satisfied: timm in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (1.0.20)\n",
      "Requirement already satisfied: torch in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (2.9.0)\n",
      "Requirement already satisfied: torchgeo in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (0.7.1)\n",
      "Requirement already satisfied: torchinfo in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (1.8.0)\n",
      "Requirement already satisfied: tqdm in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (4.67.1)\n",
      "Requirement already satisfied: transformers in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geoai-py==0.17.0) (4.57.1)\n",
      "Requirement already satisfied: filelock in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (2.2.6)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (2025.9.0)\n",
      "Requirement already satisfied: packaging in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from datasets>=3.0->geoai-py==0.17.0) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (3.13.1)\n",
      "Requirement already satisfied: anyio in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=3.0->geoai-py==0.17.0) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=3.0->geoai-py==0.17.0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=3.0->geoai-py==0.17.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets>=3.0->geoai-py==0.17.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0->geoai-py==0.17.0) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from huggingface_hub->geoai-py==0.17.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from huggingface_hub->geoai-py==0.17.0) (1.1.10)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets>=3.0->geoai-py==0.17.0) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.0->geoai-py==0.17.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=3.0->geoai-py==0.17.0) (2.5.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from albumentations->geoai-py==0.17.0) (1.16.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from albumentations->geoai-py==0.17.0) (2.12.3)\n",
      "Requirement already satisfied: albucore==0.0.24 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from albumentations->geoai-py==0.17.0) (0.0.24)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from albucore==0.0.24->albumentations->geoai-py==0.17.0) (4.2.1)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from albucore==0.0.24->albumentations->geoai-py==0.17.0) (6.5.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations->geoai-py==0.17.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations->geoai-py==0.17.0) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from pydantic>=2.9.2->albumentations->geoai-py==0.17.0) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from anyio->httpx<1.0.0->datasets>=3.0->geoai-py==0.17.0) (1.3.1)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geopandas->geoai-py==0.17.0) (0.11.1)\n",
      "Requirement already satisfied: pyproj>=3.5.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geopandas->geoai-py==0.17.0) (3.7.2)\n",
      "Requirement already satisfied: shapely>=2.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geopandas->geoai-py==0.17.0) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from pandas->datasets>=3.0->geoai-py==0.17.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from pandas->datasets>=3.0->geoai-py==0.17.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from pandas->datasets>=3.0->geoai-py==0.17.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0->geoai-py==0.17.0) (1.17.0)\n",
      "Requirement already satisfied: geopy in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from contextily->geoai-py==0.17.0) (2.4.1)\n",
      "Requirement already satisfied: matplotlib in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from contextily->geoai-py==0.17.0) (3.10.7)\n",
      "Requirement already satisfied: mercantile in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from contextily->geoai-py==0.17.0) (1.2.1)\n",
      "Requirement already satisfied: pillow in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from contextily->geoai-py==0.17.0) (12.0.0)\n",
      "Requirement already satisfied: joblib in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from contextily->geoai-py==0.17.0) (1.5.2)\n",
      "Requirement already satisfied: xyzservices in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from contextily->geoai-py==0.17.0) (2025.4.0)\n",
      "Requirement already satisfied: prettytable in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ever-beta->geoai-py==0.17.0) (3.16.0)\n",
      "Requirement already satisfied: tensorboard>=1.14 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ever-beta->geoai-py==0.17.0) (2.20.0)\n",
      "Requirement already satisfied: tifffile in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ever-beta->geoai-py==0.17.0) (2025.10.16)\n",
      "Requirement already satisfied: wandb in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ever-beta->geoai-py==0.17.0) (0.22.2)\n",
      "Requirement already satisfied: einops in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ever-beta->geoai-py==0.17.0) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (1.75.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (3.9)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (6.33.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=1.14->ever-beta->geoai-py==0.17.0) (3.0.3)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from geopy->contextily->geoai-py==0.17.0) (2.1)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server-proxy->geoai-py==0.17.0) (2.17.0)\n",
      "Requirement already satisfied: simpervisor>=1.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server-proxy->geoai-py==0.17.0) (1.0.0)\n",
      "Requirement already satisfied: tornado>=6.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server-proxy->geoai-py==0.17.0) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server-proxy->geoai-py==0.17.0) (5.14.3)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (25.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (3.1.6)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (5.9.1)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.23.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (27.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.9.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (25.1.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (4.5.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (4.25.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (4.0.0)\n",
      "Requirement already satisfied: referencing in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.36.2)\n",
      "Requirement already satisfied: rfc3339-validator in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (2025.9.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.27.1)\n",
      "Requirement already satisfied: fqdn in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.1.0)\n",
      "Requirement already satisfied: uri-template in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (4.14.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (2.19.2)\n",
      "Requirement already satisfied: webencodings in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from nbformat>=5.3.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (2.21.2)\n",
      "Requirement already satisfied: lark>=1.2.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: ptyprocess in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (2.23)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->geoai-py==0.17.0) (2.9.0.20251008)\n",
      "Requirement already satisfied: anywidget in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (0.9.18)\n",
      "Requirement already satisfied: bqplot in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (0.12.45)\n",
      "Requirement already satisfied: duckdb in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (1.4.1)\n",
      "Requirement already satisfied: folium in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (0.20.0)\n",
      "Requirement already satisfied: gdown in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (5.2.0)\n",
      "Requirement already satisfied: geojson in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (3.2.0)\n",
      "Requirement already satisfied: ipyevents in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (2.0.4)\n",
      "Requirement already satisfied: ipyfilechooser in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (0.6.0)\n",
      "Requirement already satisfied: ipyleaflet in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (0.20.0)\n",
      "Requirement already satisfied: ipyvuetify in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (1.11.3)\n",
      "Requirement already satisfied: ipywidgets in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (8.1.7)\n",
      "Requirement already satisfied: plotly in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (6.3.1)\n",
      "Requirement already satisfied: python-box in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (7.3.2)\n",
      "Requirement already satisfied: scooby in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (0.10.2)\n",
      "Requirement already satisfied: whiteboxgui in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from leafmap->geoai-py==0.17.0) (2.3.0)\n",
      "Requirement already satisfied: psygnal>=0.8.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from anywidget->leafmap->geoai-py==0.17.0) (0.15.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipywidgets->leafmap->geoai-py==0.17.0) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipywidgets->leafmap->geoai-py==0.17.0) (9.6.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipywidgets->leafmap->geoai-py==0.17.0) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipywidgets->leafmap->geoai-py==0.17.0) (3.0.15)\n",
      "Requirement already satisfied: decorator in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (3.0.52)\n",
      "Requirement already satisfied: stack_data in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (0.8.5)\n",
      "Requirement already satisfied: traittypes>=0.0.6 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from bqplot->leafmap->geoai-py==0.17.0) (0.2.1)\n",
      "Requirement already satisfied: branca>=0.6.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from folium->leafmap->geoai-py==0.17.0) (0.8.2)\n",
      "Requirement already satisfied: jupyter-leaflet<0.21,>=0.20 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipyleaflet->leafmap->geoai-py==0.17.0) (0.20.0)\n",
      "Requirement already satisfied: ipyvue<2,>=1.7 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from ipyvuetify->leafmap->geoai-py==0.17.0) (1.11.3)\n",
      "Requirement already satisfied: click in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (8.3.0)\n",
      "Requirement already satisfied: flask<4,>=2.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (3.1.2)\n",
      "Requirement already satisfied: Flask-Caching in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (2.3.1)\n",
      "Requirement already satisfied: flask-cors in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (6.0.1)\n",
      "Requirement already satisfied: flask-restx>=1.3.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (1.3.2)\n",
      "Requirement already satisfied: rio-tiler in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (7.9.2)\n",
      "Requirement already satisfied: rio-cogeo in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (5.4.2)\n",
      "Requirement already satisfied: server-thread in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from localtileserver->geoai-py==0.17.0) (0.3.0)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from flask<4,>=2.0.0->localtileserver->geoai-py==0.17.0) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from flask<4,>=2.0.0->localtileserver->geoai-py==0.17.0) (2.2.0)\n",
      "Requirement already satisfied: aniso8601>=0.82 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from flask-restx>=1.3.0->localtileserver->geoai-py==0.17.0) (10.0.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from flask-restx>=1.3.0->localtileserver->geoai-py==0.17.0) (6.5.2)\n",
      "Requirement already satisfied: cachelib>=0.9.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from Flask-Caching->localtileserver->geoai-py==0.17.0) (0.13.0)\n",
      "Requirement already satisfied: networkx>=3.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from mapclassify->geoai-py==0.17.0) (3.5)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from scikit-learn->geoai-py==0.17.0) (3.6.0)\n",
      "Requirement already satisfied: eval-type-backport in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from maplibre->geoai-py==0.17.0) (0.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from matplotlib->contextily->geoai-py==0.17.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from matplotlib->contextily->geoai-py==0.17.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from matplotlib->contextily->geoai-py==0.17.0) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from matplotlib->contextily->geoai-py==0.17.0) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from matplotlib->contextily->geoai-py==0.17.0) (3.2.5)\n",
      "Requirement already satisfied: pystac>=1.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from planetary-computer->geoai-py==0.17.0) (1.14.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from planetary-computer->geoai-py==0.17.0) (1.1.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from plotly->leafmap->geoai-py==0.17.0) (2.8.0)\n",
      "Requirement already satisfied: affine in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rasterio->geoai-py==0.17.0) (2.4.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rasterio->geoai-py==0.17.0) (0.7.2)\n",
      "Requirement already satisfied: click-plugins in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rasterio->geoai-py==0.17.0) (1.1.1.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from requests[socks]->gdown->leafmap->geoai-py==0.17.0) (1.7.1)\n",
      "Requirement already satisfied: morecantile<7.0,>=5.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rio-cogeo->localtileserver->geoai-py==0.17.0) (6.2.0)\n",
      "Requirement already satisfied: cachetools in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rio-tiler->localtileserver->geoai-py==0.17.0) (6.2.1)\n",
      "Requirement already satisfied: color-operations in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rio-tiler->localtileserver->geoai-py==0.17.0) (0.2.0)\n",
      "Requirement already satisfied: numexpr in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rio-tiler->localtileserver->geoai-py==0.17.0) (2.14.1)\n",
      "Requirement already satisfied: xarray>=2024.7.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rioxarray->geoai-py==0.17.0) (2025.10.1)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from scikit-image->geoai-py==0.17.0) (2.37.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from scikit-image->geoai-py==0.17.0) (0.4)\n",
      "Requirement already satisfied: uvicorn in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from server-thread->localtileserver->geoai-py==0.17.0) (0.37.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets->leafmap->geoai-py==0.17.0) (0.2.3)\n",
      "Requirement already satisfied: torchvision in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from timm->geoai-py==0.17.0) (0.24.0)\n",
      "Requirement already satisfied: safetensors in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from timm->geoai-py==0.17.0) (0.6.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from torch->geoai-py==0.17.0) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch->geoai-py==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: fiona>=1.8.22 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from torchgeo->geoai-py==0.17.0) (1.10.1)\n",
      "Requirement already satisfied: kornia>=0.7.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from torchgeo->geoai-py==0.17.0) (0.8.1)\n",
      "Requirement already satisfied: lightly!=1.4.26,>=1.4.5 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from torchgeo->geoai-py==0.17.0) (1.5.22)\n",
      "Requirement already satisfied: lightning!=2.3.*,!=2.5.0,>=2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (2.5.5)\n",
      "Requirement already satisfied: rtree>=1.0.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from torchgeo->geoai-py==0.17.0) (1.4.1)\n",
      "Requirement already satisfied: segmentation-models-pytorch>=0.5 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from torchgeo->geoai-py==0.17.0) (0.5.0)\n",
      "Requirement already satisfied: torchmetrics>=1.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from torchgeo->geoai-py==0.17.0) (1.8.2)\n",
      "Requirement already satisfied: kornia_rs>=0.1.9 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from kornia>=0.7.4->torchgeo->geoai-py==0.17.0) (0.1.9)\n",
      "Requirement already satisfied: hydra-core>=1.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->geoai-py==0.17.0) (1.3.2)\n",
      "Requirement already satisfied: lightly_utils~=0.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->geoai-py==0.17.0) (0.0.2)\n",
      "Requirement already satisfied: pytorch_lightning>=1.0.4 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->geoai-py==0.17.0) (2.5.5)\n",
      "Requirement already satisfied: aenum>=3.1.11 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->geoai-py==0.17.0) (3.1.16)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo->geoai-py==0.17.0) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo->geoai-py==0.17.0) (4.9.3)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (0.15.2)\n",
      "Requirement already satisfied: jsonargparse<5.0,>=4.39.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (4.42.0)\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (14.2.0)\n",
      "Requirement already satisfied: tensorboardX<3.0,>=2.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (2.6.4)\n",
      "Requirement already satisfied: docstring-parser>=0.17 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (0.17.0)\n",
      "Requirement already satisfied: typeshed-client>=2.8.2 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (2.8.2)\n",
      "Requirement already satisfied: jsonnet>=0.21.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from jsonargparse[jsonnet,signatures]<5.0,>=4.39.0; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (0.21.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from rich<15.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo->geoai-py==0.17.0) (0.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from transformers->geoai-py==0.17.0) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from transformers->geoai-py==0.17.0) (0.22.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from wandb->ever-beta->geoai-py==0.17.0) (3.1.45)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from wandb->ever-beta->geoai-py==0.17.0) (2.42.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->ever-beta->geoai-py==0.17.0) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->ever-beta->geoai-py==0.17.0) (5.0.2)\n",
      "Requirement already satisfied: ipytree in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from whiteboxgui->leafmap->geoai-py==0.17.0) (0.2.2)\n",
      "Requirement already satisfied: whitebox in /Users/dikaizm/Documents/PROGRAMMING/ml-ai/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages (from whiteboxgui->leafmap->geoai-py==0.17.0) (2.3.6)\n",
      "Building wheels for collected packages: geoai-py\n",
      "  Building editable for geoai-py (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for geoai-py: filename=geoai_py-0.17.0-0.editable-py2.py3-none-any.whl size=8024 sha256=fcbe619534400d2c1c3aeb7bfdea32ef502cba6aaf4b13689fc45b11cf195da5\n",
      "  Stored in directory: /private/var/folders/hb/b1p7kpgj4s759vkptsfqzxzw0000gn/T/pip-ephem-wheel-cache-u2h9fdiu/wheels/96/83/71/07d484e50b706fb043707954e5f280a67453f1002a83a09468\n",
      "Successfully built geoai-py\n",
      "Installing collected packages: geoai-py\n",
      "  Attempting uninstall: geoai-py\n",
      "    Found existing installation: geoai-py 0.17.0\n",
      "    Uninstalling geoai-py-0.17.0:\n",
      "      Successfully uninstalled geoai-py-0.17.0\n",
      "Successfully installed geoai-py-0.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ../../geoai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84684855",
   "metadata": {},
   "source": [
    "### Reload Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c623d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'geoai.utils' from '/root/geoai/geoai/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geoai\n",
    "import geoai.label_utils\n",
    "import geoai.utils\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import importlib\n",
    "importlib.reload(geoai)\n",
    "importlib.reload(geoai.label_utils)\n",
    "importlib.reload(geoai.utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e4f3e",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b675b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1PF7umiS2qCj5yLJHGLmK1YQvQA9eCv3k\n",
      "From (redirected): https://drive.google.com/uc?id=1PF7umiS2qCj5yLJHGLmK1YQvQA9eCv3k&confirm=t&uuid=1abfe5c7-eb7e-4789-9193-03c90c2dea44\n",
      "To: /root/research-crop-mapping-geoai/data/raw/images/sacramento_2/S2H_2023_2023_06_30.tif\n",
      "100%|| 5.03G/5.03G [00:54<00:00, 92.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded: ../data/raw/images/sacramento_2/S2H_2023_2023_06_30.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1IshwrdrgQrtdPBVu-DXGHnuhev_hK-Zc\n",
      "To: /root/research-crop-mapping-geoai/data/raw/cdl/sacramento_2/2023_30m_cdls_7c_10m_remap.tif\n",
      "100%|| 4.56M/4.56M [00:00<00:00, 12.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloaded: ../data/raw/cdl/sacramento_2/2023_30m_cdls_7c_10m_remap.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../data/raw/cdl/sacramento_2/2023_30m_cdls_7c_10m_remap.tif'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import general\n",
    "importlib.reload(general)\n",
    "\n",
    "# Download s2\n",
    "file_id = \"1PF7umiS2qCj5yLJHGLmK1YQvQA9eCv3k\"\n",
    "general.download_from_gdrive(file_id, output_path=\"../data/raw/images/sacramento_2/S2H_2023_2023_06_30.tif\")\n",
    "\n",
    "# Download cdl\n",
    "file_id = \"1IshwrdrgQrtdPBVu-DXGHnuhev_hK-Zc\"\n",
    "general.download_from_gdrive(file_id, output_path=\"../data/raw/cdl/sacramento_2/2023_30m_cdls_7c_10m_remap.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e252c",
   "metadata": {},
   "source": [
    "## Train single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c20ad",
   "metadata": {},
   "source": [
    "### Create subset bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "523f2e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/raw/images/sacramento_2/S2H_2023_2023_06_30_subset[10,11,5,4,6].tif\n"
     ]
    }
   ],
   "source": [
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "import numpy as np\n",
    "\n",
    "s2_path = \"../data/raw/images/sacramento_2/S2H_2023_2023_06_30.tif\"\n",
    "\n",
    "subset = [10, 11, 5, 4, 6]  # Bands to extract\n",
    "\n",
    "out_path = \"../data/raw/images/sacramento_2/S2H_2023_2023_06_30_subset[10,11,5,4,6].tif\"\n",
    "\n",
    "with rasterio.open(s2_path) as src:\n",
    "    profile = src.profile.copy()\n",
    "    profile.update(count=len(subset), dtype='float64')\n",
    "    with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "        for i, b in enumerate(subset, start=1):\n",
    "            dst.write(src.read(b).astype(\"float64\"), i)\n",
    "\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51114b2",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066c1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected in_class_data as raster: ../data/raw/cdl/sacramento_2/2023_30m_cdls_7c_10m_remap.tif\n",
      "Raster CRS: EPSG:4326\n",
      "Raster dimensions: 7614 x 7503\n",
      "\n",
      "Raster info for ../data/raw/images/sacramento_2/S2H_2023_2023_06_30_subset[10,11,5,4,6].tif:\n",
      "  CRS: EPSG:4326\n",
      "  Dimensions: 7614 x 7503\n",
      "  Resolution: (8.983152850013118e-05, 8.983152832200529e-05)\n",
      "  Bands: 5\n",
      "  Bounds: BoundingBox(left=-122.073052106, bottom=38.494696387, right=-121.389074848, top=39.168702344)\n",
      "Found 7 unique classes in raster: [1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated: 9, With features: 9:   0%|          | 9/8556 [00:00<07:46, 18.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated: 8427, With features: 8427: 100%|| 8556/8556 [01:36<00:00, 89.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Export Summary -------\n",
      "Total tiles exported: 8427\n",
      "Tiles with features: 8427 (100.0%)\n",
      "Average feature pixels per tile: 67554.2\n",
      "Output saved to: ../data/processed/single_7c_2023_06_30_subset[10,11,5,4,6]\n",
      "\n",
      "------- Georeference Verification -------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_path = \"../data/raw/images/sacramento_2/S2H_2023_2023_06_30_subset[10,11,5,4,6].tif\"\n",
    "label_path = \"../data/raw/cdl/sacramento_2/2023_30m_cdls_7c_10m_remap.tif\"\n",
    "\n",
    "out_name = \"single_7c_2023_06_30_subset[10,11,5,4,6]\"\n",
    "data_folder = f\"../data/processed/{out_name}\"\n",
    "\n",
    "model_folder = f\"../models/{out_name}\"\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "dataloader_params = {\n",
    "    \"tile_size\": 256,\n",
    "    \"stride\": 80,\n",
    "    \"skip_empty_tiles\": True,\n",
    "    \"stats\": None\n",
    "}\n",
    "\n",
    "# this function will create folders (annotations, images, labels)\n",
    "tiles = geoai.export_geotiff_tiles(\n",
    "    in_raster=image_path,\n",
    "    out_folder=data_folder,\n",
    "    in_class_data=label_path,\n",
    "    tile_size=dataloader_params[\"tile_size\"],\n",
    "    stride=dataloader_params[\"stride\"],\n",
    "    buffer_radius=0,\n",
    "    skip_empty_tiles=dataloader_params[\"skip_empty_tiles\"],\n",
    ")\n",
    "\n",
    "dataloader_params[\"stats\"] = tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76dade",
   "metadata": {},
   "source": [
    "### Train DeepLabV3+ [oversampling]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32560d6",
   "metadata": {},
   "source": [
    "#### Setup loss criterios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7964bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from geoai import losses\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "# print(\"Using device:\", device)\n",
    "\n",
    "# criterion = losses.DECBLoss(\n",
    "#     num_classes=11,\n",
    "#     beta=0.9999,          # smoothing factor (as in paper)\n",
    "#     loss_type=\"ce\",       # or \"focal\"\n",
    "#     gamma=2.0,            # focal gamma\n",
    "#     ignore_index=255,\n",
    "#     device=device\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f778e71",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a1ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"https://mlflow.stelarea.com\")\n",
    "mlflow.set_experiment(\"research-crop-mapping\")\n",
    "\n",
    "model_params = {\n",
    "    \"images_dir\": f\"{data_folder}/images\",\n",
    "    \"labels_dir\": f\"{data_folder}/labels\",\n",
    "    \"output_dir\": f\"{model_folder}/deeplabv3plus_efb5\",\n",
    "    \"architecture\": \"deeplabv3plus\",\n",
    "    \"encoder_name\": \"efficientnet-b5\",\n",
    "    \"encoder_weights\": None,\n",
    "    \"num_channels\": 5,\n",
    "    \"num_classes\": 8, # main classes + background\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 100,\n",
    "    \"learning_rate\": 0.0005,\n",
    "    \"val_split\": 0.2,\n",
    "    \"class_balanced\": True,\n",
    "    \"checkpoint_interval\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85787be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 8427 image files and 8427 label files\n",
      "Training on 6741 images, validating on 1686 images\n",
      "Checking image sizes for compatibility...\n",
      "All sampled images have the same size: (256, 256)\n",
      "No resizing needed.\n",
      "Building class-balanced sampler (strategy='presence') ...\n",
      "Class-balanced sampler ready. Non-empty classes: 8/8.\n",
      "Class balanced sampler: <torch.utils.data.sampler.WeightedRandomSampler object at 0x75d5746736d0>\n",
      "Using class balanced sampler\n",
      "Testing data loader...\n",
      "Data loader test passed.\n",
      "Starting training with deeplabv3plus + efficientnet-b5\n",
      "Model parameters: 29,495,080\n",
      "Epoch: 1, Batch: 1/842, Loss: 2.0816, Time: 2.71s\n",
      "Epoch: 1, Batch: 11/842, Loss: 1.5798, Time: 0.80s\n",
      "Epoch: 1, Batch: 21/842, Loss: 1.2806, Time: 0.72s\n",
      "Epoch: 1, Batch: 31/842, Loss: 1.2461, Time: 0.75s\n",
      "Epoch: 1, Batch: 41/842, Loss: 1.4165, Time: 0.74s\n",
      "Epoch: 1, Batch: 51/842, Loss: 1.2987, Time: 0.75s\n",
      "Epoch: 1, Batch: 61/842, Loss: 1.1725, Time: 0.77s\n",
      "Epoch: 1, Batch: 71/842, Loss: 1.5282, Time: 0.76s\n",
      "Epoch: 1, Batch: 81/842, Loss: 1.1403, Time: 0.77s\n",
      "Epoch: 1, Batch: 91/842, Loss: 1.7475, Time: 0.77s\n",
      "Epoch: 1, Batch: 101/842, Loss: 1.1513, Time: 0.75s\n",
      "Epoch: 1, Batch: 111/842, Loss: 1.1040, Time: 0.77s\n",
      "Epoch: 1, Batch: 121/842, Loss: 1.2443, Time: 0.82s\n",
      "Epoch: 1, Batch: 131/842, Loss: 1.7079, Time: 0.77s\n",
      "Epoch: 1, Batch: 141/842, Loss: 1.2087, Time: 0.75s\n",
      "Epoch: 1, Batch: 151/842, Loss: 0.8822, Time: 0.81s\n",
      "Epoch: 1, Batch: 161/842, Loss: 1.1949, Time: 0.80s\n",
      "Epoch: 1, Batch: 171/842, Loss: 0.9878, Time: 0.83s\n",
      "Epoch: 1, Batch: 181/842, Loss: 1.3432, Time: 0.74s\n",
      "Epoch: 1, Batch: 191/842, Loss: 0.9920, Time: 0.77s\n",
      "Epoch: 1, Batch: 201/842, Loss: 1.4135, Time: 0.78s\n",
      "Epoch: 1, Batch: 211/842, Loss: 1.1708, Time: 0.78s\n",
      "Epoch: 1, Batch: 221/842, Loss: 0.8609, Time: 0.76s\n",
      "Epoch: 1, Batch: 231/842, Loss: 1.0786, Time: 0.76s\n",
      "Epoch: 1, Batch: 241/842, Loss: 1.2229, Time: 0.76s\n",
      "Epoch: 1, Batch: 251/842, Loss: 1.0249, Time: 0.77s\n",
      "Epoch: 1, Batch: 261/842, Loss: 0.9658, Time: 0.77s\n",
      "Epoch: 1, Batch: 271/842, Loss: 0.7771, Time: 0.76s\n",
      "Epoch: 1, Batch: 281/842, Loss: 1.1428, Time: 0.77s\n",
      "Epoch: 1, Batch: 291/842, Loss: 0.9733, Time: 0.76s\n",
      "Epoch: 1, Batch: 301/842, Loss: 0.8964, Time: 0.76s\n",
      "Epoch: 1, Batch: 311/842, Loss: 1.0237, Time: 0.76s\n",
      "Epoch: 1, Batch: 321/842, Loss: 0.9607, Time: 0.77s\n",
      "Epoch: 1, Batch: 331/842, Loss: 0.6969, Time: 0.78s\n",
      "Epoch: 1, Batch: 341/842, Loss: 0.8161, Time: 0.81s\n",
      "Epoch: 1, Batch: 351/842, Loss: 0.9658, Time: 0.81s\n",
      "Epoch: 1, Batch: 361/842, Loss: 1.1267, Time: 0.81s\n",
      "Epoch: 1, Batch: 371/842, Loss: 0.7649, Time: 0.84s\n",
      "Epoch: 1, Batch: 381/842, Loss: 0.9737, Time: 0.84s\n",
      "Epoch: 1, Batch: 391/842, Loss: 1.0546, Time: 0.85s\n",
      "Epoch: 1, Batch: 401/842, Loss: 0.8622, Time: 0.89s\n",
      "Epoch: 1, Batch: 411/842, Loss: 0.8467, Time: 0.88s\n",
      "Epoch: 1, Batch: 421/842, Loss: 0.8545, Time: 0.81s\n",
      "Epoch: 1, Batch: 431/842, Loss: 0.9443, Time: 0.82s\n",
      "Epoch: 1, Batch: 441/842, Loss: 0.9232, Time: 0.79s\n",
      "Epoch: 1, Batch: 451/842, Loss: 0.8071, Time: 0.76s\n",
      "Epoch: 1, Batch: 461/842, Loss: 1.0686, Time: 0.78s\n",
      "Epoch: 1, Batch: 471/842, Loss: 0.7115, Time: 0.79s\n",
      "Epoch: 1, Batch: 481/842, Loss: 0.8094, Time: 0.76s\n",
      "Epoch: 1, Batch: 491/842, Loss: 0.6860, Time: 0.77s\n",
      "Epoch: 1, Batch: 501/842, Loss: 0.7301, Time: 0.76s\n",
      "Epoch: 1, Batch: 511/842, Loss: 0.7221, Time: 0.73s\n",
      "Epoch: 1, Batch: 521/842, Loss: 0.8917, Time: 0.73s\n",
      "Epoch: 1, Batch: 531/842, Loss: 1.1589, Time: 0.79s\n",
      "Epoch: 1, Batch: 541/842, Loss: 0.9040, Time: 0.78s\n",
      "Epoch: 1, Batch: 551/842, Loss: 1.1059, Time: 0.81s\n",
      "Epoch: 1, Batch: 561/842, Loss: 0.8384, Time: 0.80s\n",
      "Epoch: 1, Batch: 571/842, Loss: 0.9302, Time: 0.76s\n",
      "Epoch: 1, Batch: 581/842, Loss: 0.9512, Time: 0.79s\n",
      "Epoch: 1, Batch: 591/842, Loss: 0.9900, Time: 0.77s\n",
      "Epoch: 1, Batch: 601/842, Loss: 0.7346, Time: 0.80s\n",
      "Epoch: 1, Batch: 611/842, Loss: 0.7815, Time: 0.79s\n",
      "Epoch: 1, Batch: 621/842, Loss: 0.8093, Time: 0.76s\n",
      "Epoch: 1, Batch: 631/842, Loss: 0.9956, Time: 0.79s\n",
      "Epoch: 1, Batch: 641/842, Loss: 0.9347, Time: 0.84s\n",
      "Epoch: 1, Batch: 651/842, Loss: 1.2548, Time: 0.80s\n",
      "Epoch: 1, Batch: 661/842, Loss: 0.8639, Time: 0.78s\n",
      "Epoch: 1, Batch: 671/842, Loss: 0.7579, Time: 0.75s\n",
      "Epoch: 1, Batch: 681/842, Loss: 0.8446, Time: 0.80s\n",
      "Epoch: 1, Batch: 691/842, Loss: 0.9013, Time: 0.86s\n",
      "Epoch: 1, Batch: 701/842, Loss: 0.7600, Time: 0.85s\n",
      "Epoch: 1, Batch: 711/842, Loss: 0.8965, Time: 0.76s\n",
      "Epoch: 1, Batch: 721/842, Loss: 0.5522, Time: 0.76s\n",
      "Epoch: 1, Batch: 731/842, Loss: 0.8016, Time: 0.77s\n",
      "Epoch: 1, Batch: 741/842, Loss: 0.6922, Time: 0.76s\n",
      "Epoch: 1, Batch: 751/842, Loss: 0.7273, Time: 0.71s\n",
      "Epoch: 1, Batch: 761/842, Loss: 0.8084, Time: 0.73s\n",
      "Epoch: 1, Batch: 771/842, Loss: 0.7614, Time: 0.74s\n",
      "Epoch: 1, Batch: 781/842, Loss: 1.0274, Time: 0.78s\n",
      "Epoch: 1, Batch: 791/842, Loss: 0.9154, Time: 0.77s\n",
      "Epoch: 1, Batch: 801/842, Loss: 0.9121, Time: 0.78s\n",
      "Epoch: 1, Batch: 811/842, Loss: 0.9602, Time: 0.78s\n",
      "Epoch: 1, Batch: 821/842, Loss: 0.8048, Time: 0.75s\n",
      "Epoch: 1, Batch: 831/842, Loss: 0.6954, Time: 0.84s\n",
      "Epoch: 1, Batch: 841/842, Loss: 0.9703, Time: 0.76s\n",
      "Epoch 1/100: Train Loss: 0.9955, Val Loss: 0.9137, mIoU: 0.3554, F1: 0.4931, OA: 0.9239\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.6658, F1=0.7994, OA=0.7906, Precision=0.7426, Recall=0.8656\n",
      "    Class 1: IoU=0.1413, F1=0.2476, OA=0.9877, Precision=0.3537, Recall=0.1905\n",
      "    Class 2: IoU=0.5897, F1=0.7419, OA=0.9142, Precision=0.9500, Recall=0.6086\n",
      "    Class 3: IoU=0.3103, F1=0.4737, OA=0.9520, Precision=0.7111, Recall=0.3551\n",
      "    Class 4: IoU=0.4507, F1=0.6214, OA=0.9338, Precision=0.5093, Recall=0.7968\n",
      "    Class 5: IoU=0.0656, F1=0.1232, OA=0.9915, Precision=0.7337, Recall=0.0672\n",
      "    Class 6: IoU=0.2409, F1=0.3883, OA=0.8876, Precision=0.3675, Recall=0.4117\n",
      "    Class 7: IoU=0.3790, F1=0.5497, OA=0.9336, Precision=0.6007, Recall=0.5067\n",
      "Saving best model with mIoU: 0.3554\n",
      "Epoch: 2, Batch: 1/842, Loss: 0.9528, Time: 1.65s\n",
      "Epoch: 2, Batch: 11/842, Loss: 0.9491, Time: 0.83s\n",
      "Epoch: 2, Batch: 21/842, Loss: 0.5767, Time: 0.80s\n",
      "Epoch: 2, Batch: 31/842, Loss: 0.6837, Time: 0.76s\n",
      "Epoch: 2, Batch: 41/842, Loss: 0.8955, Time: 0.82s\n",
      "Epoch: 2, Batch: 51/842, Loss: 0.8171, Time: 0.80s\n",
      "Epoch: 2, Batch: 61/842, Loss: 0.8262, Time: 0.82s\n",
      "Epoch: 2, Batch: 71/842, Loss: 0.9269, Time: 0.79s\n",
      "Epoch: 2, Batch: 81/842, Loss: 0.9306, Time: 0.77s\n",
      "Epoch: 2, Batch: 91/842, Loss: 0.7709, Time: 0.80s\n",
      "Epoch: 2, Batch: 101/842, Loss: 0.9218, Time: 0.76s\n",
      "Epoch: 2, Batch: 111/842, Loss: 0.8099, Time: 0.78s\n",
      "Epoch: 2, Batch: 121/842, Loss: 0.9686, Time: 0.77s\n",
      "Epoch: 2, Batch: 131/842, Loss: 0.9119, Time: 0.76s\n",
      "Epoch: 2, Batch: 141/842, Loss: 0.7771, Time: 0.81s\n",
      "Epoch: 2, Batch: 151/842, Loss: 0.7796, Time: 0.86s\n",
      "Epoch: 2, Batch: 161/842, Loss: 0.8290, Time: 0.80s\n",
      "Epoch: 2, Batch: 171/842, Loss: 0.7740, Time: 0.76s\n",
      "Epoch: 2, Batch: 181/842, Loss: 0.8590, Time: 0.73s\n",
      "Epoch: 2, Batch: 191/842, Loss: 0.7964, Time: 0.75s\n",
      "Epoch: 2, Batch: 201/842, Loss: 0.8838, Time: 0.80s\n",
      "Epoch: 2, Batch: 211/842, Loss: 0.7133, Time: 0.83s\n",
      "Epoch: 2, Batch: 221/842, Loss: 0.7365, Time: 0.89s\n",
      "Epoch: 2, Batch: 231/842, Loss: 0.8902, Time: 0.79s\n",
      "Epoch: 2, Batch: 241/842, Loss: 0.5881, Time: 0.77s\n",
      "Epoch: 2, Batch: 251/842, Loss: 0.8352, Time: 0.77s\n",
      "Epoch: 2, Batch: 261/842, Loss: 0.9080, Time: 0.74s\n",
      "Epoch: 2, Batch: 271/842, Loss: 0.5665, Time: 0.74s\n",
      "Epoch: 2, Batch: 281/842, Loss: 1.0843, Time: 0.72s\n",
      "Epoch: 2, Batch: 291/842, Loss: 0.8026, Time: 0.72s\n",
      "Epoch: 2, Batch: 301/842, Loss: 0.7900, Time: 0.72s\n",
      "Epoch: 2, Batch: 311/842, Loss: 0.6155, Time: 0.72s\n",
      "Epoch: 2, Batch: 321/842, Loss: 0.6834, Time: 0.72s\n",
      "Epoch: 2, Batch: 331/842, Loss: 0.5970, Time: 0.75s\n",
      "Epoch: 2, Batch: 341/842, Loss: 0.7886, Time: 0.75s\n",
      "Epoch: 2, Batch: 351/842, Loss: 0.8571, Time: 0.73s\n",
      "Epoch: 2, Batch: 361/842, Loss: 1.2098, Time: 0.77s\n",
      "Epoch: 2, Batch: 371/842, Loss: 0.7956, Time: 0.75s\n",
      "Epoch: 2, Batch: 381/842, Loss: 1.0607, Time: 0.72s\n",
      "Epoch: 2, Batch: 391/842, Loss: 0.8513, Time: 0.73s\n",
      "Epoch: 2, Batch: 401/842, Loss: 1.0989, Time: 0.74s\n",
      "Epoch: 2, Batch: 411/842, Loss: 1.2761, Time: 0.73s\n",
      "Epoch: 2, Batch: 421/842, Loss: 0.7746, Time: 0.72s\n",
      "Epoch: 2, Batch: 431/842, Loss: 0.8009, Time: 0.72s\n",
      "Epoch: 2, Batch: 441/842, Loss: 0.7527, Time: 0.73s\n",
      "Epoch: 2, Batch: 451/842, Loss: 0.7317, Time: 0.77s\n",
      "Epoch: 2, Batch: 461/842, Loss: 0.8557, Time: 0.77s\n",
      "Epoch: 2, Batch: 471/842, Loss: 0.7004, Time: 0.77s\n",
      "Epoch: 2, Batch: 481/842, Loss: 0.6576, Time: 0.79s\n",
      "Epoch: 2, Batch: 491/842, Loss: 0.8365, Time: 0.81s\n",
      "Epoch: 2, Batch: 501/842, Loss: 0.9211, Time: 0.83s\n",
      "Epoch: 2, Batch: 511/842, Loss: 0.7006, Time: 0.80s\n",
      "Epoch: 2, Batch: 521/842, Loss: 0.8883, Time: 0.75s\n",
      "Epoch: 2, Batch: 531/842, Loss: 0.8879, Time: 0.69s\n",
      "Epoch: 2, Batch: 541/842, Loss: 0.7074, Time: 0.76s\n",
      "Epoch: 2, Batch: 551/842, Loss: 0.9343, Time: 0.78s\n",
      "Epoch: 2, Batch: 561/842, Loss: 0.6876, Time: 0.74s\n",
      "Epoch: 2, Batch: 571/842, Loss: 0.8945, Time: 0.72s\n",
      "Epoch: 2, Batch: 581/842, Loss: 0.7606, Time: 0.72s\n",
      "Epoch: 2, Batch: 591/842, Loss: 1.0109, Time: 0.73s\n",
      "Epoch: 2, Batch: 601/842, Loss: 0.8512, Time: 0.72s\n",
      "Epoch: 2, Batch: 611/842, Loss: 0.9428, Time: 0.72s\n",
      "Epoch: 2, Batch: 621/842, Loss: 0.6291, Time: 0.72s\n",
      "Epoch: 2, Batch: 631/842, Loss: 0.5768, Time: 0.72s\n",
      "Epoch: 2, Batch: 641/842, Loss: 0.8721, Time: 0.74s\n",
      "Epoch: 2, Batch: 651/842, Loss: 0.6402, Time: 0.74s\n",
      "Epoch: 2, Batch: 661/842, Loss: 1.4055, Time: 0.73s\n",
      "Epoch: 2, Batch: 671/842, Loss: 0.6206, Time: 0.72s\n",
      "Epoch: 2, Batch: 681/842, Loss: 0.9517, Time: 0.73s\n",
      "Epoch: 2, Batch: 691/842, Loss: 0.7820, Time: 0.73s\n",
      "Epoch: 2, Batch: 701/842, Loss: 0.6661, Time: 0.73s\n",
      "Epoch: 2, Batch: 711/842, Loss: 0.6741, Time: 0.72s\n",
      "Epoch: 2, Batch: 721/842, Loss: 0.6702, Time: 0.71s\n",
      "Epoch: 2, Batch: 731/842, Loss: 0.6961, Time: 0.71s\n",
      "Epoch: 2, Batch: 741/842, Loss: 0.6156, Time: 0.71s\n",
      "Epoch: 2, Batch: 751/842, Loss: 0.6329, Time: 0.71s\n",
      "Epoch: 2, Batch: 761/842, Loss: 0.8482, Time: 0.72s\n",
      "Epoch: 2, Batch: 771/842, Loss: 0.5272, Time: 0.75s\n",
      "Epoch: 2, Batch: 781/842, Loss: 0.6777, Time: 0.78s\n",
      "Epoch: 2, Batch: 791/842, Loss: 0.7444, Time: 0.74s\n",
      "Epoch: 2, Batch: 801/842, Loss: 0.8012, Time: 0.75s\n",
      "Epoch: 2, Batch: 811/842, Loss: 0.8276, Time: 0.73s\n",
      "Epoch: 2, Batch: 821/842, Loss: 0.7030, Time: 0.76s\n",
      "Epoch: 2, Batch: 831/842, Loss: 0.9027, Time: 0.73s\n",
      "Epoch: 2, Batch: 841/842, Loss: 0.7280, Time: 0.71s\n",
      "Epoch 2/100: Train Loss: 0.7917, Val Loss: 0.7060, mIoU: 0.4611, F1: 0.6093, OA: 0.9409\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.6924, F1=0.8182, OA=0.8118, Precision=0.7655, Recall=0.8789\n",
      "    Class 1: IoU=0.3753, F1=0.5457, OA=0.9931, Precision=0.8996, Recall=0.3917\n",
      "    Class 2: IoU=0.8418, F1=0.9141, OA=0.9646, Precision=0.8988, Recall=0.9300\n",
      "    Class 3: IoU=0.3355, F1=0.5025, OA=0.9532, Precision=0.7114, Recall=0.3884\n",
      "    Class 4: IoU=0.4898, F1=0.6575, OA=0.9633, Precision=0.9040, Recall=0.5167\n",
      "    Class 5: IoU=0.2252, F1=0.3675, OA=0.9921, Precision=0.6443, Recall=0.2571\n",
      "    Class 6: IoU=0.3589, F1=0.5282, OA=0.9044, Precision=0.4616, Recall=0.6172\n",
      "    Class 7: IoU=0.3703, F1=0.5405, OA=0.9446, Precision=0.8044, Recall=0.4069\n",
      "Saving best model with mIoU: 0.4611\n",
      "Epoch: 3, Batch: 1/842, Loss: 0.9777, Time: 1.68s\n",
      "Epoch: 3, Batch: 11/842, Loss: 0.6187, Time: 0.75s\n",
      "Epoch: 3, Batch: 21/842, Loss: 0.7493, Time: 0.72s\n",
      "Epoch: 3, Batch: 31/842, Loss: 0.8931, Time: 0.73s\n",
      "Epoch: 3, Batch: 41/842, Loss: 0.8299, Time: 0.73s\n",
      "Epoch: 3, Batch: 51/842, Loss: 0.8139, Time: 0.79s\n",
      "Epoch: 3, Batch: 61/842, Loss: 0.6074, Time: 0.78s\n",
      "Epoch: 3, Batch: 71/842, Loss: 0.8432, Time: 0.76s\n",
      "Epoch: 3, Batch: 81/842, Loss: 0.8286, Time: 0.75s\n",
      "Epoch: 3, Batch: 91/842, Loss: 0.7786, Time: 0.74s\n",
      "Epoch: 3, Batch: 101/842, Loss: 0.7175, Time: 0.77s\n",
      "Epoch: 3, Batch: 111/842, Loss: 0.6493, Time: 0.75s\n",
      "Epoch: 3, Batch: 121/842, Loss: 0.8443, Time: 0.74s\n",
      "Epoch: 3, Batch: 131/842, Loss: 0.7328, Time: 0.71s\n",
      "Epoch: 3, Batch: 141/842, Loss: 0.8362, Time: 0.75s\n",
      "Epoch: 3, Batch: 151/842, Loss: 0.7085, Time: 0.74s\n",
      "Epoch: 3, Batch: 161/842, Loss: 0.6888, Time: 0.74s\n",
      "Epoch: 3, Batch: 171/842, Loss: 0.7521, Time: 0.73s\n",
      "Epoch: 3, Batch: 181/842, Loss: 0.7982, Time: 0.73s\n",
      "Epoch: 3, Batch: 191/842, Loss: 0.7858, Time: 0.73s\n",
      "Epoch: 3, Batch: 201/842, Loss: 0.7904, Time: 0.76s\n",
      "Epoch: 3, Batch: 211/842, Loss: 0.9518, Time: 0.72s\n",
      "Epoch: 3, Batch: 221/842, Loss: 0.9485, Time: 0.69s\n",
      "Epoch: 3, Batch: 231/842, Loss: 0.7009, Time: 0.68s\n",
      "Epoch: 3, Batch: 241/842, Loss: 0.5855, Time: 0.69s\n",
      "Epoch: 3, Batch: 251/842, Loss: 0.9384, Time: 0.70s\n",
      "Epoch: 3, Batch: 261/842, Loss: 0.6834, Time: 0.69s\n",
      "Epoch: 3, Batch: 271/842, Loss: 0.5949, Time: 0.69s\n",
      "Epoch: 3, Batch: 281/842, Loss: 0.7148, Time: 0.69s\n",
      "Epoch: 3, Batch: 291/842, Loss: 0.9803, Time: 0.70s\n",
      "Epoch: 3, Batch: 301/842, Loss: 0.8037, Time: 0.69s\n",
      "Epoch: 3, Batch: 311/842, Loss: 0.8110, Time: 0.71s\n",
      "Epoch: 3, Batch: 321/842, Loss: 0.5713, Time: 0.68s\n",
      "Epoch: 3, Batch: 331/842, Loss: 0.9464, Time: 0.68s\n",
      "Epoch: 3, Batch: 341/842, Loss: 0.7358, Time: 0.68s\n",
      "Epoch: 3, Batch: 351/842, Loss: 0.6531, Time: 0.68s\n",
      "Epoch: 3, Batch: 361/842, Loss: 0.8449, Time: 0.73s\n",
      "Epoch: 3, Batch: 371/842, Loss: 0.5862, Time: 0.71s\n",
      "Epoch: 3, Batch: 381/842, Loss: 0.7267, Time: 0.71s\n",
      "Epoch: 3, Batch: 391/842, Loss: 0.7210, Time: 0.75s\n",
      "Epoch: 3, Batch: 401/842, Loss: 0.7324, Time: 0.71s\n",
      "Epoch: 3, Batch: 411/842, Loss: 0.9345, Time: 0.71s\n",
      "Epoch: 3, Batch: 421/842, Loss: 0.7655, Time: 0.77s\n",
      "Epoch: 3, Batch: 431/842, Loss: 0.8268, Time: 0.76s\n",
      "Epoch: 3, Batch: 441/842, Loss: 0.7887, Time: 0.71s\n",
      "Epoch: 3, Batch: 451/842, Loss: 0.6846, Time: 0.71s\n",
      "Epoch: 3, Batch: 461/842, Loss: 0.8180, Time: 0.72s\n",
      "Epoch: 3, Batch: 471/842, Loss: 0.7209, Time: 0.73s\n",
      "Epoch: 3, Batch: 481/842, Loss: 0.8993, Time: 0.73s\n",
      "Epoch: 3, Batch: 491/842, Loss: 0.8309, Time: 0.74s\n",
      "Epoch: 3, Batch: 501/842, Loss: 0.7069, Time: 0.73s\n",
      "Epoch: 3, Batch: 511/842, Loss: 0.7333, Time: 0.77s\n",
      "Epoch: 3, Batch: 521/842, Loss: 0.7235, Time: 0.77s\n",
      "Epoch: 3, Batch: 531/842, Loss: 0.8343, Time: 0.91s\n",
      "Epoch: 3, Batch: 541/842, Loss: 0.6960, Time: 0.89s\n",
      "Epoch: 3, Batch: 551/842, Loss: 0.7319, Time: 0.87s\n",
      "Epoch: 3, Batch: 561/842, Loss: 0.8471, Time: 0.93s\n",
      "Epoch: 3, Batch: 571/842, Loss: 0.6011, Time: 0.90s\n",
      "Epoch: 3, Batch: 581/842, Loss: 0.8218, Time: 0.86s\n",
      "Epoch: 3, Batch: 591/842, Loss: 0.8097, Time: 0.89s\n",
      "Epoch: 3, Batch: 601/842, Loss: 0.7792, Time: 0.85s\n",
      "Epoch: 3, Batch: 611/842, Loss: 0.6039, Time: 0.76s\n",
      "Epoch: 3, Batch: 621/842, Loss: 0.8320, Time: 0.74s\n",
      "Epoch: 3, Batch: 631/842, Loss: 0.4854, Time: 0.74s\n",
      "Epoch: 3, Batch: 641/842, Loss: 0.7701, Time: 0.75s\n",
      "Epoch: 3, Batch: 651/842, Loss: 0.5284, Time: 0.74s\n",
      "Epoch: 3, Batch: 661/842, Loss: 0.6200, Time: 0.75s\n",
      "Epoch: 3, Batch: 671/842, Loss: 0.6618, Time: 0.72s\n",
      "Epoch: 3, Batch: 681/842, Loss: 0.8567, Time: 0.74s\n",
      "Epoch: 3, Batch: 691/842, Loss: 0.6376, Time: 0.74s\n",
      "Epoch: 3, Batch: 701/842, Loss: 0.6329, Time: 0.73s\n",
      "Epoch: 3, Batch: 711/842, Loss: 0.7854, Time: 0.73s\n",
      "Epoch: 3, Batch: 721/842, Loss: 0.7436, Time: 0.74s\n",
      "Epoch: 3, Batch: 731/842, Loss: 0.5291, Time: 0.73s\n",
      "Epoch: 3, Batch: 741/842, Loss: 0.5146, Time: 0.73s\n",
      "Epoch: 3, Batch: 751/842, Loss: 0.7105, Time: 0.74s\n",
      "Epoch: 3, Batch: 761/842, Loss: 0.5751, Time: 0.77s\n",
      "Epoch: 3, Batch: 771/842, Loss: 0.8472, Time: 0.74s\n",
      "Epoch: 3, Batch: 781/842, Loss: 0.5560, Time: 0.73s\n",
      "Epoch: 3, Batch: 791/842, Loss: 0.8732, Time: 0.75s\n",
      "Epoch: 3, Batch: 801/842, Loss: 0.7600, Time: 0.76s\n",
      "Epoch: 3, Batch: 811/842, Loss: 0.8506, Time: 0.74s\n",
      "Epoch: 3, Batch: 821/842, Loss: 0.6694, Time: 0.73s\n",
      "Epoch: 3, Batch: 831/842, Loss: 0.6028, Time: 0.72s\n",
      "Epoch: 3, Batch: 841/842, Loss: 0.6408, Time: 0.71s\n",
      "Epoch 3/100: Train Loss: 0.7248, Val Loss: 0.6942, mIoU: 0.4970, F1: 0.6447, OA: 0.9432\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.6951, F1=0.8201, OA=0.8105, Precision=0.7560, Recall=0.8962\n",
      "    Class 1: IoU=0.5469, F1=0.7071, OA=0.9947, Precision=0.8598, Recall=0.6005\n",
      "    Class 2: IoU=0.8515, F1=0.9198, OA=0.9680, Precision=0.9347, Recall=0.9053\n",
      "    Class 3: IoU=0.2864, F1=0.4452, OA=0.9532, Precision=0.7972, Recall=0.3089\n",
      "    Class 4: IoU=0.4846, F1=0.6528, OA=0.9632, Precision=0.9142, Recall=0.5077\n",
      "    Class 5: IoU=0.2823, F1=0.4403, OA=0.9906, Precision=0.4672, Recall=0.4164\n",
      "    Class 6: IoU=0.3975, F1=0.5688, OA=0.9198, Precision=0.5328, Recall=0.6101\n",
      "    Class 7: IoU=0.4317, F1=0.6031, OA=0.9460, Precision=0.7315, Recall=0.5130\n",
      "Saving best model with mIoU: 0.4970\n",
      "Epoch: 4, Batch: 1/842, Loss: 0.5817, Time: 1.65s\n",
      "Epoch: 4, Batch: 11/842, Loss: 0.7612, Time: 0.74s\n",
      "Epoch: 4, Batch: 21/842, Loss: 0.6248, Time: 0.73s\n",
      "Epoch: 4, Batch: 31/842, Loss: 0.5660, Time: 0.73s\n",
      "Epoch: 4, Batch: 41/842, Loss: 0.6929, Time: 0.72s\n",
      "Epoch: 4, Batch: 51/842, Loss: 0.6171, Time: 0.73s\n",
      "Epoch: 4, Batch: 61/842, Loss: 0.6265, Time: 0.73s\n",
      "Epoch: 4, Batch: 71/842, Loss: 0.6357, Time: 1.34s\n",
      "Epoch: 4, Batch: 81/842, Loss: 0.7037, Time: 0.72s\n",
      "Epoch: 4, Batch: 91/842, Loss: 0.6171, Time: 0.72s\n",
      "Epoch: 4, Batch: 101/842, Loss: 0.8042, Time: 0.73s\n",
      "Epoch: 4, Batch: 111/842, Loss: 0.6970, Time: 0.79s\n",
      "Epoch: 4, Batch: 121/842, Loss: 0.8878, Time: 0.79s\n",
      "Epoch: 4, Batch: 131/842, Loss: 0.5200, Time: 0.75s\n",
      "Epoch: 4, Batch: 141/842, Loss: 0.6171, Time: 0.73s\n",
      "Epoch: 4, Batch: 151/842, Loss: 0.5506, Time: 0.73s\n",
      "Epoch: 4, Batch: 161/842, Loss: 0.6380, Time: 0.73s\n",
      "Epoch: 4, Batch: 171/842, Loss: 0.7732, Time: 0.72s\n",
      "Epoch: 4, Batch: 181/842, Loss: 0.4402, Time: 0.72s\n",
      "Epoch: 4, Batch: 191/842, Loss: 0.7418, Time: 0.73s\n",
      "Epoch: 4, Batch: 201/842, Loss: 0.4838, Time: 0.72s\n",
      "Epoch: 4, Batch: 211/842, Loss: 0.5373, Time: 0.72s\n",
      "Epoch: 4, Batch: 221/842, Loss: 0.6020, Time: 0.86s\n",
      "Epoch: 4, Batch: 231/842, Loss: 0.7170, Time: 0.88s\n",
      "Epoch: 4, Batch: 241/842, Loss: 0.6945, Time: 0.86s\n",
      "Epoch: 4, Batch: 251/842, Loss: 0.7404, Time: 0.87s\n",
      "Epoch: 4, Batch: 261/842, Loss: 0.8135, Time: 0.85s\n",
      "Epoch: 4, Batch: 271/842, Loss: 0.8470, Time: 0.90s\n",
      "Epoch: 4, Batch: 281/842, Loss: 0.5996, Time: 0.80s\n",
      "Epoch: 4, Batch: 291/842, Loss: 0.8613, Time: 0.80s\n",
      "Epoch: 4, Batch: 301/842, Loss: 0.9025, Time: 0.82s\n",
      "Epoch: 4, Batch: 311/842, Loss: 0.8485, Time: 0.88s\n",
      "Epoch: 4, Batch: 321/842, Loss: 0.7006, Time: 0.76s\n",
      "Epoch: 4, Batch: 331/842, Loss: 0.6316, Time: 0.75s\n",
      "Epoch: 4, Batch: 341/842, Loss: 0.8192, Time: 0.73s\n",
      "Epoch: 4, Batch: 351/842, Loss: 0.8047, Time: 0.72s\n",
      "Epoch: 4, Batch: 361/842, Loss: 0.6097, Time: 0.71s\n",
      "Epoch: 4, Batch: 371/842, Loss: 0.5774, Time: 0.73s\n",
      "Epoch: 4, Batch: 381/842, Loss: 0.6957, Time: 0.71s\n",
      "Epoch: 4, Batch: 391/842, Loss: 0.8801, Time: 0.75s\n",
      "Epoch: 4, Batch: 401/842, Loss: 0.7401, Time: 0.75s\n",
      "Epoch: 4, Batch: 411/842, Loss: 0.6798, Time: 0.73s\n",
      "Epoch: 4, Batch: 421/842, Loss: 0.5744, Time: 0.73s\n",
      "Epoch: 4, Batch: 431/842, Loss: 0.7002, Time: 0.73s\n",
      "Epoch: 4, Batch: 441/842, Loss: 0.5879, Time: 0.72s\n",
      "Epoch: 4, Batch: 451/842, Loss: 0.8225, Time: 0.79s\n",
      "Epoch: 4, Batch: 461/842, Loss: 0.6251, Time: 0.78s\n",
      "Epoch: 4, Batch: 471/842, Loss: 0.6707, Time: 0.73s\n",
      "Epoch: 4, Batch: 481/842, Loss: 0.7261, Time: 0.75s\n",
      "Epoch: 4, Batch: 491/842, Loss: 1.0471, Time: 0.73s\n",
      "Epoch: 4, Batch: 501/842, Loss: 0.7535, Time: 0.74s\n",
      "Epoch: 4, Batch: 511/842, Loss: 0.8311, Time: 0.73s\n",
      "Epoch: 4, Batch: 521/842, Loss: 0.6432, Time: 0.74s\n",
      "Epoch: 4, Batch: 531/842, Loss: 0.5851, Time: 0.73s\n",
      "Epoch: 4, Batch: 541/842, Loss: 0.6496, Time: 0.73s\n",
      "Epoch: 4, Batch: 551/842, Loss: 0.7205, Time: 0.73s\n",
      "Epoch: 4, Batch: 561/842, Loss: 0.7835, Time: 0.73s\n",
      "Epoch: 4, Batch: 571/842, Loss: 0.7844, Time: 0.75s\n",
      "Epoch: 4, Batch: 581/842, Loss: 0.5800, Time: 0.76s\n",
      "Epoch: 4, Batch: 591/842, Loss: 0.6134, Time: 0.74s\n",
      "Epoch: 4, Batch: 601/842, Loss: 0.9054, Time: 0.74s\n",
      "Epoch: 4, Batch: 611/842, Loss: 0.6495, Time: 0.74s\n",
      "Epoch: 4, Batch: 621/842, Loss: 0.6488, Time: 0.76s\n",
      "Epoch: 4, Batch: 631/842, Loss: 0.6859, Time: 0.73s\n",
      "Epoch: 4, Batch: 641/842, Loss: 0.8033, Time: 0.75s\n",
      "Epoch: 4, Batch: 651/842, Loss: 0.7355, Time: 0.72s\n",
      "Epoch: 4, Batch: 661/842, Loss: 0.4182, Time: 0.73s\n",
      "Epoch: 4, Batch: 671/842, Loss: 0.6125, Time: 0.72s\n",
      "Epoch: 4, Batch: 681/842, Loss: 0.6357, Time: 0.75s\n",
      "Epoch: 4, Batch: 691/842, Loss: 0.8245, Time: 0.81s\n",
      "Epoch: 4, Batch: 701/842, Loss: 0.6623, Time: 0.81s\n",
      "Epoch: 4, Batch: 711/842, Loss: 0.6046, Time: 0.81s\n",
      "Epoch: 4, Batch: 721/842, Loss: 0.5194, Time: 0.75s\n",
      "Epoch: 4, Batch: 731/842, Loss: 0.5725, Time: 0.78s\n",
      "Epoch: 4, Batch: 741/842, Loss: 0.6462, Time: 0.76s\n",
      "Epoch: 4, Batch: 751/842, Loss: 0.5973, Time: 0.79s\n",
      "Epoch: 4, Batch: 761/842, Loss: 0.7723, Time: 0.76s\n",
      "Epoch: 4, Batch: 771/842, Loss: 0.6270, Time: 0.85s\n",
      "Epoch: 4, Batch: 781/842, Loss: 1.0396, Time: 0.85s\n",
      "Epoch: 4, Batch: 791/842, Loss: 0.6757, Time: 0.83s\n",
      "Epoch: 4, Batch: 801/842, Loss: 0.5043, Time: 0.78s\n",
      "Epoch: 4, Batch: 811/842, Loss: 0.6933, Time: 0.72s\n",
      "Epoch: 4, Batch: 821/842, Loss: 0.7419, Time: 0.75s\n",
      "Epoch: 4, Batch: 831/842, Loss: 0.7466, Time: 0.73s\n",
      "Epoch: 4, Batch: 841/842, Loss: 0.6442, Time: 0.73s\n",
      "Epoch 4/100: Train Loss: 0.6834, Val Loss: 0.7630, mIoU: 0.3923, F1: 0.5360, OA: 0.9342\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.6712, F1=0.8032, OA=0.7753, Precision=0.6950, Recall=0.9514\n",
      "    Class 1: IoU=0.2882, F1=0.4475, OA=0.9823, Precision=0.3348, Recall=0.6745\n",
      "    Class 2: IoU=0.7411, F1=0.8513, OA=0.9452, Precision=0.9452, Recall=0.7744\n",
      "    Class 3: IoU=0.2150, F1=0.3539, OA=0.9509, Precision=0.8839, Recall=0.2212\n",
      "    Class 4: IoU=0.2335, F1=0.3786, OA=0.9467, Precision=0.9209, Recall=0.2383\n",
      "    Class 5: IoU=0.1706, F1=0.2915, OA=0.9920, Precision=0.6970, Recall=0.1843\n",
      "    Class 6: IoU=0.4055, F1=0.5770, OA=0.9354, Precision=0.6674, Recall=0.5082\n",
      "    Class 7: IoU=0.4135, F1=0.5850, OA=0.9455, Precision=0.7480, Recall=0.4804\n",
      "Epoch: 5, Batch: 1/842, Loss: 0.6528, Time: 1.62s\n",
      "Epoch: 5, Batch: 11/842, Loss: 0.5280, Time: 0.90s\n",
      "Epoch: 5, Batch: 21/842, Loss: 0.8078, Time: 0.84s\n",
      "Epoch: 5, Batch: 31/842, Loss: 0.6627, Time: 0.85s\n",
      "Epoch: 5, Batch: 41/842, Loss: 0.8380, Time: 0.89s\n",
      "Epoch: 5, Batch: 51/842, Loss: 0.7648, Time: 0.86s\n",
      "Epoch: 5, Batch: 61/842, Loss: 0.5855, Time: 0.84s\n",
      "Epoch: 5, Batch: 71/842, Loss: 0.5945, Time: 0.82s\n",
      "Epoch: 5, Batch: 81/842, Loss: 0.5926, Time: 0.87s\n",
      "Epoch: 5, Batch: 91/842, Loss: 0.5858, Time: 0.82s\n",
      "Epoch: 5, Batch: 101/842, Loss: 0.5311, Time: 0.76s\n",
      "Epoch: 5, Batch: 111/842, Loss: 0.7347, Time: 0.77s\n",
      "Epoch: 5, Batch: 121/842, Loss: 0.6309, Time: 0.77s\n",
      "Epoch: 5, Batch: 131/842, Loss: 0.7219, Time: 0.78s\n",
      "Epoch: 5, Batch: 141/842, Loss: 0.8552, Time: 0.75s\n",
      "Epoch: 5, Batch: 151/842, Loss: 0.7147, Time: 0.77s\n",
      "Epoch: 5, Batch: 161/842, Loss: 0.5061, Time: 0.76s\n",
      "Epoch: 5, Batch: 171/842, Loss: 0.6254, Time: 0.75s\n",
      "Epoch: 5, Batch: 181/842, Loss: 0.5172, Time: 0.76s\n",
      "Epoch: 5, Batch: 191/842, Loss: 0.7969, Time: 0.75s\n",
      "Epoch: 5, Batch: 201/842, Loss: 0.6502, Time: 0.76s\n",
      "Epoch: 5, Batch: 211/842, Loss: 0.6946, Time: 0.76s\n",
      "Epoch: 5, Batch: 221/842, Loss: 0.9478, Time: 0.78s\n",
      "Epoch: 5, Batch: 231/842, Loss: 0.5894, Time: 0.76s\n",
      "Epoch: 5, Batch: 241/842, Loss: 0.5933, Time: 0.77s\n",
      "Epoch: 5, Batch: 251/842, Loss: 0.7514, Time: 0.78s\n",
      "Epoch: 5, Batch: 261/842, Loss: 0.5685, Time: 0.81s\n",
      "Epoch: 5, Batch: 271/842, Loss: 0.6626, Time: 0.77s\n",
      "Epoch: 5, Batch: 281/842, Loss: 0.5786, Time: 0.96s\n",
      "Epoch: 5, Batch: 291/842, Loss: 0.7266, Time: 0.88s\n",
      "Epoch: 5, Batch: 301/842, Loss: 0.4479, Time: 0.80s\n",
      "Epoch: 5, Batch: 311/842, Loss: 0.9437, Time: 0.78s\n",
      "Epoch: 5, Batch: 321/842, Loss: 0.6972, Time: 0.76s\n",
      "Epoch: 5, Batch: 331/842, Loss: 0.8816, Time: 0.75s\n",
      "Epoch: 5, Batch: 341/842, Loss: 0.8621, Time: 0.77s\n",
      "Epoch: 5, Batch: 351/842, Loss: 0.5780, Time: 0.73s\n",
      "Epoch: 5, Batch: 361/842, Loss: 0.6277, Time: 0.74s\n",
      "Epoch: 5, Batch: 371/842, Loss: 0.6722, Time: 0.74s\n",
      "Epoch: 5, Batch: 381/842, Loss: 0.6811, Time: 0.74s\n",
      "Epoch: 5, Batch: 391/842, Loss: 0.6853, Time: 0.74s\n",
      "Epoch: 5, Batch: 401/842, Loss: 0.6738, Time: 0.73s\n",
      "Epoch: 5, Batch: 411/842, Loss: 0.8113, Time: 0.73s\n",
      "Epoch: 5, Batch: 421/842, Loss: 0.6425, Time: 0.73s\n",
      "Epoch: 5, Batch: 431/842, Loss: 0.9713, Time: 0.72s\n",
      "Epoch: 5, Batch: 441/842, Loss: 0.6143, Time: 0.72s\n",
      "Epoch: 5, Batch: 451/842, Loss: 0.7331, Time: 0.72s\n",
      "Epoch: 5, Batch: 461/842, Loss: 0.7065, Time: 0.72s\n",
      "Epoch: 5, Batch: 471/842, Loss: 0.5844, Time: 0.72s\n",
      "Epoch: 5, Batch: 481/842, Loss: 0.7495, Time: 0.73s\n",
      "Epoch: 5, Batch: 491/842, Loss: 0.5407, Time: 0.73s\n",
      "Epoch: 5, Batch: 501/842, Loss: 0.7807, Time: 0.72s\n",
      "Epoch: 5, Batch: 511/842, Loss: 0.5581, Time: 0.72s\n",
      "Epoch: 5, Batch: 521/842, Loss: 0.5798, Time: 0.72s\n",
      "Epoch: 5, Batch: 531/842, Loss: 0.4899, Time: 0.72s\n",
      "Epoch: 5, Batch: 541/842, Loss: 0.5213, Time: 0.71s\n",
      "Epoch: 5, Batch: 551/842, Loss: 0.7148, Time: 0.72s\n",
      "Epoch: 5, Batch: 561/842, Loss: 0.6977, Time: 0.72s\n",
      "Epoch: 5, Batch: 571/842, Loss: 0.8284, Time: 0.75s\n",
      "Epoch: 5, Batch: 581/842, Loss: 0.4493, Time: 0.79s\n",
      "Epoch: 5, Batch: 591/842, Loss: 0.7280, Time: 0.75s\n",
      "Epoch: 5, Batch: 601/842, Loss: 0.6806, Time: 0.75s\n",
      "Epoch: 5, Batch: 611/842, Loss: 0.7375, Time: 0.77s\n",
      "Epoch: 5, Batch: 621/842, Loss: 0.8053, Time: 0.75s\n",
      "Epoch: 5, Batch: 631/842, Loss: 0.6900, Time: 0.75s\n",
      "Epoch: 5, Batch: 641/842, Loss: 0.6685, Time: 0.75s\n",
      "Epoch: 5, Batch: 651/842, Loss: 0.6728, Time: 0.74s\n",
      "Epoch: 5, Batch: 661/842, Loss: 0.6978, Time: 0.74s\n",
      "Epoch: 5, Batch: 671/842, Loss: 0.5745, Time: 0.73s\n",
      "Epoch: 5, Batch: 681/842, Loss: 0.6306, Time: 0.74s\n",
      "Epoch: 5, Batch: 691/842, Loss: 0.7763, Time: 0.74s\n",
      "Epoch: 5, Batch: 701/842, Loss: 0.6583, Time: 0.73s\n",
      "Epoch: 5, Batch: 711/842, Loss: 0.6719, Time: 0.74s\n",
      "Epoch: 5, Batch: 721/842, Loss: 0.4405, Time: 0.73s\n",
      "Epoch: 5, Batch: 731/842, Loss: 0.6551, Time: 0.75s\n",
      "Epoch: 5, Batch: 741/842, Loss: 0.4842, Time: 0.73s\n",
      "Epoch: 5, Batch: 751/842, Loss: 0.7121, Time: 0.73s\n",
      "Epoch: 5, Batch: 761/842, Loss: 0.6239, Time: 0.75s\n",
      "Epoch: 5, Batch: 771/842, Loss: 0.6764, Time: 0.73s\n",
      "Epoch: 5, Batch: 781/842, Loss: 0.7790, Time: 0.72s\n",
      "Epoch: 5, Batch: 791/842, Loss: 0.8382, Time: 0.72s\n",
      "Epoch: 5, Batch: 801/842, Loss: 0.6961, Time: 0.72s\n",
      "Epoch: 5, Batch: 811/842, Loss: 0.6776, Time: 0.73s\n",
      "Epoch: 5, Batch: 821/842, Loss: 0.7541, Time: 0.74s\n",
      "Epoch: 5, Batch: 831/842, Loss: 0.7113, Time: 0.73s\n",
      "Epoch: 5, Batch: 841/842, Loss: 0.6589, Time: 0.71s\n",
      "Epoch 5/100: Train Loss: 0.6606, Val Loss: 0.6141, mIoU: 0.5505, F1: 0.6970, OA: 0.9493\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7079, F1=0.8290, OA=0.8212, Precision=0.7691, Recall=0.8990\n",
      "    Class 1: IoU=0.5552, F1=0.7140, OA=0.9937, Precision=0.6939, Recall=0.7353\n",
      "    Class 2: IoU=0.8711, F1=0.9311, OA=0.9721, Precision=0.9328, Recall=0.9295\n",
      "    Class 3: IoU=0.3496, F1=0.5180, OA=0.9539, Precision=0.7113, Recall=0.4074\n",
      "    Class 4: IoU=0.5941, F1=0.7454, OA=0.9667, Precision=0.7793, Recall=0.7143\n",
      "    Class 5: IoU=0.4183, F1=0.5898, OA=0.9935, Precision=0.6691, Recall=0.5274\n",
      "    Class 6: IoU=0.4361, F1=0.6073, OA=0.9461, Precision=0.8252, Recall=0.4805\n",
      "    Class 7: IoU=0.4719, F1=0.6412, OA=0.9467, Precision=0.6955, Recall=0.5947\n",
      "Saving best model with mIoU: 0.5505\n",
      "Epoch: 6, Batch: 1/842, Loss: 0.5576, Time: 1.57s\n",
      "Epoch: 6, Batch: 11/842, Loss: 0.5981, Time: 0.76s\n",
      "Epoch: 6, Batch: 21/842, Loss: 0.7087, Time: 0.74s\n",
      "Epoch: 6, Batch: 31/842, Loss: 0.6548, Time: 0.73s\n",
      "Epoch: 6, Batch: 41/842, Loss: 0.6824, Time: 0.72s\n",
      "Epoch: 6, Batch: 51/842, Loss: 0.4013, Time: 0.72s\n",
      "Epoch: 6, Batch: 61/842, Loss: 0.6923, Time: 0.73s\n",
      "Epoch: 6, Batch: 71/842, Loss: 0.7131, Time: 0.72s\n",
      "Epoch: 6, Batch: 81/842, Loss: 0.6811, Time: 0.72s\n",
      "Epoch: 6, Batch: 91/842, Loss: 0.7043, Time: 0.73s\n",
      "Epoch: 6, Batch: 101/842, Loss: 0.6520, Time: 0.71s\n",
      "Epoch: 6, Batch: 111/842, Loss: 0.7548, Time: 0.72s\n",
      "Epoch: 6, Batch: 121/842, Loss: 0.4069, Time: 0.71s\n",
      "Epoch: 6, Batch: 131/842, Loss: 0.5607, Time: 0.75s\n",
      "Epoch: 6, Batch: 141/842, Loss: 0.6241, Time: 0.74s\n",
      "Epoch: 6, Batch: 151/842, Loss: 0.5285, Time: 0.74s\n",
      "Epoch: 6, Batch: 161/842, Loss: 0.6467, Time: 0.74s\n",
      "Epoch: 6, Batch: 171/842, Loss: 0.5692, Time: 0.72s\n",
      "Epoch: 6, Batch: 181/842, Loss: 0.7923, Time: 0.74s\n",
      "Epoch: 6, Batch: 191/842, Loss: 0.8195, Time: 0.77s\n",
      "Epoch: 6, Batch: 201/842, Loss: 0.6606, Time: 0.77s\n",
      "Epoch: 6, Batch: 211/842, Loss: 0.4787, Time: 0.73s\n",
      "Epoch: 6, Batch: 221/842, Loss: 0.6829, Time: 0.73s\n",
      "Epoch: 6, Batch: 231/842, Loss: 0.5587, Time: 0.72s\n",
      "Epoch: 6, Batch: 241/842, Loss: 0.7045, Time: 0.72s\n",
      "Epoch: 6, Batch: 251/842, Loss: 0.5813, Time: 0.73s\n",
      "Epoch: 6, Batch: 261/842, Loss: 0.6625, Time: 0.77s\n",
      "Epoch: 6, Batch: 271/842, Loss: 0.6429, Time: 0.78s\n",
      "Epoch: 6, Batch: 281/842, Loss: 0.4034, Time: 0.79s\n",
      "Epoch: 6, Batch: 291/842, Loss: 0.7662, Time: 0.77s\n",
      "Epoch: 6, Batch: 301/842, Loss: 0.7433, Time: 0.78s\n",
      "Epoch: 6, Batch: 311/842, Loss: 0.5927, Time: 0.79s\n",
      "Epoch: 6, Batch: 321/842, Loss: 0.9946, Time: 0.82s\n",
      "Epoch: 6, Batch: 331/842, Loss: 0.4543, Time: 0.79s\n",
      "Epoch: 6, Batch: 341/842, Loss: 0.6637, Time: 0.80s\n",
      "Epoch: 6, Batch: 351/842, Loss: 0.5636, Time: 0.75s\n",
      "Epoch: 6, Batch: 361/842, Loss: 0.8531, Time: 0.74s\n",
      "Epoch: 6, Batch: 371/842, Loss: 0.8178, Time: 0.73s\n",
      "Epoch: 6, Batch: 381/842, Loss: 0.7181, Time: 0.74s\n",
      "Epoch: 6, Batch: 391/842, Loss: 0.7177, Time: 0.72s\n",
      "Epoch: 6, Batch: 401/842, Loss: 0.6485, Time: 0.73s\n",
      "Epoch: 6, Batch: 411/842, Loss: 0.6228, Time: 0.72s\n",
      "Epoch: 6, Batch: 421/842, Loss: 0.6677, Time: 0.72s\n",
      "Epoch: 6, Batch: 431/842, Loss: 0.5327, Time: 0.69s\n",
      "Epoch: 6, Batch: 441/842, Loss: 0.7143, Time: 0.69s\n",
      "Epoch: 6, Batch: 451/842, Loss: 0.6851, Time: 0.68s\n",
      "Epoch: 6, Batch: 461/842, Loss: 0.7451, Time: 0.68s\n",
      "Epoch: 6, Batch: 471/842, Loss: 0.5642, Time: 0.67s\n",
      "Epoch: 6, Batch: 481/842, Loss: 0.5886, Time: 0.74s\n",
      "Epoch: 6, Batch: 491/842, Loss: 0.5612, Time: 0.75s\n",
      "Epoch: 6, Batch: 501/842, Loss: 0.5550, Time: 0.72s\n",
      "Epoch: 6, Batch: 511/842, Loss: 0.8394, Time: 0.71s\n",
      "Epoch: 6, Batch: 521/842, Loss: 0.7366, Time: 0.73s\n",
      "Epoch: 6, Batch: 531/842, Loss: 0.5547, Time: 0.71s\n",
      "Epoch: 6, Batch: 541/842, Loss: 0.6191, Time: 0.71s\n",
      "Epoch: 6, Batch: 551/842, Loss: 0.7120, Time: 0.71s\n",
      "Epoch: 6, Batch: 561/842, Loss: 0.5802, Time: 0.71s\n",
      "Epoch: 6, Batch: 571/842, Loss: 0.5930, Time: 0.71s\n",
      "Epoch: 6, Batch: 581/842, Loss: 0.5805, Time: 0.70s\n",
      "Epoch: 6, Batch: 591/842, Loss: 0.5427, Time: 0.71s\n",
      "Epoch: 6, Batch: 601/842, Loss: 0.7561, Time: 0.71s\n",
      "Epoch: 6, Batch: 611/842, Loss: 0.7762, Time: 0.70s\n",
      "Epoch: 6, Batch: 621/842, Loss: 0.4869, Time: 0.72s\n",
      "Epoch: 6, Batch: 631/842, Loss: 0.9076, Time: 0.70s\n",
      "Epoch: 6, Batch: 641/842, Loss: 0.6043, Time: 0.74s\n",
      "Epoch: 6, Batch: 651/842, Loss: 0.6402, Time: 0.77s\n",
      "Epoch: 6, Batch: 661/842, Loss: 0.5648, Time: 0.77s\n",
      "Epoch: 6, Batch: 671/842, Loss: 0.6676, Time: 0.75s\n",
      "Epoch: 6, Batch: 681/842, Loss: 0.7194, Time: 0.74s\n",
      "Epoch: 6, Batch: 691/842, Loss: 0.6681, Time: 0.76s\n",
      "Epoch: 6, Batch: 701/842, Loss: 0.9042, Time: 0.79s\n",
      "Epoch: 6, Batch: 711/842, Loss: 0.5010, Time: 0.79s\n",
      "Epoch: 6, Batch: 721/842, Loss: 0.6410, Time: 0.72s\n",
      "Epoch: 6, Batch: 731/842, Loss: 0.6382, Time: 0.73s\n",
      "Epoch: 6, Batch: 741/842, Loss: 0.6477, Time: 0.72s\n",
      "Epoch: 6, Batch: 751/842, Loss: 0.4885, Time: 0.71s\n",
      "Epoch: 6, Batch: 761/842, Loss: 0.6651, Time: 0.71s\n",
      "Epoch: 6, Batch: 771/842, Loss: 0.5640, Time: 0.71s\n",
      "Epoch: 6, Batch: 781/842, Loss: 0.6292, Time: 0.74s\n",
      "Epoch: 6, Batch: 791/842, Loss: 0.7906, Time: 0.77s\n",
      "Epoch: 6, Batch: 801/842, Loss: 0.6653, Time: 0.78s\n",
      "Epoch: 6, Batch: 811/842, Loss: 0.6995, Time: 0.75s\n",
      "Epoch: 6, Batch: 821/842, Loss: 0.7166, Time: 0.75s\n",
      "Epoch: 6, Batch: 831/842, Loss: 0.6863, Time: 0.73s\n",
      "Epoch: 6, Batch: 841/842, Loss: 0.7479, Time: 0.71s\n",
      "Epoch 6/100: Train Loss: 0.6540, Val Loss: 0.6385, mIoU: 0.5247, F1: 0.6688, OA: 0.9481\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7041, F1=0.8263, OA=0.8131, Precision=0.7485, Recall=0.9222\n",
      "    Class 1: IoU=0.5740, F1=0.7294, OA=0.9947, Precision=0.7911, Recall=0.6765\n",
      "    Class 2: IoU=0.8782, F1=0.9352, OA=0.9741, Precision=0.9472, Recall=0.9235\n",
      "    Class 3: IoU=0.3164, F1=0.4807, OA=0.9548, Precision=0.7962, Recall=0.3443\n",
      "    Class 4: IoU=0.5378, F1=0.6994, OA=0.9668, Precision=0.9136, Recall=0.5666\n",
      "    Class 5: IoU=0.2609, F1=0.4138, OA=0.9832, Precision=0.3004, Recall=0.6648\n",
      "    Class 6: IoU=0.4743, F1=0.6434, OA=0.9453, Precision=0.7395, Recall=0.5694\n",
      "    Class 7: IoU=0.4517, F1=0.6223, OA=0.9531, Precision=0.8737, Recall=0.4833\n",
      "Epoch: 7, Batch: 1/842, Loss: 0.5580, Time: 1.63s\n",
      "Epoch: 7, Batch: 11/842, Loss: 0.7180, Time: 0.75s\n",
      "Epoch: 7, Batch: 21/842, Loss: 0.5946, Time: 0.76s\n",
      "Epoch: 7, Batch: 31/842, Loss: 0.6254, Time: 0.75s\n",
      "Epoch: 7, Batch: 41/842, Loss: 0.4925, Time: 0.74s\n",
      "Epoch: 7, Batch: 51/842, Loss: 0.4841, Time: 0.75s\n",
      "Epoch: 7, Batch: 61/842, Loss: 0.7502, Time: 0.75s\n",
      "Epoch: 7, Batch: 71/842, Loss: 0.6640, Time: 0.74s\n",
      "Epoch: 7, Batch: 81/842, Loss: 0.5771, Time: 0.75s\n",
      "Epoch: 7, Batch: 91/842, Loss: 0.7189, Time: 0.76s\n",
      "Epoch: 7, Batch: 101/842, Loss: 0.7134, Time: 0.76s\n",
      "Epoch: 7, Batch: 111/842, Loss: 0.6505, Time: 0.82s\n",
      "Epoch: 7, Batch: 121/842, Loss: 0.5556, Time: 0.76s\n",
      "Epoch: 7, Batch: 131/842, Loss: 0.7813, Time: 0.76s\n",
      "Epoch: 7, Batch: 141/842, Loss: 0.7281, Time: 0.75s\n",
      "Epoch: 7, Batch: 151/842, Loss: 0.6577, Time: 0.74s\n",
      "Epoch: 7, Batch: 161/842, Loss: 0.8018, Time: 0.74s\n",
      "Epoch: 7, Batch: 171/842, Loss: 0.5773, Time: 0.74s\n",
      "Epoch: 7, Batch: 181/842, Loss: 0.6026, Time: 0.75s\n",
      "Epoch: 7, Batch: 191/842, Loss: 0.4913, Time: 0.74s\n",
      "Epoch: 7, Batch: 201/842, Loss: 0.7326, Time: 0.75s\n",
      "Epoch: 7, Batch: 211/842, Loss: 0.7978, Time: 0.72s\n",
      "Epoch: 7, Batch: 221/842, Loss: 0.7561, Time: 0.73s\n",
      "Epoch: 7, Batch: 231/842, Loss: 0.4759, Time: 0.72s\n",
      "Epoch: 7, Batch: 241/842, Loss: 0.5239, Time: 0.73s\n",
      "Epoch: 7, Batch: 251/842, Loss: 0.8900, Time: 0.71s\n",
      "Epoch: 7, Batch: 261/842, Loss: 0.6123, Time: 0.71s\n",
      "Epoch: 7, Batch: 271/842, Loss: 0.4139, Time: 0.75s\n",
      "Epoch: 7, Batch: 281/842, Loss: 0.8343, Time: 0.80s\n",
      "Epoch: 7, Batch: 291/842, Loss: 0.5438, Time: 0.77s\n",
      "Epoch: 7, Batch: 301/842, Loss: 0.5887, Time: 0.77s\n",
      "Epoch: 7, Batch: 311/842, Loss: 0.5290, Time: 0.78s\n",
      "Epoch: 7, Batch: 321/842, Loss: 0.5385, Time: 0.79s\n",
      "Epoch: 7, Batch: 331/842, Loss: 0.5730, Time: 0.77s\n",
      "Epoch: 7, Batch: 341/842, Loss: 0.5347, Time: 0.77s\n",
      "Epoch: 7, Batch: 351/842, Loss: 0.4800, Time: 0.76s\n",
      "Epoch: 7, Batch: 361/842, Loss: 0.5483, Time: 0.80s\n",
      "Epoch: 7, Batch: 371/842, Loss: 0.5632, Time: 0.76s\n",
      "Epoch: 7, Batch: 381/842, Loss: 0.5672, Time: 0.79s\n",
      "Epoch: 7, Batch: 391/842, Loss: 0.7115, Time: 0.79s\n",
      "Epoch: 7, Batch: 401/842, Loss: 0.6967, Time: 1.34s\n",
      "Epoch: 7, Batch: 411/842, Loss: 0.7943, Time: 0.75s\n",
      "Epoch: 7, Batch: 421/842, Loss: 0.6884, Time: 0.76s\n",
      "Epoch: 7, Batch: 431/842, Loss: 0.5148, Time: 0.76s\n",
      "Epoch: 7, Batch: 441/842, Loss: 0.6872, Time: 0.76s\n",
      "Epoch: 7, Batch: 451/842, Loss: 0.7647, Time: 0.83s\n",
      "Epoch: 7, Batch: 461/842, Loss: 0.7820, Time: 0.83s\n",
      "Epoch: 7, Batch: 471/842, Loss: 0.4544, Time: 0.80s\n",
      "Epoch: 7, Batch: 481/842, Loss: 0.6226, Time: 0.80s\n",
      "Epoch: 7, Batch: 491/842, Loss: 0.6919, Time: 0.77s\n",
      "Epoch: 7, Batch: 501/842, Loss: 0.4786, Time: 0.75s\n",
      "Epoch: 7, Batch: 511/842, Loss: 0.6564, Time: 0.74s\n",
      "Epoch: 7, Batch: 521/842, Loss: 0.5736, Time: 0.74s\n",
      "Epoch: 7, Batch: 531/842, Loss: 0.8006, Time: 0.76s\n",
      "Epoch: 7, Batch: 541/842, Loss: 0.7023, Time: 0.75s\n",
      "Epoch: 7, Batch: 551/842, Loss: 0.5494, Time: 0.76s\n",
      "Epoch: 7, Batch: 561/842, Loss: 0.6935, Time: 0.74s\n",
      "Epoch: 7, Batch: 571/842, Loss: 0.7163, Time: 0.75s\n",
      "Epoch: 7, Batch: 581/842, Loss: 0.5452, Time: 0.76s\n",
      "Epoch: 7, Batch: 591/842, Loss: 0.7164, Time: 0.74s\n",
      "Epoch: 7, Batch: 601/842, Loss: 0.7387, Time: 0.73s\n",
      "Epoch: 7, Batch: 611/842, Loss: 0.7186, Time: 0.74s\n",
      "Epoch: 7, Batch: 621/842, Loss: 0.4825, Time: 0.73s\n",
      "Epoch: 7, Batch: 631/842, Loss: 0.6615, Time: 0.73s\n",
      "Epoch: 7, Batch: 641/842, Loss: 0.6341, Time: 0.72s\n",
      "Epoch: 7, Batch: 651/842, Loss: 0.5943, Time: 0.73s\n",
      "Epoch: 7, Batch: 661/842, Loss: 0.6826, Time: 0.72s\n",
      "Epoch: 7, Batch: 671/842, Loss: 0.7297, Time: 0.75s\n",
      "Epoch: 7, Batch: 681/842, Loss: 0.5390, Time: 0.76s\n",
      "Epoch: 7, Batch: 691/842, Loss: 0.5099, Time: 0.74s\n",
      "Epoch: 7, Batch: 701/842, Loss: 0.5809, Time: 0.73s\n",
      "Epoch: 7, Batch: 711/842, Loss: 0.7001, Time: 0.77s\n",
      "Epoch: 7, Batch: 721/842, Loss: 0.6689, Time: 0.74s\n",
      "Epoch: 7, Batch: 731/842, Loss: 0.5392, Time: 0.72s\n",
      "Epoch: 7, Batch: 741/842, Loss: 0.5861, Time: 0.73s\n",
      "Epoch: 7, Batch: 751/842, Loss: 0.7548, Time: 0.73s\n",
      "Epoch: 7, Batch: 761/842, Loss: 0.5966, Time: 0.75s\n",
      "Epoch: 7, Batch: 771/842, Loss: 0.7164, Time: 0.73s\n",
      "Epoch: 7, Batch: 781/842, Loss: 0.4237, Time: 0.73s\n",
      "Epoch: 7, Batch: 791/842, Loss: 0.7525, Time: 0.73s\n",
      "Epoch: 7, Batch: 801/842, Loss: 0.4908, Time: 0.73s\n",
      "Epoch: 7, Batch: 811/842, Loss: 0.7072, Time: 0.79s\n",
      "Epoch: 7, Batch: 821/842, Loss: 0.7277, Time: 0.79s\n",
      "Epoch: 7, Batch: 831/842, Loss: 0.5661, Time: 0.78s\n",
      "Epoch: 7, Batch: 841/842, Loss: 0.7397, Time: 0.73s\n",
      "Epoch 7/100: Train Loss: 0.6353, Val Loss: 0.5892, mIoU: 0.5689, F1: 0.7115, OA: 0.9518\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7210, F1=0.8379, OA=0.8296, Precision=0.7740, Recall=0.9134\n",
      "    Class 1: IoU=0.5840, F1=0.7374, OA=0.9951, Precision=0.8538, Recall=0.6489\n",
      "    Class 2: IoU=0.8812, F1=0.9368, OA=0.9744, Precision=0.9361, Recall=0.9376\n",
      "    Class 3: IoU=0.3405, F1=0.5080, OA=0.9556, Precision=0.7785, Recall=0.3770\n",
      "    Class 4: IoU=0.6546, F1=0.7913, OA=0.9723, Precision=0.8129, Recall=0.7707\n",
      "    Class 5: IoU=0.4579, F1=0.6281, OA=0.9945, Precision=0.7904, Recall=0.5211\n",
      "    Class 6: IoU=0.4644, F1=0.6342, OA=0.9408, Precision=0.6822, Recall=0.5925\n",
      "    Class 7: IoU=0.4477, F1=0.6185, OA=0.9524, Precision=0.8625, Recall=0.4821\n",
      "Saving best model with mIoU: 0.5689\n",
      "Epoch: 8, Batch: 1/842, Loss: 0.5040, Time: 1.64s\n",
      "Epoch: 8, Batch: 11/842, Loss: 0.4459, Time: 0.77s\n",
      "Epoch: 8, Batch: 21/842, Loss: 0.5025, Time: 0.75s\n",
      "Epoch: 8, Batch: 31/842, Loss: 0.6749, Time: 0.75s\n",
      "Epoch: 8, Batch: 41/842, Loss: 0.5476, Time: 0.75s\n",
      "Epoch: 8, Batch: 51/842, Loss: 0.8081, Time: 0.76s\n",
      "Epoch: 8, Batch: 61/842, Loss: 0.6085, Time: 0.76s\n",
      "Epoch: 8, Batch: 71/842, Loss: 0.5092, Time: 0.82s\n",
      "Epoch: 8, Batch: 81/842, Loss: 0.6216, Time: 0.80s\n",
      "Epoch: 8, Batch: 91/842, Loss: 0.6710, Time: 0.75s\n",
      "Epoch: 8, Batch: 101/842, Loss: 0.5979, Time: 0.74s\n",
      "Epoch: 8, Batch: 111/842, Loss: 0.5575, Time: 0.75s\n",
      "Epoch: 8, Batch: 121/842, Loss: 0.6631, Time: 0.76s\n",
      "Epoch: 8, Batch: 131/842, Loss: 0.5157, Time: 0.75s\n",
      "Epoch: 8, Batch: 141/842, Loss: 0.5977, Time: 0.76s\n",
      "Epoch: 8, Batch: 151/842, Loss: 0.6774, Time: 0.75s\n",
      "Epoch: 8, Batch: 161/842, Loss: 0.6368, Time: 0.75s\n",
      "Epoch: 8, Batch: 171/842, Loss: 0.5496, Time: 0.74s\n",
      "Epoch: 8, Batch: 181/842, Loss: 0.5073, Time: 0.75s\n",
      "Epoch: 8, Batch: 191/842, Loss: 0.7696, Time: 0.74s\n",
      "Epoch: 8, Batch: 201/842, Loss: 0.5446, Time: 0.74s\n",
      "Epoch: 8, Batch: 211/842, Loss: 0.5575, Time: 0.73s\n",
      "Epoch: 8, Batch: 221/842, Loss: 0.5912, Time: 0.75s\n",
      "Epoch: 8, Batch: 231/842, Loss: 0.4805, Time: 0.73s\n",
      "Epoch: 8, Batch: 241/842, Loss: 0.6030, Time: 0.72s\n",
      "Epoch: 8, Batch: 251/842, Loss: 0.4591, Time: 0.75s\n",
      "Epoch: 8, Batch: 261/842, Loss: 0.6268, Time: 0.74s\n",
      "Epoch: 8, Batch: 271/842, Loss: 0.5588, Time: 0.74s\n",
      "Epoch: 8, Batch: 281/842, Loss: 0.5487, Time: 0.72s\n",
      "Epoch: 8, Batch: 291/842, Loss: 0.6347, Time: 0.77s\n",
      "Epoch: 8, Batch: 301/842, Loss: 0.5022, Time: 0.81s\n",
      "Epoch: 8, Batch: 311/842, Loss: 0.5320, Time: 0.79s\n",
      "Epoch: 8, Batch: 321/842, Loss: 0.5197, Time: 0.76s\n",
      "Epoch: 8, Batch: 331/842, Loss: 0.6024, Time: 0.77s\n",
      "Epoch: 8, Batch: 341/842, Loss: 0.6935, Time: 0.77s\n",
      "Epoch: 8, Batch: 351/842, Loss: 0.6064, Time: 0.84s\n",
      "Epoch: 8, Batch: 361/842, Loss: 0.5473, Time: 0.77s\n",
      "Epoch: 8, Batch: 371/842, Loss: 0.7424, Time: 0.75s\n",
      "Epoch: 8, Batch: 381/842, Loss: 0.6262, Time: 0.75s\n",
      "Epoch: 8, Batch: 391/842, Loss: 0.4741, Time: 0.81s\n",
      "Epoch: 8, Batch: 401/842, Loss: 0.8305, Time: 0.77s\n",
      "Epoch: 8, Batch: 411/842, Loss: 0.6805, Time: 0.77s\n",
      "Epoch: 8, Batch: 421/842, Loss: 0.5881, Time: 0.76s\n",
      "Epoch: 8, Batch: 431/842, Loss: 0.7067, Time: 0.75s\n",
      "Epoch: 8, Batch: 441/842, Loss: 0.6326, Time: 0.74s\n",
      "Epoch: 8, Batch: 451/842, Loss: 0.7230, Time: 0.78s\n",
      "Epoch: 8, Batch: 461/842, Loss: 0.4442, Time: 0.81s\n",
      "Epoch: 8, Batch: 471/842, Loss: 0.6458, Time: 0.83s\n",
      "Epoch: 8, Batch: 481/842, Loss: 0.8299, Time: 0.83s\n",
      "Epoch: 8, Batch: 491/842, Loss: 0.6060, Time: 0.82s\n",
      "Epoch: 8, Batch: 501/842, Loss: 0.5843, Time: 0.96s\n",
      "Epoch: 8, Batch: 511/842, Loss: 0.6403, Time: 0.98s\n",
      "Epoch: 8, Batch: 521/842, Loss: 0.7551, Time: 0.98s\n",
      "Epoch: 8, Batch: 531/842, Loss: 0.5865, Time: 0.89s\n",
      "Epoch: 8, Batch: 541/842, Loss: 0.6962, Time: 0.88s\n",
      "Epoch: 8, Batch: 551/842, Loss: 0.6272, Time: 0.82s\n",
      "Epoch: 8, Batch: 561/842, Loss: 0.6742, Time: 0.78s\n",
      "Epoch: 8, Batch: 571/842, Loss: 0.5998, Time: 0.80s\n",
      "Epoch: 8, Batch: 581/842, Loss: 0.6026, Time: 0.72s\n",
      "Epoch: 8, Batch: 591/842, Loss: 0.4292, Time: 0.73s\n",
      "Epoch: 8, Batch: 601/842, Loss: 0.5316, Time: 0.74s\n",
      "Epoch: 8, Batch: 611/842, Loss: 0.5391, Time: 0.74s\n",
      "Epoch: 8, Batch: 621/842, Loss: 0.5150, Time: 0.74s\n",
      "Epoch: 8, Batch: 631/842, Loss: 0.8023, Time: 0.73s\n",
      "Epoch: 8, Batch: 641/842, Loss: 0.5429, Time: 0.74s\n",
      "Epoch: 8, Batch: 651/842, Loss: 0.4955, Time: 0.73s\n",
      "Epoch: 8, Batch: 661/842, Loss: 0.4355, Time: 0.73s\n",
      "Epoch: 8, Batch: 671/842, Loss: 0.6184, Time: 0.73s\n",
      "Epoch: 8, Batch: 681/842, Loss: 0.6539, Time: 0.74s\n",
      "Epoch: 8, Batch: 691/842, Loss: 0.7129, Time: 0.74s\n",
      "Epoch: 8, Batch: 701/842, Loss: 0.7005, Time: 0.74s\n",
      "Epoch: 8, Batch: 711/842, Loss: 0.6013, Time: 0.72s\n",
      "Epoch: 8, Batch: 721/842, Loss: 0.4367, Time: 0.73s\n",
      "Epoch: 8, Batch: 731/842, Loss: 0.4464, Time: 0.73s\n",
      "Epoch: 8, Batch: 741/842, Loss: 0.4971, Time: 0.73s\n",
      "Epoch: 8, Batch: 751/842, Loss: 0.7114, Time: 0.76s\n",
      "Epoch: 8, Batch: 761/842, Loss: 0.6716, Time: 0.73s\n",
      "Epoch: 8, Batch: 771/842, Loss: 0.6446, Time: 0.75s\n",
      "Epoch: 8, Batch: 781/842, Loss: 0.6659, Time: 0.74s\n",
      "Epoch: 8, Batch: 791/842, Loss: 0.6213, Time: 0.73s\n",
      "Epoch: 8, Batch: 801/842, Loss: 0.5623, Time: 0.74s\n",
      "Epoch: 8, Batch: 811/842, Loss: 0.4818, Time: 0.76s\n",
      "Epoch: 8, Batch: 821/842, Loss: 0.5743, Time: 0.73s\n",
      "Epoch: 8, Batch: 831/842, Loss: 0.5659, Time: 0.71s\n",
      "Epoch: 8, Batch: 841/842, Loss: 0.7179, Time: 0.70s\n",
      "Epoch 8/100: Train Loss: 0.6242, Val Loss: 0.6172, mIoU: 0.5513, F1: 0.6964, OA: 0.9485\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7062, F1=0.8278, OA=0.8221, Precision=0.7761, Recall=0.8868\n",
      "    Class 1: IoU=0.5778, F1=0.7324, OA=0.9948, Precision=0.8061, Recall=0.6710\n",
      "    Class 2: IoU=0.8755, F1=0.9336, OA=0.9738, Precision=0.9597, Recall=0.9089\n",
      "    Class 3: IoU=0.3052, F1=0.4677, OA=0.9522, Precision=0.7230, Recall=0.3456\n",
      "    Class 4: IoU=0.5884, F1=0.7409, OA=0.9618, Precision=0.6895, Recall=0.8006\n",
      "    Class 5: IoU=0.4342, F1=0.6055, OA=0.9926, Precision=0.5748, Recall=0.6396\n",
      "    Class 6: IoU=0.4617, F1=0.6317, OA=0.9378, Precision=0.6486, Recall=0.6157\n",
      "    Class 7: IoU=0.4618, F1=0.6318, OA=0.9529, Precision=0.8436, Recall=0.5050\n",
      "Epoch: 9, Batch: 1/842, Loss: 0.5243, Time: 1.83s\n",
      "Epoch: 9, Batch: 11/842, Loss: 0.6785, Time: 0.77s\n",
      "Epoch: 9, Batch: 21/842, Loss: 0.5959, Time: 0.75s\n",
      "Epoch: 9, Batch: 31/842, Loss: 0.6413, Time: 0.74s\n",
      "Epoch: 9, Batch: 41/842, Loss: 0.6806, Time: 0.79s\n",
      "Epoch: 9, Batch: 51/842, Loss: 0.4639, Time: 0.77s\n",
      "Epoch: 9, Batch: 61/842, Loss: 0.4933, Time: 0.75s\n",
      "Epoch: 9, Batch: 71/842, Loss: 0.6542, Time: 0.76s\n",
      "Epoch: 9, Batch: 81/842, Loss: 0.6480, Time: 0.73s\n",
      "Epoch: 9, Batch: 91/842, Loss: 0.8284, Time: 0.74s\n",
      "Epoch: 9, Batch: 101/842, Loss: 0.6770, Time: 0.74s\n",
      "Epoch: 9, Batch: 111/842, Loss: 0.7116, Time: 0.72s\n",
      "Epoch: 9, Batch: 121/842, Loss: 0.5772, Time: 0.68s\n",
      "Epoch: 9, Batch: 131/842, Loss: 0.5684, Time: 0.67s\n",
      "Epoch: 9, Batch: 141/842, Loss: 0.6017, Time: 0.67s\n",
      "Epoch: 9, Batch: 151/842, Loss: 0.6315, Time: 0.68s\n",
      "Epoch: 9, Batch: 161/842, Loss: 0.5789, Time: 0.67s\n",
      "Epoch: 9, Batch: 171/842, Loss: 0.4504, Time: 0.71s\n",
      "Epoch: 9, Batch: 181/842, Loss: 0.7314, Time: 0.76s\n",
      "Epoch: 9, Batch: 191/842, Loss: 0.5853, Time: 0.73s\n",
      "Epoch: 9, Batch: 201/842, Loss: 0.5993, Time: 0.71s\n",
      "Epoch: 9, Batch: 211/842, Loss: 0.5399, Time: 0.72s\n",
      "Epoch: 9, Batch: 221/842, Loss: 0.6426, Time: 0.73s\n",
      "Epoch: 9, Batch: 231/842, Loss: 0.6551, Time: 0.71s\n",
      "Epoch: 9, Batch: 241/842, Loss: 0.7543, Time: 0.72s\n",
      "Epoch: 9, Batch: 251/842, Loss: 0.6190, Time: 0.72s\n",
      "Epoch: 9, Batch: 261/842, Loss: 0.5278, Time: 0.72s\n",
      "Epoch: 9, Batch: 271/842, Loss: 0.6342, Time: 0.75s\n",
      "Epoch: 9, Batch: 281/842, Loss: 0.6330, Time: 0.71s\n",
      "Epoch: 9, Batch: 291/842, Loss: 0.6858, Time: 0.72s\n",
      "Epoch: 9, Batch: 301/842, Loss: 0.5917, Time: 0.75s\n",
      "Epoch: 9, Batch: 311/842, Loss: 0.4959, Time: 0.73s\n",
      "Epoch: 9, Batch: 321/842, Loss: 0.5534, Time: 0.72s\n",
      "Epoch: 9, Batch: 331/842, Loss: 0.6037, Time: 0.72s\n",
      "Epoch: 9, Batch: 341/842, Loss: 0.4917, Time: 0.72s\n",
      "Epoch: 9, Batch: 351/842, Loss: 0.5304, Time: 0.68s\n",
      "Epoch: 9, Batch: 361/842, Loss: 0.6408, Time: 0.72s\n",
      "Epoch: 9, Batch: 371/842, Loss: 0.5444, Time: 0.95s\n",
      "Epoch: 9, Batch: 381/842, Loss: 0.4532, Time: 0.85s\n",
      "Epoch: 9, Batch: 391/842, Loss: 0.8517, Time: 0.79s\n",
      "Epoch: 9, Batch: 401/842, Loss: 0.7195, Time: 0.72s\n",
      "Epoch: 9, Batch: 411/842, Loss: 0.5812, Time: 0.72s\n",
      "Epoch: 9, Batch: 421/842, Loss: 0.5805, Time: 0.72s\n",
      "Epoch: 9, Batch: 431/842, Loss: 0.6880, Time: 0.73s\n",
      "Epoch: 9, Batch: 441/842, Loss: 0.5354, Time: 0.73s\n",
      "Epoch: 9, Batch: 451/842, Loss: 0.7904, Time: 0.67s\n",
      "Epoch: 9, Batch: 461/842, Loss: 0.6019, Time: 0.78s\n",
      "Epoch: 9, Batch: 471/842, Loss: 0.4808, Time: 0.82s\n",
      "Epoch: 9, Batch: 481/842, Loss: 0.6511, Time: 0.84s\n",
      "Epoch: 9, Batch: 491/842, Loss: 0.5283, Time: 0.82s\n",
      "Epoch: 9, Batch: 501/842, Loss: 0.4951, Time: 0.80s\n",
      "Epoch: 9, Batch: 511/842, Loss: 0.7744, Time: 0.84s\n",
      "Epoch: 9, Batch: 521/842, Loss: 0.7227, Time: 0.91s\n",
      "Epoch: 9, Batch: 531/842, Loss: 0.6363, Time: 0.80s\n",
      "Epoch: 9, Batch: 541/842, Loss: 0.7508, Time: 0.79s\n",
      "Epoch: 9, Batch: 551/842, Loss: 0.6332, Time: 0.79s\n",
      "Epoch: 9, Batch: 561/842, Loss: 0.5400, Time: 0.78s\n",
      "Epoch: 9, Batch: 571/842, Loss: 0.5472, Time: 0.81s\n",
      "Epoch: 9, Batch: 581/842, Loss: 0.5722, Time: 0.76s\n",
      "Epoch: 9, Batch: 591/842, Loss: 0.6512, Time: 0.73s\n",
      "Epoch: 9, Batch: 601/842, Loss: 0.6356, Time: 0.75s\n",
      "Epoch: 9, Batch: 611/842, Loss: 0.5627, Time: 0.74s\n",
      "Epoch: 9, Batch: 621/842, Loss: 0.6077, Time: 0.73s\n",
      "Epoch: 9, Batch: 631/842, Loss: 0.5227, Time: 0.78s\n",
      "Epoch: 9, Batch: 641/842, Loss: 0.6567, Time: 0.79s\n",
      "Epoch: 9, Batch: 651/842, Loss: 0.5702, Time: 0.76s\n",
      "Epoch: 9, Batch: 661/842, Loss: 0.7446, Time: 0.75s\n",
      "Epoch: 9, Batch: 671/842, Loss: 0.5311, Time: 0.73s\n",
      "Epoch: 9, Batch: 681/842, Loss: 0.4635, Time: 0.74s\n",
      "Epoch: 9, Batch: 691/842, Loss: 0.6739, Time: 0.75s\n",
      "Epoch: 9, Batch: 701/842, Loss: 0.6221, Time: 0.80s\n",
      "Epoch: 9, Batch: 711/842, Loss: 0.5137, Time: 0.76s\n",
      "Epoch: 9, Batch: 721/842, Loss: 0.5058, Time: 0.74s\n",
      "Epoch: 9, Batch: 731/842, Loss: 0.4773, Time: 0.75s\n",
      "Epoch: 9, Batch: 741/842, Loss: 0.6467, Time: 0.80s\n",
      "Epoch: 9, Batch: 751/842, Loss: 0.6742, Time: 0.77s\n",
      "Epoch: 9, Batch: 761/842, Loss: 0.5634, Time: 0.84s\n",
      "Epoch: 9, Batch: 771/842, Loss: 0.6312, Time: 0.76s\n",
      "Epoch: 9, Batch: 781/842, Loss: 0.7358, Time: 0.74s\n",
      "Epoch: 9, Batch: 791/842, Loss: 0.5543, Time: 0.74s\n",
      "Epoch: 9, Batch: 801/842, Loss: 0.5331, Time: 0.75s\n",
      "Epoch: 9, Batch: 811/842, Loss: 0.5836, Time: 0.73s\n",
      "Epoch: 9, Batch: 821/842, Loss: 0.4922, Time: 0.74s\n",
      "Epoch: 9, Batch: 831/842, Loss: 0.6660, Time: 0.75s\n",
      "Epoch: 9, Batch: 841/842, Loss: 0.7060, Time: 0.73s\n",
      "Epoch 9/100: Train Loss: 0.6075, Val Loss: 0.5593, mIoU: 0.5960, F1: 0.7360, OA: 0.9543\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7297, F1=0.8437, OA=0.8376, Precision=0.7869, Recall=0.9093\n",
      "    Class 1: IoU=0.6080, F1=0.7562, OA=0.9953, Precision=0.8377, Recall=0.6892\n",
      "    Class 2: IoU=0.8808, F1=0.9366, OA=0.9748, Precision=0.9547, Recall=0.9192\n",
      "    Class 3: IoU=0.3824, F1=0.5533, OA=0.9559, Precision=0.7213, Recall=0.4487\n",
      "    Class 4: IoU=0.6785, F1=0.8085, OA=0.9750, Precision=0.8452, Recall=0.7748\n",
      "    Class 5: IoU=0.4790, F1=0.6477, OA=0.9937, Precision=0.6456, Recall=0.6498\n",
      "    Class 6: IoU=0.4976, F1=0.6646, OA=0.9465, Precision=0.7284, Recall=0.6110\n",
      "    Class 7: IoU=0.5123, F1=0.6775, OA=0.9558, Precision=0.8144, Recall=0.5800\n",
      "Saving best model with mIoU: 0.5960\n",
      "Epoch: 10, Batch: 1/842, Loss: 0.6934, Time: 1.59s\n",
      "Epoch: 10, Batch: 11/842, Loss: 0.7187, Time: 0.87s\n",
      "Epoch: 10, Batch: 21/842, Loss: 0.5316, Time: 0.77s\n",
      "Epoch: 10, Batch: 31/842, Loss: 0.5563, Time: 0.76s\n",
      "Epoch: 10, Batch: 41/842, Loss: 0.8843, Time: 0.75s\n",
      "Epoch: 10, Batch: 51/842, Loss: 0.4325, Time: 0.76s\n",
      "Epoch: 10, Batch: 61/842, Loss: 0.6814, Time: 0.80s\n",
      "Epoch: 10, Batch: 71/842, Loss: 0.6049, Time: 0.81s\n",
      "Epoch: 10, Batch: 81/842, Loss: 0.6235, Time: 0.74s\n",
      "Epoch: 10, Batch: 91/842, Loss: 0.4494, Time: 0.75s\n",
      "Epoch: 10, Batch: 101/842, Loss: 0.5620, Time: 0.76s\n",
      "Epoch: 10, Batch: 111/842, Loss: 0.5932, Time: 0.80s\n",
      "Epoch: 10, Batch: 121/842, Loss: 0.6955, Time: 0.73s\n",
      "Epoch: 10, Batch: 131/842, Loss: 0.5610, Time: 0.76s\n",
      "Epoch: 10, Batch: 141/842, Loss: 0.5834, Time: 0.73s\n",
      "Epoch: 10, Batch: 151/842, Loss: 0.5000, Time: 0.72s\n",
      "Epoch: 10, Batch: 161/842, Loss: 0.6108, Time: 0.74s\n",
      "Epoch: 10, Batch: 171/842, Loss: 0.5928, Time: 0.77s\n",
      "Epoch: 10, Batch: 181/842, Loss: 0.5515, Time: 0.80s\n",
      "Epoch: 10, Batch: 191/842, Loss: 0.4927, Time: 0.77s\n",
      "Epoch: 10, Batch: 201/842, Loss: 0.4097, Time: 0.78s\n",
      "Epoch: 10, Batch: 211/842, Loss: 0.6034, Time: 0.80s\n",
      "Epoch: 10, Batch: 221/842, Loss: 0.6961, Time: 0.79s\n",
      "Epoch: 10, Batch: 231/842, Loss: 0.4511, Time: 0.78s\n",
      "Epoch: 10, Batch: 241/842, Loss: 0.5529, Time: 0.79s\n",
      "Epoch: 10, Batch: 251/842, Loss: 0.6643, Time: 0.82s\n",
      "Epoch: 10, Batch: 261/842, Loss: 0.6482, Time: 0.78s\n",
      "Epoch: 10, Batch: 271/842, Loss: 0.6478, Time: 0.77s\n",
      "Epoch: 10, Batch: 281/842, Loss: 0.5144, Time: 0.80s\n",
      "Epoch: 10, Batch: 291/842, Loss: 0.6505, Time: 0.73s\n",
      "Epoch: 10, Batch: 301/842, Loss: 0.6177, Time: 0.78s\n",
      "Epoch: 10, Batch: 311/842, Loss: 0.5704, Time: 0.76s\n",
      "Epoch: 10, Batch: 321/842, Loss: 0.5400, Time: 0.76s\n",
      "Epoch: 10, Batch: 331/842, Loss: 0.6037, Time: 0.78s\n",
      "Epoch: 10, Batch: 341/842, Loss: 0.6342, Time: 0.79s\n",
      "Epoch: 10, Batch: 351/842, Loss: 0.8118, Time: 0.83s\n",
      "Epoch: 10, Batch: 361/842, Loss: 0.4582, Time: 0.81s\n",
      "Epoch: 10, Batch: 371/842, Loss: 0.5814, Time: 0.79s\n",
      "Epoch: 10, Batch: 381/842, Loss: 0.6460, Time: 0.77s\n",
      "Epoch: 10, Batch: 391/842, Loss: 0.6267, Time: 0.81s\n",
      "Epoch: 10, Batch: 401/842, Loss: 0.5830, Time: 0.78s\n",
      "Epoch: 10, Batch: 411/842, Loss: 0.4399, Time: 0.78s\n",
      "Epoch: 10, Batch: 421/842, Loss: 0.5861, Time: 0.73s\n",
      "Epoch: 10, Batch: 431/842, Loss: 0.6465, Time: 0.70s\n",
      "Epoch: 10, Batch: 441/842, Loss: 0.4768, Time: 0.72s\n",
      "Epoch: 10, Batch: 451/842, Loss: 0.5473, Time: 0.85s\n",
      "Epoch: 10, Batch: 461/842, Loss: 0.4893, Time: 0.81s\n",
      "Epoch: 10, Batch: 471/842, Loss: 0.5049, Time: 0.87s\n",
      "Epoch: 10, Batch: 481/842, Loss: 0.4550, Time: 0.83s\n",
      "Epoch: 10, Batch: 491/842, Loss: 0.5754, Time: 0.79s\n",
      "Epoch: 10, Batch: 501/842, Loss: 0.5230, Time: 0.78s\n",
      "Epoch: 10, Batch: 511/842, Loss: 0.4624, Time: 0.77s\n",
      "Epoch: 10, Batch: 521/842, Loss: 0.6507, Time: 0.78s\n",
      "Epoch: 10, Batch: 531/842, Loss: 0.5903, Time: 0.82s\n",
      "Epoch: 10, Batch: 541/842, Loss: 0.5395, Time: 0.79s\n",
      "Epoch: 10, Batch: 551/842, Loss: 0.5108, Time: 0.78s\n",
      "Epoch: 10, Batch: 561/842, Loss: 0.6461, Time: 0.77s\n",
      "Epoch: 10, Batch: 571/842, Loss: 0.5636, Time: 0.79s\n",
      "Epoch: 10, Batch: 581/842, Loss: 0.6882, Time: 0.80s\n",
      "Epoch: 10, Batch: 591/842, Loss: 0.5896, Time: 0.76s\n",
      "Epoch: 10, Batch: 601/842, Loss: 0.4761, Time: 0.78s\n",
      "Epoch: 10, Batch: 611/842, Loss: 0.4631, Time: 0.77s\n",
      "Epoch: 10, Batch: 621/842, Loss: 0.5616, Time: 0.77s\n",
      "Epoch: 10, Batch: 631/842, Loss: 0.5678, Time: 0.78s\n",
      "Epoch: 10, Batch: 641/842, Loss: 0.6997, Time: 0.88s\n",
      "Epoch: 10, Batch: 651/842, Loss: 0.7088, Time: 0.88s\n",
      "Epoch: 10, Batch: 661/842, Loss: 0.5374, Time: 0.92s\n",
      "Epoch: 10, Batch: 671/842, Loss: 0.5139, Time: 0.88s\n",
      "Epoch: 10, Batch: 681/842, Loss: 0.4633, Time: 0.86s\n",
      "Epoch: 10, Batch: 691/842, Loss: 0.6660, Time: 0.82s\n",
      "Epoch: 10, Batch: 701/842, Loss: 0.7344, Time: 0.75s\n",
      "Epoch: 10, Batch: 711/842, Loss: 0.6625, Time: 0.77s\n",
      "Epoch: 10, Batch: 721/842, Loss: 0.5894, Time: 0.81s\n",
      "Epoch: 10, Batch: 731/842, Loss: 0.6038, Time: 0.82s\n",
      "Epoch: 10, Batch: 741/842, Loss: 0.5961, Time: 0.82s\n",
      "Epoch: 10, Batch: 751/842, Loss: 0.8088, Time: 0.82s\n",
      "Epoch: 10, Batch: 761/842, Loss: 0.6000, Time: 0.85s\n",
      "Epoch: 10, Batch: 771/842, Loss: 0.8618, Time: 0.81s\n",
      "Epoch: 10, Batch: 781/842, Loss: 0.5653, Time: 0.85s\n",
      "Epoch: 10, Batch: 791/842, Loss: 0.8676, Time: 0.82s\n",
      "Epoch: 10, Batch: 801/842, Loss: 0.6702, Time: 0.80s\n",
      "Epoch: 10, Batch: 811/842, Loss: 0.6180, Time: 0.82s\n",
      "Epoch: 10, Batch: 821/842, Loss: 0.5928, Time: 0.86s\n",
      "Epoch: 10, Batch: 831/842, Loss: 0.6306, Time: 0.82s\n",
      "Epoch: 10, Batch: 841/842, Loss: 0.4720, Time: 0.85s\n",
      "Epoch 10/100: Train Loss: 0.5993, Val Loss: 0.5879, mIoU: 0.5640, F1: 0.7094, OA: 0.9512\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7232, F1=0.8394, OA=0.8355, Precision=0.7928, Recall=0.8917\n",
      "    Class 1: IoU=0.5172, F1=0.6818, OA=0.9930, Precision=0.6602, Recall=0.7047\n",
      "    Class 2: IoU=0.8446, F1=0.9158, OA=0.9675, Precision=0.9654, Recall=0.8710\n",
      "    Class 3: IoU=0.3847, F1=0.5556, OA=0.9535, Precision=0.6639, Recall=0.4777\n",
      "    Class 4: IoU=0.6541, F1=0.7909, OA=0.9710, Precision=0.7785, Recall=0.8038\n",
      "    Class 5: IoU=0.3859, F1=0.5569, OA=0.9901, Precision=0.4620, Recall=0.7008\n",
      "    Class 6: IoU=0.4881, F1=0.6560, OA=0.9442, Precision=0.7046, Recall=0.6137\n",
      "    Class 7: IoU=0.5140, F1=0.6790, OA=0.9549, Precision=0.7894, Recall=0.5957\n",
      "Epoch: 11, Batch: 1/842, Loss: 0.5290, Time: 1.43s\n",
      "Epoch: 11, Batch: 11/842, Loss: 0.5385, Time: 0.81s\n",
      "Epoch: 11, Batch: 21/842, Loss: 0.5836, Time: 0.79s\n",
      "Epoch: 11, Batch: 31/842, Loss: 0.5916, Time: 0.84s\n",
      "Epoch: 11, Batch: 41/842, Loss: 0.6713, Time: 0.86s\n",
      "Epoch: 11, Batch: 51/842, Loss: 0.4647, Time: 0.84s\n",
      "Epoch: 11, Batch: 61/842, Loss: 0.6382, Time: 0.73s\n",
      "Epoch: 11, Batch: 71/842, Loss: 0.6355, Time: 0.73s\n",
      "Epoch: 11, Batch: 81/842, Loss: 0.5243, Time: 0.71s\n",
      "Epoch: 11, Batch: 91/842, Loss: 0.6884, Time: 0.69s\n",
      "Epoch: 11, Batch: 101/842, Loss: 0.4856, Time: 0.71s\n",
      "Epoch: 11, Batch: 111/842, Loss: 0.5047, Time: 0.75s\n",
      "Epoch: 11, Batch: 121/842, Loss: 0.6817, Time: 0.74s\n",
      "Epoch: 11, Batch: 131/842, Loss: 0.4708, Time: 0.74s\n",
      "Epoch: 11, Batch: 141/842, Loss: 0.5895, Time: 0.82s\n",
      "Epoch: 11, Batch: 151/842, Loss: 0.5721, Time: 0.77s\n",
      "Epoch: 11, Batch: 161/842, Loss: 0.6662, Time: 0.69s\n",
      "Epoch: 11, Batch: 171/842, Loss: 0.5929, Time: 0.72s\n",
      "Epoch: 11, Batch: 181/842, Loss: 0.5881, Time: 0.74s\n",
      "Epoch: 11, Batch: 191/842, Loss: 0.6081, Time: 0.76s\n",
      "Epoch: 11, Batch: 201/842, Loss: 0.6333, Time: 0.80s\n",
      "Epoch: 11, Batch: 211/842, Loss: 0.5593, Time: 0.82s\n",
      "Epoch: 11, Batch: 221/842, Loss: 0.6676, Time: 0.80s\n",
      "Epoch: 11, Batch: 231/842, Loss: 0.5472, Time: 0.76s\n",
      "Epoch: 11, Batch: 241/842, Loss: 0.6867, Time: 0.74s\n",
      "Epoch: 11, Batch: 251/842, Loss: 0.3981, Time: 0.75s\n",
      "Epoch: 11, Batch: 261/842, Loss: 0.6365, Time: 0.74s\n",
      "Epoch: 11, Batch: 271/842, Loss: 0.5089, Time: 0.75s\n",
      "Epoch: 11, Batch: 281/842, Loss: 0.6135, Time: 0.73s\n",
      "Epoch: 11, Batch: 291/842, Loss: 0.6363, Time: 0.74s\n",
      "Epoch: 11, Batch: 301/842, Loss: 0.5833, Time: 0.73s\n",
      "Epoch: 11, Batch: 311/842, Loss: 0.7771, Time: 0.73s\n",
      "Epoch: 11, Batch: 321/842, Loss: 0.5333, Time: 0.74s\n",
      "Epoch: 11, Batch: 331/842, Loss: 0.5972, Time: 0.73s\n",
      "Epoch: 11, Batch: 341/842, Loss: 0.6663, Time: 0.73s\n",
      "Epoch: 11, Batch: 351/842, Loss: 0.6706, Time: 0.72s\n",
      "Epoch: 11, Batch: 361/842, Loss: 0.7730, Time: 0.74s\n",
      "Epoch: 11, Batch: 371/842, Loss: 0.5360, Time: 0.74s\n",
      "Epoch: 11, Batch: 381/842, Loss: 0.5880, Time: 0.75s\n",
      "Epoch: 11, Batch: 391/842, Loss: 0.5800, Time: 0.76s\n",
      "Epoch: 11, Batch: 401/842, Loss: 0.6262, Time: 0.73s\n",
      "Epoch: 11, Batch: 411/842, Loss: 0.6251, Time: 0.73s\n",
      "Epoch: 11, Batch: 421/842, Loss: 0.5941, Time: 0.76s\n",
      "Epoch: 11, Batch: 431/842, Loss: 0.4036, Time: 0.79s\n",
      "Epoch: 11, Batch: 441/842, Loss: 0.3963, Time: 0.79s\n",
      "Epoch: 11, Batch: 451/842, Loss: 0.4262, Time: 0.76s\n",
      "Epoch: 11, Batch: 461/842, Loss: 0.7461, Time: 0.76s\n",
      "Epoch: 11, Batch: 471/842, Loss: 0.5307, Time: 0.76s\n",
      "Epoch: 11, Batch: 481/842, Loss: 0.6289, Time: 0.76s\n",
      "Epoch: 11, Batch: 491/842, Loss: 0.3766, Time: 0.82s\n",
      "Epoch: 11, Batch: 501/842, Loss: 0.6834, Time: 0.86s\n",
      "Epoch: 11, Batch: 511/842, Loss: 0.6297, Time: 0.86s\n",
      "Epoch: 11, Batch: 521/842, Loss: 0.5259, Time: 0.89s\n",
      "Epoch: 11, Batch: 531/842, Loss: 0.4644, Time: 0.86s\n",
      "Epoch: 11, Batch: 541/842, Loss: 0.5122, Time: 0.84s\n",
      "Epoch: 11, Batch: 551/842, Loss: 0.5169, Time: 0.70s\n",
      "Epoch: 11, Batch: 561/842, Loss: 0.7612, Time: 0.69s\n",
      "Epoch: 11, Batch: 571/842, Loss: 0.6255, Time: 0.73s\n",
      "Epoch: 11, Batch: 581/842, Loss: 0.6363, Time: 0.77s\n",
      "Epoch: 11, Batch: 591/842, Loss: 0.6746, Time: 0.80s\n",
      "Epoch: 11, Batch: 601/842, Loss: 0.5308, Time: 0.93s\n",
      "Epoch: 11, Batch: 611/842, Loss: 0.5477, Time: 0.84s\n",
      "Epoch: 11, Batch: 621/842, Loss: 0.5552, Time: 0.80s\n",
      "Epoch: 11, Batch: 631/842, Loss: 0.5628, Time: 0.80s\n",
      "Epoch: 11, Batch: 641/842, Loss: 0.5541, Time: 0.79s\n",
      "Epoch: 11, Batch: 651/842, Loss: 0.3873, Time: 0.77s\n",
      "Epoch: 11, Batch: 661/842, Loss: 0.4764, Time: 0.73s\n",
      "Epoch: 11, Batch: 671/842, Loss: 0.4176, Time: 0.73s\n",
      "Epoch: 11, Batch: 681/842, Loss: 0.4818, Time: 0.71s\n",
      "Epoch: 11, Batch: 691/842, Loss: 0.9043, Time: 0.71s\n",
      "Epoch: 11, Batch: 701/842, Loss: 0.6413, Time: 0.77s\n",
      "Epoch: 11, Batch: 711/842, Loss: 0.6565, Time: 0.73s\n",
      "Epoch: 11, Batch: 721/842, Loss: 0.5421, Time: 0.75s\n",
      "Epoch: 11, Batch: 731/842, Loss: 0.6291, Time: 0.68s\n",
      "Epoch: 11, Batch: 741/842, Loss: 0.5250, Time: 0.77s\n",
      "Epoch: 11, Batch: 751/842, Loss: 0.7906, Time: 0.73s\n",
      "Epoch: 11, Batch: 761/842, Loss: 0.5601, Time: 0.75s\n",
      "Epoch: 11, Batch: 771/842, Loss: 0.7057, Time: 0.78s\n",
      "Epoch: 11, Batch: 781/842, Loss: 0.5619, Time: 0.77s\n",
      "Epoch: 11, Batch: 791/842, Loss: 0.6137, Time: 0.87s\n",
      "Epoch: 11, Batch: 801/842, Loss: 0.5334, Time: 0.83s\n",
      "Epoch: 11, Batch: 811/842, Loss: 0.5915, Time: 0.83s\n",
      "Epoch: 11, Batch: 821/842, Loss: 0.5480, Time: 0.83s\n",
      "Epoch: 11, Batch: 831/842, Loss: 0.4596, Time: 0.81s\n",
      "Epoch: 11, Batch: 841/842, Loss: 0.4508, Time: 0.83s\n",
      "Epoch 11/100: Train Loss: 0.5893, Val Loss: 0.5999, mIoU: 0.5627, F1: 0.7063, OA: 0.9509\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7171, F1=0.8352, OA=0.8247, Precision=0.7638, Recall=0.9214\n",
      "    Class 1: IoU=0.5702, F1=0.7263, OA=0.9944, Precision=0.7508, Recall=0.7033\n",
      "    Class 2: IoU=0.8630, F1=0.9265, OA=0.9713, Precision=0.9633, Recall=0.8923\n",
      "    Class 3: IoU=0.3166, F1=0.4809, OA=0.9558, Precision=0.8387, Recall=0.3371\n",
      "    Class 4: IoU=0.6491, F1=0.7873, OA=0.9706, Precision=0.7775, Recall=0.7973\n",
      "    Class 5: IoU=0.4498, F1=0.6205, OA=0.9932, Precision=0.6210, Recall=0.6201\n",
      "    Class 6: IoU=0.4420, F1=0.6131, OA=0.9470, Precision=0.8348, Recall=0.4844\n",
      "    Class 7: IoU=0.4936, F1=0.6609, OA=0.9501, Precision=0.7242, Recall=0.6078\n",
      "Epoch: 12, Batch: 1/842, Loss: 0.4383, Time: 1.51s\n",
      "Epoch: 12, Batch: 11/842, Loss: 0.5555, Time: 0.75s\n",
      "Epoch: 12, Batch: 21/842, Loss: 0.5091, Time: 0.73s\n",
      "Epoch: 12, Batch: 31/842, Loss: 0.4722, Time: 0.74s\n",
      "Epoch: 12, Batch: 41/842, Loss: 0.5508, Time: 0.75s\n",
      "Epoch: 12, Batch: 51/842, Loss: 0.3973, Time: 0.76s\n",
      "Epoch: 12, Batch: 61/842, Loss: 0.7050, Time: 0.75s\n",
      "Epoch: 12, Batch: 71/842, Loss: 0.4785, Time: 0.77s\n",
      "Epoch: 12, Batch: 81/842, Loss: 0.5330, Time: 0.77s\n",
      "Epoch: 12, Batch: 91/842, Loss: 0.4640, Time: 0.80s\n",
      "Epoch: 12, Batch: 101/842, Loss: 0.5603, Time: 0.79s\n",
      "Epoch: 12, Batch: 111/842, Loss: 0.4608, Time: 0.73s\n",
      "Epoch: 12, Batch: 121/842, Loss: 0.3636, Time: 0.73s\n",
      "Epoch: 12, Batch: 131/842, Loss: 0.5901, Time: 0.72s\n",
      "Epoch: 12, Batch: 141/842, Loss: 0.5763, Time: 0.72s\n",
      "Epoch: 12, Batch: 151/842, Loss: 0.5023, Time: 0.73s\n",
      "Epoch: 12, Batch: 161/842, Loss: 0.4653, Time: 0.90s\n",
      "Epoch: 12, Batch: 171/842, Loss: 0.5354, Time: 0.93s\n",
      "Epoch: 12, Batch: 181/842, Loss: 0.5641, Time: 0.87s\n",
      "Epoch: 12, Batch: 191/842, Loss: 0.6439, Time: 0.79s\n",
      "Epoch: 12, Batch: 201/842, Loss: 0.6547, Time: 0.75s\n",
      "Epoch: 12, Batch: 211/842, Loss: 0.4209, Time: 0.76s\n",
      "Epoch: 12, Batch: 221/842, Loss: 0.4065, Time: 0.74s\n",
      "Epoch: 12, Batch: 231/842, Loss: 0.5200, Time: 0.77s\n",
      "Epoch: 12, Batch: 241/842, Loss: 0.6570, Time: 0.76s\n",
      "Epoch: 12, Batch: 251/842, Loss: 0.3815, Time: 0.86s\n",
      "Epoch: 12, Batch: 261/842, Loss: 0.6484, Time: 0.85s\n",
      "Epoch: 12, Batch: 271/842, Loss: 0.6001, Time: 0.86s\n",
      "Epoch: 12, Batch: 281/842, Loss: 0.5533, Time: 0.81s\n",
      "Epoch: 12, Batch: 291/842, Loss: 0.5197, Time: 0.80s\n",
      "Epoch: 12, Batch: 301/842, Loss: 0.5175, Time: 0.75s\n",
      "Epoch: 12, Batch: 311/842, Loss: 0.6787, Time: 0.74s\n",
      "Epoch: 12, Batch: 321/842, Loss: 0.5448, Time: 0.73s\n",
      "Epoch: 12, Batch: 331/842, Loss: 0.6914, Time: 0.76s\n",
      "Epoch: 12, Batch: 341/842, Loss: 0.4228, Time: 0.78s\n",
      "Epoch: 12, Batch: 351/842, Loss: 0.5296, Time: 0.75s\n",
      "Epoch: 12, Batch: 361/842, Loss: 0.6399, Time: 0.76s\n",
      "Epoch: 12, Batch: 371/842, Loss: 0.5671, Time: 0.78s\n",
      "Epoch: 12, Batch: 381/842, Loss: 0.5277, Time: 0.75s\n",
      "Epoch: 12, Batch: 391/842, Loss: 0.4724, Time: 0.75s\n",
      "Epoch: 12, Batch: 401/842, Loss: 0.6650, Time: 0.80s\n",
      "Epoch: 12, Batch: 411/842, Loss: 0.5987, Time: 0.82s\n",
      "Epoch: 12, Batch: 421/842, Loss: 0.5185, Time: 0.76s\n",
      "Epoch: 12, Batch: 431/842, Loss: 0.3770, Time: 0.82s\n",
      "Epoch: 12, Batch: 441/842, Loss: 0.4318, Time: 0.82s\n",
      "Epoch: 12, Batch: 451/842, Loss: 0.6192, Time: 0.79s\n",
      "Epoch: 12, Batch: 461/842, Loss: 0.6195, Time: 0.75s\n",
      "Epoch: 12, Batch: 471/842, Loss: 0.4760, Time: 0.73s\n",
      "Epoch: 12, Batch: 481/842, Loss: 0.4454, Time: 0.76s\n",
      "Epoch: 12, Batch: 491/842, Loss: 0.7090, Time: 0.83s\n",
      "Epoch: 12, Batch: 501/842, Loss: 0.7208, Time: 0.89s\n",
      "Epoch: 12, Batch: 511/842, Loss: 0.4614, Time: 0.89s\n",
      "Epoch: 12, Batch: 521/842, Loss: 0.5334, Time: 0.77s\n",
      "Epoch: 12, Batch: 531/842, Loss: 0.7037, Time: 0.76s\n",
      "Epoch: 12, Batch: 541/842, Loss: 0.6939, Time: 0.77s\n",
      "Epoch: 12, Batch: 551/842, Loss: 0.5800, Time: 0.84s\n",
      "Epoch: 12, Batch: 561/842, Loss: 0.4241, Time: 0.92s\n",
      "Epoch: 12, Batch: 571/842, Loss: 0.4982, Time: 0.80s\n",
      "Epoch: 12, Batch: 581/842, Loss: 0.3895, Time: 0.79s\n",
      "Epoch: 12, Batch: 591/842, Loss: 0.6683, Time: 0.80s\n",
      "Epoch: 12, Batch: 601/842, Loss: 0.6138, Time: 0.76s\n",
      "Epoch: 12, Batch: 611/842, Loss: 0.5436, Time: 0.76s\n",
      "Epoch: 12, Batch: 621/842, Loss: 1.0246, Time: 0.75s\n",
      "Epoch: 12, Batch: 631/842, Loss: 0.6172, Time: 0.77s\n",
      "Epoch: 12, Batch: 641/842, Loss: 0.5415, Time: 0.77s\n",
      "Epoch: 12, Batch: 651/842, Loss: 0.4931, Time: 0.77s\n",
      "Epoch: 12, Batch: 661/842, Loss: 0.6437, Time: 0.78s\n",
      "Epoch: 12, Batch: 671/842, Loss: 0.5967, Time: 0.75s\n",
      "Epoch: 12, Batch: 681/842, Loss: 0.6883, Time: 0.74s\n",
      "Epoch: 12, Batch: 691/842, Loss: 0.5091, Time: 0.74s\n",
      "Epoch: 12, Batch: 701/842, Loss: 0.6989, Time: 0.73s\n",
      "Epoch: 12, Batch: 711/842, Loss: 0.5435, Time: 0.73s\n",
      "Epoch: 12, Batch: 721/842, Loss: 0.5188, Time: 0.75s\n",
      "Epoch: 12, Batch: 731/842, Loss: 0.5450, Time: 0.73s\n",
      "Epoch: 12, Batch: 741/842, Loss: 0.6515, Time: 0.75s\n",
      "Epoch: 12, Batch: 751/842, Loss: 0.6355, Time: 0.75s\n",
      "Epoch: 12, Batch: 761/842, Loss: 0.4347, Time: 0.83s\n",
      "Epoch: 12, Batch: 771/842, Loss: 0.8173, Time: 0.78s\n",
      "Epoch: 12, Batch: 781/842, Loss: 0.6177, Time: 0.76s\n",
      "Epoch: 12, Batch: 791/842, Loss: 0.4927, Time: 0.73s\n",
      "Epoch: 12, Batch: 801/842, Loss: 0.6423, Time: 0.73s\n",
      "Epoch: 12, Batch: 811/842, Loss: 0.6306, Time: 0.73s\n",
      "Epoch: 12, Batch: 821/842, Loss: 0.8593, Time: 0.73s\n",
      "Epoch: 12, Batch: 831/842, Loss: 0.7386, Time: 0.72s\n",
      "Epoch: 12, Batch: 841/842, Loss: 0.5212, Time: 0.72s\n",
      "Epoch 12/100: Train Loss: 0.5835, Val Loss: 0.5494, mIoU: 0.6011, F1: 0.7401, OA: 0.9547\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7280, F1=0.8426, OA=0.8405, Precision=0.8039, Recall=0.8853\n",
      "    Class 1: IoU=0.6273, F1=0.7709, OA=0.9954, Precision=0.8183, Recall=0.7288\n",
      "    Class 2: IoU=0.8850, F1=0.9390, OA=0.9752, Precision=0.9367, Recall=0.9413\n",
      "    Class 3: IoU=0.3632, F1=0.5329, OA=0.9567, Precision=0.7754, Recall=0.4059\n",
      "    Class 4: IoU=0.6514, F1=0.7889, OA=0.9695, Precision=0.7469, Recall=0.8358\n",
      "    Class 5: IoU=0.4952, F1=0.6624, OA=0.9940, Precision=0.6678, Recall=0.6571\n",
      "    Class 6: IoU=0.5084, F1=0.6740, OA=0.9524, Precision=0.8290, Recall=0.5679\n",
      "    Class 7: IoU=0.5501, F1=0.7098, OA=0.9537, Precision=0.7114, Recall=0.7081\n",
      "Saving best model with mIoU: 0.6011\n",
      "Epoch: 13, Batch: 1/842, Loss: 0.6720, Time: 1.50s\n",
      "Epoch: 13, Batch: 11/842, Loss: 0.5736, Time: 0.72s\n",
      "Epoch: 13, Batch: 21/842, Loss: 0.5954, Time: 0.74s\n",
      "Epoch: 13, Batch: 31/842, Loss: 0.7309, Time: 0.72s\n",
      "Epoch: 13, Batch: 41/842, Loss: 0.6664, Time: 0.74s\n",
      "Epoch: 13, Batch: 51/842, Loss: 0.6212, Time: 0.76s\n",
      "Epoch: 13, Batch: 61/842, Loss: 0.6113, Time: 0.76s\n",
      "Epoch: 13, Batch: 71/842, Loss: 0.6945, Time: 0.75s\n",
      "Epoch: 13, Batch: 81/842, Loss: 0.5354, Time: 0.76s\n",
      "Epoch: 13, Batch: 91/842, Loss: 0.5113, Time: 0.75s\n",
      "Epoch: 13, Batch: 101/842, Loss: 0.4903, Time: 0.76s\n",
      "Epoch: 13, Batch: 111/842, Loss: 0.4274, Time: 0.80s\n",
      "Epoch: 13, Batch: 121/842, Loss: 0.5873, Time: 0.81s\n",
      "Epoch: 13, Batch: 131/842, Loss: 0.6167, Time: 0.81s\n",
      "Epoch: 13, Batch: 141/842, Loss: 0.5533, Time: 0.83s\n",
      "Epoch: 13, Batch: 151/842, Loss: 0.6240, Time: 0.80s\n",
      "Epoch: 13, Batch: 161/842, Loss: 0.4380, Time: 0.80s\n",
      "Epoch: 13, Batch: 171/842, Loss: 0.4939, Time: 0.80s\n",
      "Epoch: 13, Batch: 181/842, Loss: 0.4906, Time: 0.80s\n",
      "Epoch: 13, Batch: 191/842, Loss: 0.5472, Time: 0.81s\n",
      "Epoch: 13, Batch: 201/842, Loss: 0.6969, Time: 0.89s\n",
      "Epoch: 13, Batch: 211/842, Loss: 0.3647, Time: 0.84s\n",
      "Epoch: 13, Batch: 221/842, Loss: 0.4826, Time: 0.73s\n",
      "Epoch: 13, Batch: 231/842, Loss: 0.6641, Time: 0.75s\n",
      "Epoch: 13, Batch: 241/842, Loss: 0.5564, Time: 0.69s\n",
      "Epoch: 13, Batch: 251/842, Loss: 0.5949, Time: 0.74s\n",
      "Epoch: 13, Batch: 261/842, Loss: 0.5696, Time: 0.79s\n",
      "Epoch: 13, Batch: 271/842, Loss: 0.5569, Time: 0.84s\n",
      "Epoch: 13, Batch: 281/842, Loss: 0.6455, Time: 0.81s\n",
      "Epoch: 13, Batch: 291/842, Loss: 0.5925, Time: 0.72s\n",
      "Epoch: 13, Batch: 301/842, Loss: 0.5237, Time: 0.74s\n",
      "Epoch: 13, Batch: 311/842, Loss: 0.5581, Time: 0.73s\n",
      "Epoch: 13, Batch: 321/842, Loss: 0.6487, Time: 0.73s\n",
      "Epoch: 13, Batch: 331/842, Loss: 0.6117, Time: 0.77s\n",
      "Epoch: 13, Batch: 341/842, Loss: 0.6755, Time: 0.78s\n",
      "Epoch: 13, Batch: 351/842, Loss: 0.5559, Time: 0.80s\n",
      "Epoch: 13, Batch: 361/842, Loss: 0.6718, Time: 0.78s\n",
      "Epoch: 13, Batch: 371/842, Loss: 0.7970, Time: 0.76s\n",
      "Epoch: 13, Batch: 381/842, Loss: 0.6839, Time: 0.77s\n",
      "Epoch: 13, Batch: 391/842, Loss: 0.4883, Time: 0.77s\n",
      "Epoch: 13, Batch: 401/842, Loss: 0.4865, Time: 0.77s\n",
      "Epoch: 13, Batch: 411/842, Loss: 0.5212, Time: 0.76s\n",
      "Epoch: 13, Batch: 421/842, Loss: 0.4949, Time: 0.75s\n",
      "Epoch: 13, Batch: 431/842, Loss: 0.6249, Time: 0.74s\n",
      "Epoch: 13, Batch: 441/842, Loss: 0.6066, Time: 0.75s\n",
      "Epoch: 13, Batch: 451/842, Loss: 0.5849, Time: 0.74s\n",
      "Epoch: 13, Batch: 461/842, Loss: 0.6323, Time: 0.72s\n",
      "Epoch: 13, Batch: 471/842, Loss: 0.4943, Time: 0.73s\n",
      "Epoch: 13, Batch: 481/842, Loss: 0.6890, Time: 0.73s\n",
      "Epoch: 13, Batch: 491/842, Loss: 0.8407, Time: 0.69s\n",
      "Epoch: 13, Batch: 501/842, Loss: 0.6577, Time: 0.69s\n",
      "Epoch: 13, Batch: 511/842, Loss: 0.5400, Time: 0.69s\n",
      "Epoch: 13, Batch: 521/842, Loss: 0.5987, Time: 0.68s\n",
      "Epoch: 13, Batch: 531/842, Loss: 0.6984, Time: 0.69s\n",
      "Epoch: 13, Batch: 541/842, Loss: 0.4974, Time: 0.70s\n",
      "Epoch: 13, Batch: 551/842, Loss: 0.5305, Time: 0.85s\n",
      "Epoch: 13, Batch: 561/842, Loss: 0.5933, Time: 0.79s\n",
      "Epoch: 13, Batch: 571/842, Loss: 0.4434, Time: 0.71s\n",
      "Epoch: 13, Batch: 581/842, Loss: 0.4450, Time: 0.72s\n",
      "Epoch: 13, Batch: 591/842, Loss: 0.4172, Time: 0.71s\n",
      "Epoch: 13, Batch: 601/842, Loss: 0.7197, Time: 0.76s\n",
      "Epoch: 13, Batch: 611/842, Loss: 0.5509, Time: 0.75s\n",
      "Epoch: 13, Batch: 621/842, Loss: 0.6663, Time: 0.76s\n",
      "Epoch: 13, Batch: 631/842, Loss: 0.7387, Time: 0.74s\n",
      "Epoch: 13, Batch: 641/842, Loss: 0.6277, Time: 0.75s\n",
      "Epoch: 13, Batch: 651/842, Loss: 0.5472, Time: 0.75s\n",
      "Epoch: 13, Batch: 661/842, Loss: 0.5823, Time: 0.70s\n",
      "Epoch: 13, Batch: 671/842, Loss: 0.4802, Time: 0.71s\n",
      "Epoch: 13, Batch: 681/842, Loss: 0.6605, Time: 0.69s\n",
      "Epoch: 13, Batch: 691/842, Loss: 0.5583, Time: 0.69s\n",
      "Epoch: 13, Batch: 701/842, Loss: 0.5299, Time: 0.70s\n",
      "Epoch: 13, Batch: 711/842, Loss: 0.5005, Time: 0.68s\n",
      "Epoch: 13, Batch: 721/842, Loss: 0.6196, Time: 0.67s\n",
      "Epoch: 13, Batch: 731/842, Loss: 0.5441, Time: 0.68s\n",
      "Epoch: 13, Batch: 741/842, Loss: 0.5289, Time: 0.72s\n",
      "Epoch: 13, Batch: 751/842, Loss: 0.5555, Time: 0.71s\n",
      "Epoch: 13, Batch: 761/842, Loss: 0.6489, Time: 0.71s\n",
      "Epoch: 13, Batch: 771/842, Loss: 0.6805, Time: 0.72s\n",
      "Epoch: 13, Batch: 781/842, Loss: 0.5276, Time: 0.73s\n",
      "Epoch: 13, Batch: 791/842, Loss: 0.6655, Time: 0.70s\n",
      "Epoch: 13, Batch: 801/842, Loss: 0.5611, Time: 0.69s\n",
      "Epoch: 13, Batch: 811/842, Loss: 0.6516, Time: 0.70s\n",
      "Epoch: 13, Batch: 821/842, Loss: 0.5561, Time: 0.70s\n",
      "Epoch: 13, Batch: 831/842, Loss: 0.6303, Time: 0.71s\n",
      "Epoch: 13, Batch: 841/842, Loss: 0.4867, Time: 0.70s\n",
      "Epoch 13/100: Train Loss: 0.5787, Val Loss: 0.5599, mIoU: 0.5736, F1: 0.7126, OA: 0.9539\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7269, F1=0.8419, OA=0.8363, Precision=0.7877, Recall=0.9040\n",
      "    Class 1: IoU=0.6265, F1=0.7703, OA=0.9955, Precision=0.8500, Recall=0.7043\n",
      "    Class 2: IoU=0.8869, F1=0.9401, OA=0.9754, Precision=0.9274, Recall=0.9531\n",
      "    Class 3: IoU=0.3701, F1=0.5402, OA=0.9563, Precision=0.7508, Recall=0.4219\n",
      "    Class 4: IoU=0.6627, F1=0.7971, OA=0.9743, Precision=0.8624, Recall=0.7410\n",
      "    Class 5: IoU=0.3112, F1=0.4747, OA=0.9935, Precision=0.8517, Recall=0.3291\n",
      "    Class 6: IoU=0.4788, F1=0.6476, OA=0.9430, Precision=0.6978, Recall=0.6041\n",
      "    Class 7: IoU=0.5257, F1=0.6892, OA=0.9570, Precision=0.8172, Recall=0.5958\n",
      "Epoch: 14, Batch: 1/842, Loss: 0.6098, Time: 1.70s\n",
      "Epoch: 14, Batch: 11/842, Loss: 0.5049, Time: 0.81s\n",
      "Epoch: 14, Batch: 21/842, Loss: 0.7836, Time: 0.74s\n",
      "Epoch: 14, Batch: 31/842, Loss: 0.5424, Time: 0.73s\n",
      "Epoch: 14, Batch: 41/842, Loss: 0.5104, Time: 0.73s\n",
      "Epoch: 14, Batch: 51/842, Loss: 0.7398, Time: 0.78s\n",
      "Epoch: 14, Batch: 61/842, Loss: 0.5015, Time: 0.82s\n",
      "Epoch: 14, Batch: 71/842, Loss: 0.5692, Time: 0.79s\n",
      "Epoch: 14, Batch: 81/842, Loss: 0.4815, Time: 0.76s\n",
      "Epoch: 14, Batch: 91/842, Loss: 0.5148, Time: 0.76s\n",
      "Epoch: 14, Batch: 101/842, Loss: 0.5565, Time: 0.79s\n",
      "Epoch: 14, Batch: 111/842, Loss: 0.5785, Time: 0.75s\n",
      "Epoch: 14, Batch: 121/842, Loss: 0.6334, Time: 0.73s\n",
      "Epoch: 14, Batch: 131/842, Loss: 0.5875, Time: 0.74s\n",
      "Epoch: 14, Batch: 141/842, Loss: 0.7023, Time: 0.82s\n",
      "Epoch: 14, Batch: 151/842, Loss: 0.5889, Time: 0.79s\n",
      "Epoch: 14, Batch: 161/842, Loss: 0.6561, Time: 0.81s\n",
      "Epoch: 14, Batch: 171/842, Loss: 0.6141, Time: 0.78s\n",
      "Epoch: 14, Batch: 181/842, Loss: 0.5937, Time: 0.73s\n",
      "Epoch: 14, Batch: 191/842, Loss: 0.5241, Time: 0.72s\n",
      "Epoch: 14, Batch: 201/842, Loss: 0.4744, Time: 0.72s\n",
      "Epoch: 14, Batch: 211/842, Loss: 0.6314, Time: 0.73s\n",
      "Epoch: 14, Batch: 221/842, Loss: 0.6582, Time: 0.72s\n",
      "Epoch: 14, Batch: 231/842, Loss: 0.7779, Time: 0.81s\n",
      "Epoch: 14, Batch: 241/842, Loss: 0.6448, Time: 0.83s\n",
      "Epoch: 14, Batch: 251/842, Loss: 0.6711, Time: 0.72s\n",
      "Epoch: 14, Batch: 261/842, Loss: 0.5658, Time: 0.72s\n",
      "Epoch: 14, Batch: 271/842, Loss: 0.6185, Time: 0.73s\n",
      "Epoch: 14, Batch: 281/842, Loss: 0.3502, Time: 0.72s\n",
      "Epoch: 14, Batch: 291/842, Loss: 0.7295, Time: 0.72s\n",
      "Epoch: 14, Batch: 301/842, Loss: 0.5928, Time: 0.71s\n",
      "Epoch: 14, Batch: 311/842, Loss: 0.3517, Time: 0.68s\n",
      "Epoch: 14, Batch: 321/842, Loss: 0.7363, Time: 0.67s\n",
      "Epoch: 14, Batch: 331/842, Loss: 0.4825, Time: 0.68s\n",
      "Epoch: 14, Batch: 341/842, Loss: 0.6664, Time: 0.80s\n",
      "Epoch: 14, Batch: 351/842, Loss: 0.6617, Time: 0.82s\n",
      "Epoch: 14, Batch: 361/842, Loss: 0.4584, Time: 0.84s\n",
      "Epoch: 14, Batch: 371/842, Loss: 0.6697, Time: 0.83s\n",
      "Epoch: 14, Batch: 381/842, Loss: 0.7130, Time: 0.82s\n",
      "Epoch: 14, Batch: 391/842, Loss: 0.5546, Time: 0.79s\n",
      "Epoch: 14, Batch: 401/842, Loss: 0.5652, Time: 0.76s\n",
      "Epoch: 14, Batch: 411/842, Loss: 0.6100, Time: 0.78s\n",
      "Epoch: 14, Batch: 421/842, Loss: 0.4749, Time: 0.80s\n",
      "Epoch: 14, Batch: 431/842, Loss: 0.4085, Time: 0.83s\n",
      "Epoch: 14, Batch: 441/842, Loss: 0.8134, Time: 0.78s\n",
      "Epoch: 14, Batch: 451/842, Loss: 0.4877, Time: 0.80s\n",
      "Epoch: 14, Batch: 461/842, Loss: 0.6877, Time: 0.77s\n",
      "Epoch: 14, Batch: 471/842, Loss: 0.5334, Time: 0.74s\n",
      "Epoch: 14, Batch: 481/842, Loss: 0.6573, Time: 0.76s\n",
      "Epoch: 14, Batch: 491/842, Loss: 0.7190, Time: 1.25s\n",
      "Epoch: 14, Batch: 501/842, Loss: 0.7178, Time: 0.74s\n",
      "Epoch: 14, Batch: 511/842, Loss: 0.5110, Time: 0.75s\n",
      "Epoch: 14, Batch: 521/842, Loss: 0.4726, Time: 0.74s\n",
      "Epoch: 14, Batch: 531/842, Loss: 0.4575, Time: 0.73s\n",
      "Epoch: 14, Batch: 541/842, Loss: 0.7675, Time: 0.75s\n",
      "Epoch: 14, Batch: 551/842, Loss: 0.4204, Time: 0.72s\n",
      "Epoch: 14, Batch: 561/842, Loss: 0.4831, Time: 0.71s\n",
      "Epoch: 14, Batch: 571/842, Loss: 0.6071, Time: 0.71s\n",
      "Epoch: 14, Batch: 581/842, Loss: 0.4162, Time: 0.71s\n",
      "Epoch: 14, Batch: 591/842, Loss: 0.4506, Time: 0.72s\n",
      "Epoch: 14, Batch: 601/842, Loss: 0.5816, Time: 0.71s\n",
      "Epoch: 14, Batch: 611/842, Loss: 0.5250, Time: 0.71s\n",
      "Epoch: 14, Batch: 621/842, Loss: 0.4004, Time: 0.72s\n",
      "Epoch: 14, Batch: 631/842, Loss: 0.6410, Time: 0.71s\n",
      "Epoch: 14, Batch: 641/842, Loss: 0.4944, Time: 0.70s\n",
      "Epoch: 14, Batch: 651/842, Loss: 0.4661, Time: 0.71s\n",
      "Epoch: 14, Batch: 661/842, Loss: 0.5708, Time: 0.71s\n",
      "Epoch: 14, Batch: 671/842, Loss: 0.5881, Time: 0.71s\n",
      "Epoch: 14, Batch: 681/842, Loss: 0.4837, Time: 0.71s\n",
      "Epoch: 14, Batch: 691/842, Loss: 0.6065, Time: 0.71s\n",
      "Epoch: 14, Batch: 701/842, Loss: 0.6455, Time: 0.71s\n",
      "Epoch: 14, Batch: 711/842, Loss: 0.5174, Time: 0.72s\n",
      "Epoch: 14, Batch: 721/842, Loss: 0.4853, Time: 0.72s\n",
      "Epoch: 14, Batch: 731/842, Loss: 0.4887, Time: 0.72s\n",
      "Epoch: 14, Batch: 741/842, Loss: 0.3972, Time: 0.76s\n",
      "Epoch: 14, Batch: 751/842, Loss: 0.5975, Time: 0.82s\n",
      "Epoch: 14, Batch: 761/842, Loss: 0.5112, Time: 0.82s\n",
      "Epoch: 14, Batch: 771/842, Loss: 0.4575, Time: 0.82s\n",
      "Epoch: 14, Batch: 781/842, Loss: 0.8218, Time: 0.82s\n",
      "Epoch: 14, Batch: 791/842, Loss: 0.4670, Time: 0.79s\n",
      "Epoch: 14, Batch: 801/842, Loss: 0.3941, Time: 0.80s\n",
      "Epoch: 14, Batch: 811/842, Loss: 0.5757, Time: 0.76s\n",
      "Epoch: 14, Batch: 821/842, Loss: 0.5713, Time: 0.79s\n",
      "Epoch: 14, Batch: 831/842, Loss: 0.6475, Time: 0.77s\n",
      "Epoch: 14, Batch: 841/842, Loss: 0.5962, Time: 0.71s\n",
      "Epoch 14/100: Train Loss: 0.5754, Val Loss: 0.5904, mIoU: 0.5600, F1: 0.7035, OA: 0.9511\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7258, F1=0.8411, OA=0.8310, Precision=0.7694, Recall=0.9276\n",
      "    Class 1: IoU=0.5636, F1=0.7209, OA=0.9949, Precision=0.8551, Recall=0.6231\n",
      "    Class 2: IoU=0.8860, F1=0.9396, OA=0.9755, Precision=0.9402, Recall=0.9389\n",
      "    Class 3: IoU=0.3897, F1=0.5608, OA=0.9569, Precision=0.7368, Recall=0.4527\n",
      "    Class 4: IoU=0.6326, F1=0.7750, OA=0.9725, Precision=0.8741, Recall=0.6960\n",
      "    Class 5: IoU=0.4773, F1=0.6462, OA=0.9932, Precision=0.6007, Recall=0.6990\n",
      "    Class 6: IoU=0.4460, F1=0.6168, OA=0.9387, Precision=0.6729, Recall=0.5694\n",
      "    Class 7: IoU=0.3587, F1=0.5280, OA=0.9465, Precision=0.8993, Recall=0.3737\n",
      "Epoch: 15, Batch: 1/842, Loss: 0.4515, Time: 1.53s\n",
      "Epoch: 15, Batch: 11/842, Loss: 0.5361, Time: 0.74s\n",
      "Epoch: 15, Batch: 21/842, Loss: 0.4948, Time: 0.73s\n",
      "Epoch: 15, Batch: 31/842, Loss: 0.6437, Time: 0.76s\n",
      "Epoch: 15, Batch: 41/842, Loss: 0.5937, Time: 0.78s\n",
      "Epoch: 15, Batch: 51/842, Loss: 0.5319, Time: 0.85s\n",
      "Epoch: 15, Batch: 61/842, Loss: 0.6428, Time: 0.77s\n",
      "Epoch: 15, Batch: 71/842, Loss: 0.5403, Time: 0.70s\n",
      "Epoch: 15, Batch: 81/842, Loss: 0.4635, Time: 0.68s\n",
      "Epoch: 15, Batch: 91/842, Loss: 0.6088, Time: 0.69s\n",
      "Epoch: 15, Batch: 101/842, Loss: 0.6484, Time: 0.73s\n",
      "Epoch: 15, Batch: 111/842, Loss: 0.4280, Time: 0.73s\n",
      "Epoch: 15, Batch: 121/842, Loss: 0.4244, Time: 0.73s\n",
      "Epoch: 15, Batch: 131/842, Loss: 0.5894, Time: 0.74s\n",
      "Epoch: 15, Batch: 141/842, Loss: 0.5766, Time: 0.73s\n",
      "Epoch: 15, Batch: 151/842, Loss: 0.6064, Time: 0.72s\n",
      "Epoch: 15, Batch: 161/842, Loss: 0.7888, Time: 0.72s\n",
      "Epoch: 15, Batch: 171/842, Loss: 0.4610, Time: 0.72s\n",
      "Epoch: 15, Batch: 181/842, Loss: 0.5025, Time: 0.71s\n",
      "Epoch: 15, Batch: 191/842, Loss: 0.6296, Time: 0.72s\n",
      "Epoch: 15, Batch: 201/842, Loss: 0.5525, Time: 0.71s\n",
      "Epoch: 15, Batch: 211/842, Loss: 0.5405, Time: 0.73s\n",
      "Epoch: 15, Batch: 221/842, Loss: 0.4251, Time: 0.71s\n",
      "Epoch: 15, Batch: 231/842, Loss: 0.5085, Time: 0.71s\n",
      "Epoch: 15, Batch: 241/842, Loss: 0.5373, Time: 0.72s\n",
      "Epoch: 15, Batch: 251/842, Loss: 0.6319, Time: 0.71s\n",
      "Epoch: 15, Batch: 261/842, Loss: 0.6200, Time: 0.72s\n",
      "Epoch: 15, Batch: 271/842, Loss: 0.4823, Time: 0.73s\n",
      "Epoch: 15, Batch: 281/842, Loss: 0.3769, Time: 0.72s\n",
      "Epoch: 15, Batch: 291/842, Loss: 0.5334, Time: 0.71s\n",
      "Epoch: 15, Batch: 301/842, Loss: 0.5723, Time: 0.74s\n",
      "Epoch: 15, Batch: 311/842, Loss: 0.5310, Time: 0.73s\n",
      "Epoch: 15, Batch: 321/842, Loss: 0.6323, Time: 0.78s\n",
      "Epoch: 15, Batch: 331/842, Loss: 0.6696, Time: 0.79s\n",
      "Epoch: 15, Batch: 341/842, Loss: 0.5815, Time: 0.73s\n",
      "Epoch: 15, Batch: 351/842, Loss: 0.6366, Time: 0.77s\n",
      "Epoch: 15, Batch: 361/842, Loss: 0.3557, Time: 0.78s\n",
      "Epoch: 15, Batch: 371/842, Loss: 0.6241, Time: 0.79s\n",
      "Epoch: 15, Batch: 381/842, Loss: 0.4421, Time: 0.74s\n",
      "Epoch: 15, Batch: 391/842, Loss: 0.5612, Time: 0.75s\n",
      "Epoch: 15, Batch: 401/842, Loss: 0.5027, Time: 0.76s\n",
      "Epoch: 15, Batch: 411/842, Loss: 0.5197, Time: 0.75s\n",
      "Epoch: 15, Batch: 421/842, Loss: 0.5470, Time: 0.74s\n",
      "Epoch: 15, Batch: 431/842, Loss: 0.5600, Time: 0.73s\n",
      "Epoch: 15, Batch: 441/842, Loss: 0.6785, Time: 0.72s\n",
      "Epoch: 15, Batch: 451/842, Loss: 0.5484, Time: 0.72s\n",
      "Epoch: 15, Batch: 461/842, Loss: 0.7378, Time: 0.72s\n",
      "Epoch: 15, Batch: 471/842, Loss: 0.4260, Time: 0.72s\n",
      "Epoch: 15, Batch: 481/842, Loss: 0.7569, Time: 0.72s\n",
      "Epoch: 15, Batch: 491/842, Loss: 0.5833, Time: 0.73s\n",
      "Epoch: 15, Batch: 501/842, Loss: 0.5687, Time: 0.74s\n",
      "Epoch: 15, Batch: 511/842, Loss: 0.5879, Time: 0.73s\n",
      "Epoch: 15, Batch: 521/842, Loss: 0.6509, Time: 0.71s\n",
      "Epoch: 15, Batch: 531/842, Loss: 0.5448, Time: 0.71s\n",
      "Epoch: 15, Batch: 541/842, Loss: 0.5947, Time: 0.71s\n",
      "Epoch: 15, Batch: 551/842, Loss: 0.5391, Time: 0.71s\n",
      "Epoch: 15, Batch: 561/842, Loss: 0.6069, Time: 0.70s\n",
      "Epoch: 15, Batch: 571/842, Loss: 0.5744, Time: 0.72s\n",
      "Epoch: 15, Batch: 581/842, Loss: 0.5463, Time: 0.71s\n",
      "Epoch: 15, Batch: 591/842, Loss: 0.5268, Time: 0.71s\n",
      "Epoch: 15, Batch: 601/842, Loss: 0.6830, Time: 0.71s\n",
      "Epoch: 15, Batch: 611/842, Loss: 0.5863, Time: 0.71s\n",
      "Epoch: 15, Batch: 621/842, Loss: 0.3937, Time: 0.71s\n",
      "Epoch: 15, Batch: 631/842, Loss: 0.6138, Time: 0.71s\n",
      "Epoch: 15, Batch: 641/842, Loss: 0.4409, Time: 0.72s\n",
      "Epoch: 15, Batch: 651/842, Loss: 0.7273, Time: 0.74s\n",
      "Epoch: 15, Batch: 661/842, Loss: 0.4145, Time: 0.73s\n",
      "Epoch: 15, Batch: 671/842, Loss: 0.6773, Time: 0.76s\n",
      "Epoch: 15, Batch: 681/842, Loss: 0.6250, Time: 0.70s\n",
      "Epoch: 15, Batch: 691/842, Loss: 0.4794, Time: 0.70s\n",
      "Epoch: 15, Batch: 701/842, Loss: 0.4113, Time: 0.69s\n",
      "Epoch: 15, Batch: 711/842, Loss: 0.7358, Time: 0.71s\n",
      "Epoch: 15, Batch: 721/842, Loss: 0.5827, Time: 0.69s\n",
      "Epoch: 15, Batch: 731/842, Loss: 0.4776, Time: 0.70s\n",
      "Epoch: 15, Batch: 741/842, Loss: 0.5159, Time: 0.69s\n",
      "Epoch: 15, Batch: 751/842, Loss: 0.4912, Time: 0.68s\n",
      "Epoch: 15, Batch: 761/842, Loss: 0.6354, Time: 0.69s\n",
      "Epoch: 15, Batch: 771/842, Loss: 0.7400, Time: 0.68s\n",
      "Epoch: 15, Batch: 781/842, Loss: 0.5242, Time: 0.69s\n",
      "Epoch: 15, Batch: 791/842, Loss: 0.4107, Time: 0.68s\n",
      "Epoch: 15, Batch: 801/842, Loss: 0.5886, Time: 0.68s\n",
      "Epoch: 15, Batch: 811/842, Loss: 0.4845, Time: 0.69s\n",
      "Epoch: 15, Batch: 821/842, Loss: 0.7325, Time: 0.68s\n",
      "Epoch: 15, Batch: 831/842, Loss: 0.4804, Time: 0.67s\n",
      "Epoch: 15, Batch: 841/842, Loss: 0.5619, Time: 0.67s\n",
      "Epoch 15/100: Train Loss: 0.5653, Val Loss: 0.5352, mIoU: 0.6032, F1: 0.7410, OA: 0.9559\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7388, F1=0.8498, OA=0.8442, Precision=0.7941, Recall=0.9139\n",
      "    Class 1: IoU=0.6159, F1=0.7623, OA=0.9952, Precision=0.7958, Recall=0.7314\n",
      "    Class 2: IoU=0.8939, F1=0.9440, OA=0.9774, Precision=0.9501, Recall=0.9380\n",
      "    Class 3: IoU=0.3785, F1=0.5491, OA=0.9566, Precision=0.7454, Recall=0.4347\n",
      "    Class 4: IoU=0.6942, F1=0.8195, OA=0.9767, Precision=0.8672, Recall=0.7768\n",
      "    Class 5: IoU=0.4883, F1=0.6562, OA=0.9945, Precision=0.7389, Recall=0.5902\n",
      "    Class 6: IoU=0.5040, F1=0.6702, OA=0.9457, Precision=0.7083, Recall=0.6360\n",
      "    Class 7: IoU=0.5120, F1=0.6773, OA=0.9568, Precision=0.8419, Recall=0.5665\n",
      "Saving best model with mIoU: 0.6032\n",
      "Epoch: 16, Batch: 1/842, Loss: 0.7289, Time: 1.60s\n",
      "Epoch: 16, Batch: 11/842, Loss: 0.5160, Time: 0.75s\n",
      "Epoch: 16, Batch: 21/842, Loss: 0.4171, Time: 0.77s\n",
      "Epoch: 16, Batch: 31/842, Loss: 0.4732, Time: 0.77s\n",
      "Epoch: 16, Batch: 41/842, Loss: 0.5449, Time: 0.76s\n",
      "Epoch: 16, Batch: 51/842, Loss: 0.5160, Time: 0.75s\n",
      "Epoch: 16, Batch: 61/842, Loss: 0.6028, Time: 0.73s\n",
      "Epoch: 16, Batch: 71/842, Loss: 0.5110, Time: 0.78s\n",
      "Epoch: 16, Batch: 81/842, Loss: 0.7387, Time: 0.82s\n",
      "Epoch: 16, Batch: 91/842, Loss: 0.5725, Time: 0.84s\n",
      "Epoch: 16, Batch: 101/842, Loss: 0.4376, Time: 0.82s\n",
      "Epoch: 16, Batch: 111/842, Loss: 0.5650, Time: 0.81s\n",
      "Epoch: 16, Batch: 121/842, Loss: 0.4725, Time: 0.82s\n",
      "Epoch: 16, Batch: 131/842, Loss: 0.4906, Time: 0.84s\n",
      "Epoch: 16, Batch: 141/842, Loss: 0.5272, Time: 0.84s\n",
      "Epoch: 16, Batch: 151/842, Loss: 0.5671, Time: 0.81s\n",
      "Epoch: 16, Batch: 161/842, Loss: 0.5303, Time: 0.84s\n",
      "Epoch: 16, Batch: 171/842, Loss: 0.4422, Time: 0.82s\n",
      "Epoch: 16, Batch: 181/842, Loss: 0.4136, Time: 0.81s\n",
      "Epoch: 16, Batch: 191/842, Loss: 0.7071, Time: 0.81s\n",
      "Epoch: 16, Batch: 201/842, Loss: 0.5231, Time: 0.80s\n",
      "Epoch: 16, Batch: 211/842, Loss: 0.4785, Time: 0.77s\n",
      "Epoch: 16, Batch: 221/842, Loss: 0.5234, Time: 0.76s\n",
      "Epoch: 16, Batch: 231/842, Loss: 0.6034, Time: 0.77s\n",
      "Epoch: 16, Batch: 241/842, Loss: 0.5205, Time: 0.76s\n",
      "Epoch: 16, Batch: 251/842, Loss: 0.6886, Time: 0.77s\n",
      "Epoch: 16, Batch: 261/842, Loss: 0.5613, Time: 0.81s\n",
      "Epoch: 16, Batch: 271/842, Loss: 0.4370, Time: 0.80s\n",
      "Epoch: 16, Batch: 281/842, Loss: 0.9776, Time: 0.69s\n",
      "Epoch: 16, Batch: 291/842, Loss: 0.6303, Time: 0.70s\n",
      "Epoch: 16, Batch: 301/842, Loss: 0.7346, Time: 0.71s\n",
      "Epoch: 16, Batch: 311/842, Loss: 0.6860, Time: 0.71s\n",
      "Epoch: 16, Batch: 321/842, Loss: 0.5687, Time: 0.71s\n",
      "Epoch: 16, Batch: 331/842, Loss: 0.8482, Time: 0.70s\n",
      "Epoch: 16, Batch: 341/842, Loss: 0.5978, Time: 0.71s\n",
      "Epoch: 16, Batch: 351/842, Loss: 0.4460, Time: 0.70s\n",
      "Epoch: 16, Batch: 361/842, Loss: 0.5096, Time: 0.76s\n",
      "Epoch: 16, Batch: 371/842, Loss: 0.6261, Time: 0.79s\n",
      "Epoch: 16, Batch: 381/842, Loss: 0.4541, Time: 0.83s\n",
      "Epoch: 16, Batch: 391/842, Loss: 0.6109, Time: 0.79s\n",
      "Epoch: 16, Batch: 401/842, Loss: 0.4493, Time: 0.82s\n",
      "Epoch: 16, Batch: 411/842, Loss: 0.4780, Time: 0.79s\n",
      "Epoch: 16, Batch: 421/842, Loss: 0.5106, Time: 0.76s\n",
      "Epoch: 16, Batch: 431/842, Loss: 0.5069, Time: 0.77s\n",
      "Epoch: 16, Batch: 441/842, Loss: 0.5085, Time: 0.80s\n",
      "Epoch: 16, Batch: 451/842, Loss: 0.7057, Time: 0.75s\n",
      "Epoch: 16, Batch: 461/842, Loss: 0.6651, Time: 0.76s\n",
      "Epoch: 16, Batch: 471/842, Loss: 0.9311, Time: 0.76s\n",
      "Epoch: 16, Batch: 481/842, Loss: 0.4855, Time: 0.77s\n",
      "Epoch: 16, Batch: 491/842, Loss: 0.6240, Time: 0.78s\n",
      "Epoch: 16, Batch: 501/842, Loss: 0.6469, Time: 0.76s\n",
      "Epoch: 16, Batch: 511/842, Loss: 0.4948, Time: 0.75s\n",
      "Epoch: 16, Batch: 521/842, Loss: 0.5784, Time: 0.78s\n",
      "Epoch: 16, Batch: 531/842, Loss: 0.5452, Time: 0.75s\n",
      "Epoch: 16, Batch: 541/842, Loss: 0.4715, Time: 0.76s\n",
      "Epoch: 16, Batch: 551/842, Loss: 0.3962, Time: 0.83s\n",
      "Epoch: 16, Batch: 561/842, Loss: 0.4487, Time: 0.81s\n",
      "Epoch: 16, Batch: 571/842, Loss: 0.5595, Time: 0.77s\n",
      "Epoch: 16, Batch: 581/842, Loss: 0.3875, Time: 0.76s\n",
      "Epoch: 16, Batch: 591/842, Loss: 0.5523, Time: 0.78s\n",
      "Epoch: 16, Batch: 601/842, Loss: 0.5361, Time: 0.82s\n",
      "Epoch: 16, Batch: 611/842, Loss: 0.7105, Time: 0.78s\n",
      "Epoch: 16, Batch: 621/842, Loss: 0.5456, Time: 0.72s\n",
      "Epoch: 16, Batch: 631/842, Loss: 0.3939, Time: 0.70s\n",
      "Epoch: 16, Batch: 641/842, Loss: 0.5693, Time: 0.70s\n",
      "Epoch: 16, Batch: 651/842, Loss: 0.4769, Time: 0.72s\n",
      "Epoch: 16, Batch: 661/842, Loss: 0.6040, Time: 0.77s\n",
      "Epoch: 16, Batch: 671/842, Loss: 0.5090, Time: 0.74s\n",
      "Epoch: 16, Batch: 681/842, Loss: 0.4333, Time: 0.76s\n",
      "Epoch: 16, Batch: 691/842, Loss: 0.3854, Time: 0.75s\n",
      "Epoch: 16, Batch: 701/842, Loss: 0.4682, Time: 0.75s\n",
      "Epoch: 16, Batch: 711/842, Loss: 0.4747, Time: 0.74s\n",
      "Epoch: 16, Batch: 721/842, Loss: 0.5902, Time: 0.76s\n",
      "Epoch: 16, Batch: 731/842, Loss: 0.6620, Time: 0.74s\n",
      "Epoch: 16, Batch: 741/842, Loss: 0.4927, Time: 0.74s\n",
      "Epoch: 16, Batch: 751/842, Loss: 0.5637, Time: 0.74s\n",
      "Epoch: 16, Batch: 761/842, Loss: 0.4812, Time: 0.77s\n",
      "Epoch: 16, Batch: 771/842, Loss: 0.6813, Time: 0.74s\n",
      "Epoch: 16, Batch: 781/842, Loss: 0.4760, Time: 0.74s\n",
      "Epoch: 16, Batch: 791/842, Loss: 0.5084, Time: 0.73s\n",
      "Epoch: 16, Batch: 801/842, Loss: 0.5619, Time: 0.76s\n",
      "Epoch: 16, Batch: 811/842, Loss: 0.5051, Time: 0.74s\n",
      "Epoch: 16, Batch: 821/842, Loss: 0.7634, Time: 0.74s\n",
      "Epoch: 16, Batch: 831/842, Loss: 0.4984, Time: 0.74s\n",
      "Epoch: 16, Batch: 841/842, Loss: 0.5545, Time: 0.74s\n",
      "Epoch 16/100: Train Loss: 0.5540, Val Loss: 0.5829, mIoU: 0.5668, F1: 0.7074, OA: 0.9515\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7224, F1=0.8389, OA=0.8326, Precision=0.7826, Recall=0.9039\n",
      "    Class 1: IoU=0.6297, F1=0.7728, OA=0.9954, Precision=0.8202, Recall=0.7305\n",
      "    Class 2: IoU=0.8731, F1=0.9322, OA=0.9734, Precision=0.9644, Recall=0.9022\n",
      "    Class 3: IoU=0.4096, F1=0.5811, OA=0.9537, Precision=0.6454, Recall=0.5285\n",
      "    Class 4: IoU=0.6768, F1=0.8072, OA=0.9735, Precision=0.8004, Recall=0.8142\n",
      "    Class 5: IoU=0.3159, F1=0.4801, OA=0.9858, Precision=0.3561, Recall=0.7364\n",
      "    Class 6: IoU=0.4742, F1=0.6433, OA=0.9462, Precision=0.7572, Recall=0.5592\n",
      "    Class 7: IoU=0.4327, F1=0.6040, OA=0.9515, Precision=0.8709, Recall=0.4623\n",
      "Epoch: 17, Batch: 1/842, Loss: 0.6152, Time: 1.58s\n",
      "Epoch: 17, Batch: 11/842, Loss: 0.5681, Time: 0.72s\n",
      "Epoch: 17, Batch: 21/842, Loss: 0.4272, Time: 0.75s\n",
      "Epoch: 17, Batch: 31/842, Loss: 0.4290, Time: 0.77s\n",
      "Epoch: 17, Batch: 41/842, Loss: 0.7671, Time: 0.77s\n",
      "Epoch: 17, Batch: 51/842, Loss: 0.6995, Time: 0.76s\n",
      "Epoch: 17, Batch: 61/842, Loss: 0.4215, Time: 0.79s\n",
      "Epoch: 17, Batch: 71/842, Loss: 0.7651, Time: 0.78s\n",
      "Epoch: 17, Batch: 81/842, Loss: 0.6733, Time: 0.77s\n",
      "Epoch: 17, Batch: 91/842, Loss: 0.6266, Time: 0.76s\n",
      "Epoch: 17, Batch: 101/842, Loss: 0.5532, Time: 0.73s\n",
      "Epoch: 17, Batch: 111/842, Loss: 0.5049, Time: 0.75s\n",
      "Epoch: 17, Batch: 121/842, Loss: 0.5666, Time: 0.75s\n",
      "Epoch: 17, Batch: 131/842, Loss: 0.7030, Time: 0.73s\n",
      "Epoch: 17, Batch: 141/842, Loss: 0.5132, Time: 0.74s\n",
      "Epoch: 17, Batch: 151/842, Loss: 0.5224, Time: 0.75s\n",
      "Epoch: 17, Batch: 161/842, Loss: 0.6684, Time: 0.76s\n",
      "Epoch: 17, Batch: 171/842, Loss: 0.4696, Time: 0.80s\n",
      "Epoch: 17, Batch: 181/842, Loss: 0.4217, Time: 0.80s\n",
      "Epoch: 17, Batch: 191/842, Loss: 0.6533, Time: 0.75s\n",
      "Epoch: 17, Batch: 201/842, Loss: 0.6667, Time: 0.75s\n",
      "Epoch: 17, Batch: 211/842, Loss: 0.5595, Time: 0.74s\n",
      "Epoch: 17, Batch: 221/842, Loss: 0.5734, Time: 0.73s\n",
      "Epoch: 17, Batch: 231/842, Loss: 0.5951, Time: 0.74s\n",
      "Epoch: 17, Batch: 241/842, Loss: 0.5200, Time: 0.78s\n",
      "Epoch: 17, Batch: 251/842, Loss: 0.5600, Time: 0.76s\n",
      "Epoch: 17, Batch: 261/842, Loss: 0.4629, Time: 0.75s\n",
      "Epoch: 17, Batch: 271/842, Loss: 0.5260, Time: 0.74s\n",
      "Epoch: 17, Batch: 281/842, Loss: 0.5991, Time: 0.71s\n",
      "Epoch: 17, Batch: 291/842, Loss: 0.6110, Time: 0.76s\n",
      "Epoch: 17, Batch: 301/842, Loss: 0.6778, Time: 0.77s\n",
      "Epoch: 17, Batch: 311/842, Loss: 0.5204, Time: 0.74s\n",
      "Epoch: 17, Batch: 321/842, Loss: 0.5478, Time: 0.75s\n",
      "Epoch: 17, Batch: 331/842, Loss: 0.5782, Time: 0.77s\n",
      "Epoch: 17, Batch: 341/842, Loss: 0.4257, Time: 0.75s\n",
      "Epoch: 17, Batch: 351/842, Loss: 0.5663, Time: 0.77s\n",
      "Epoch: 17, Batch: 361/842, Loss: 0.7063, Time: 0.74s\n",
      "Epoch: 17, Batch: 371/842, Loss: 0.5787, Time: 0.74s\n",
      "Epoch: 17, Batch: 381/842, Loss: 0.6050, Time: 0.74s\n",
      "Epoch: 17, Batch: 391/842, Loss: 0.5570, Time: 0.74s\n",
      "Epoch: 17, Batch: 401/842, Loss: 0.4974, Time: 0.74s\n",
      "Epoch: 17, Batch: 411/842, Loss: 0.6365, Time: 0.73s\n",
      "Epoch: 17, Batch: 421/842, Loss: 0.5608, Time: 0.73s\n",
      "Epoch: 17, Batch: 431/842, Loss: 0.6727, Time: 0.74s\n",
      "Epoch: 17, Batch: 441/842, Loss: 0.5942, Time: 0.73s\n",
      "Epoch: 17, Batch: 451/842, Loss: 0.5948, Time: 0.78s\n",
      "Epoch: 17, Batch: 461/842, Loss: 0.5235, Time: 0.75s\n",
      "Epoch: 17, Batch: 471/842, Loss: 0.7204, Time: 0.75s\n",
      "Epoch: 17, Batch: 481/842, Loss: 0.6084, Time: 0.75s\n",
      "Epoch: 17, Batch: 491/842, Loss: 0.5324, Time: 0.72s\n",
      "Epoch: 17, Batch: 501/842, Loss: 0.5919, Time: 0.71s\n",
      "Epoch: 17, Batch: 511/842, Loss: 0.6001, Time: 0.72s\n",
      "Epoch: 17, Batch: 521/842, Loss: 0.4896, Time: 0.72s\n",
      "Epoch: 17, Batch: 531/842, Loss: 0.7045, Time: 0.72s\n",
      "Epoch: 17, Batch: 541/842, Loss: 0.7324, Time: 0.74s\n",
      "Epoch: 17, Batch: 551/842, Loss: 0.6493, Time: 0.72s\n",
      "Epoch: 17, Batch: 561/842, Loss: 0.5525, Time: 0.71s\n",
      "Epoch: 17, Batch: 571/842, Loss: 0.6169, Time: 0.71s\n",
      "Epoch: 17, Batch: 581/842, Loss: 0.6730, Time: 0.71s\n",
      "Epoch: 17, Batch: 591/842, Loss: 0.5938, Time: 0.71s\n",
      "Epoch: 17, Batch: 601/842, Loss: 0.4463, Time: 0.72s\n",
      "Epoch: 17, Batch: 611/842, Loss: 0.5153, Time: 0.71s\n",
      "Epoch: 17, Batch: 621/842, Loss: 0.5449, Time: 0.72s\n",
      "Epoch: 17, Batch: 631/842, Loss: 0.6120, Time: 0.71s\n",
      "Epoch: 17, Batch: 641/842, Loss: 0.5142, Time: 0.72s\n",
      "Epoch: 17, Batch: 651/842, Loss: 0.4722, Time: 0.71s\n",
      "Epoch: 17, Batch: 661/842, Loss: 0.3974, Time: 0.68s\n",
      "Epoch: 17, Batch: 671/842, Loss: 0.5589, Time: 0.68s\n",
      "Epoch: 17, Batch: 681/842, Loss: 0.4044, Time: 0.71s\n",
      "Epoch: 17, Batch: 691/842, Loss: 0.6513, Time: 0.72s\n",
      "Epoch: 17, Batch: 701/842, Loss: 0.5566, Time: 0.72s\n",
      "Epoch: 17, Batch: 711/842, Loss: 0.5872, Time: 0.72s\n",
      "Epoch: 17, Batch: 721/842, Loss: 0.6354, Time: 0.69s\n",
      "Epoch: 17, Batch: 731/842, Loss: 0.6435, Time: 0.70s\n",
      "Epoch: 17, Batch: 741/842, Loss: 0.5457, Time: 0.69s\n",
      "Epoch: 17, Batch: 751/842, Loss: 0.5374, Time: 0.70s\n",
      "Epoch: 17, Batch: 761/842, Loss: 0.5246, Time: 0.76s\n",
      "Epoch: 17, Batch: 771/842, Loss: 0.4887, Time: 0.75s\n",
      "Epoch: 17, Batch: 781/842, Loss: 0.6712, Time: 0.73s\n",
      "Epoch: 17, Batch: 791/842, Loss: 0.6904, Time: 0.74s\n",
      "Epoch: 17, Batch: 801/842, Loss: 0.5645, Time: 0.75s\n",
      "Epoch: 17, Batch: 811/842, Loss: 0.4966, Time: 1.31s\n",
      "Epoch: 17, Batch: 821/842, Loss: 0.4757, Time: 0.72s\n",
      "Epoch: 17, Batch: 831/842, Loss: 0.6129, Time: 0.74s\n",
      "Epoch: 17, Batch: 841/842, Loss: 0.4666, Time: 0.71s\n",
      "Epoch 17/100: Train Loss: 0.5632, Val Loss: 0.5576, mIoU: 0.5714, F1: 0.7162, OA: 0.9534\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7294, F1=0.8435, OA=0.8372, Precision=0.7860, Recall=0.9101\n",
      "    Class 1: IoU=0.4347, F1=0.6060, OA=0.9899, Precision=0.5187, Recall=0.7286\n",
      "    Class 2: IoU=0.8562, F1=0.9225, OA=0.9697, Precision=0.9556, Recall=0.8917\n",
      "    Class 3: IoU=0.4087, F1=0.5802, OA=0.9537, Precision=0.6469, Recall=0.5260\n",
      "    Class 4: IoU=0.6340, F1=0.7760, OA=0.9722, Precision=0.8607, Recall=0.7066\n",
      "    Class 5: IoU=0.4343, F1=0.6056, OA=0.9913, Precision=0.5079, Recall=0.7497\n",
      "    Class 6: IoU=0.5062, F1=0.6722, OA=0.9525, Precision=0.8370, Recall=0.5616\n",
      "    Class 7: IoU=0.5673, F1=0.7239, OA=0.9611, Precision=0.8365, Recall=0.6380\n",
      "Epoch: 18, Batch: 1/842, Loss: 0.7985, Time: 1.59s\n",
      "Epoch: 18, Batch: 11/842, Loss: 0.5473, Time: 0.73s\n",
      "Epoch: 18, Batch: 21/842, Loss: 0.3768, Time: 0.72s\n",
      "Epoch: 18, Batch: 31/842, Loss: 0.5554, Time: 0.73s\n",
      "Epoch: 18, Batch: 41/842, Loss: 0.4073, Time: 0.72s\n",
      "Epoch: 18, Batch: 51/842, Loss: 0.6435, Time: 0.76s\n",
      "Epoch: 18, Batch: 61/842, Loss: 0.7554, Time: 0.85s\n",
      "Epoch: 18, Batch: 71/842, Loss: 0.6224, Time: 0.84s\n",
      "Epoch: 18, Batch: 81/842, Loss: 0.6619, Time: 0.75s\n",
      "Epoch: 18, Batch: 91/842, Loss: 0.5873, Time: 0.67s\n",
      "Epoch: 18, Batch: 101/842, Loss: 0.3827, Time: 0.72s\n",
      "Epoch: 18, Batch: 111/842, Loss: 0.4979, Time: 0.80s\n",
      "Epoch: 18, Batch: 121/842, Loss: 0.7343, Time: 0.79s\n",
      "Epoch: 18, Batch: 131/842, Loss: 0.5192, Time: 0.79s\n",
      "Epoch: 18, Batch: 141/842, Loss: 0.5129, Time: 0.82s\n",
      "Epoch: 18, Batch: 151/842, Loss: 0.6862, Time: 0.77s\n",
      "Epoch: 18, Batch: 161/842, Loss: 0.5645, Time: 0.78s\n",
      "Epoch: 18, Batch: 171/842, Loss: 0.4663, Time: 0.73s\n",
      "Epoch: 18, Batch: 181/842, Loss: 0.6153, Time: 0.73s\n",
      "Epoch: 18, Batch: 191/842, Loss: 0.6324, Time: 0.72s\n",
      "Epoch: 18, Batch: 201/842, Loss: 0.7061, Time: 0.73s\n",
      "Epoch: 18, Batch: 211/842, Loss: 0.6055, Time: 0.71s\n",
      "Epoch: 18, Batch: 221/842, Loss: 0.7173, Time: 0.79s\n",
      "Epoch: 18, Batch: 231/842, Loss: 0.5280, Time: 0.73s\n",
      "Epoch: 18, Batch: 241/842, Loss: 0.4267, Time: 0.72s\n",
      "Epoch: 18, Batch: 251/842, Loss: 0.4457, Time: 0.72s\n",
      "Epoch: 18, Batch: 261/842, Loss: 0.5405, Time: 0.74s\n",
      "Epoch: 18, Batch: 271/842, Loss: 0.4449, Time: 0.73s\n",
      "Epoch: 18, Batch: 281/842, Loss: 0.4677, Time: 0.75s\n",
      "Epoch: 18, Batch: 291/842, Loss: 0.6600, Time: 0.74s\n",
      "Epoch: 18, Batch: 301/842, Loss: 0.6480, Time: 0.73s\n",
      "Epoch: 18, Batch: 311/842, Loss: 0.6018, Time: 0.74s\n",
      "Epoch: 18, Batch: 321/842, Loss: 0.4143, Time: 0.72s\n",
      "Epoch: 18, Batch: 331/842, Loss: 0.4966, Time: 0.72s\n",
      "Epoch: 18, Batch: 341/842, Loss: 0.5013, Time: 0.73s\n",
      "Epoch: 18, Batch: 351/842, Loss: 0.4494, Time: 0.74s\n",
      "Epoch: 18, Batch: 361/842, Loss: 0.5071, Time: 0.73s\n",
      "Epoch: 18, Batch: 371/842, Loss: 0.6174, Time: 0.75s\n",
      "Epoch: 18, Batch: 381/842, Loss: 0.5929, Time: 0.72s\n",
      "Epoch: 18, Batch: 391/842, Loss: 0.5051, Time: 0.72s\n",
      "Epoch: 18, Batch: 401/842, Loss: 0.4368, Time: 0.73s\n",
      "Epoch: 18, Batch: 411/842, Loss: 0.5493, Time: 0.71s\n",
      "Epoch: 18, Batch: 421/842, Loss: 0.6482, Time: 0.72s\n",
      "Epoch: 18, Batch: 431/842, Loss: 0.4878, Time: 0.71s\n",
      "Epoch: 18, Batch: 441/842, Loss: 0.5876, Time: 0.70s\n",
      "Epoch: 18, Batch: 451/842, Loss: 0.3519, Time: 0.72s\n",
      "Epoch: 18, Batch: 461/842, Loss: 0.5680, Time: 0.76s\n",
      "Epoch: 18, Batch: 471/842, Loss: 0.5659, Time: 0.76s\n",
      "Epoch: 18, Batch: 481/842, Loss: 0.4499, Time: 0.74s\n",
      "Epoch: 18, Batch: 491/842, Loss: 0.5538, Time: 0.74s\n",
      "Epoch: 18, Batch: 501/842, Loss: 0.5287, Time: 0.75s\n",
      "Epoch: 18, Batch: 511/842, Loss: 0.5852, Time: 0.76s\n",
      "Epoch: 18, Batch: 521/842, Loss: 0.6305, Time: 0.75s\n",
      "Epoch: 18, Batch: 531/842, Loss: 0.4896, Time: 0.79s\n",
      "Epoch: 18, Batch: 541/842, Loss: 0.5057, Time: 0.80s\n",
      "Epoch: 18, Batch: 551/842, Loss: 0.5354, Time: 0.80s\n",
      "Epoch: 18, Batch: 561/842, Loss: 0.5524, Time: 0.78s\n",
      "Epoch: 18, Batch: 571/842, Loss: 0.4262, Time: 0.75s\n",
      "Epoch: 18, Batch: 581/842, Loss: 0.5336, Time: 0.75s\n",
      "Epoch: 18, Batch: 591/842, Loss: 0.7245, Time: 0.75s\n",
      "Epoch: 18, Batch: 601/842, Loss: 0.5770, Time: 0.74s\n",
      "Epoch: 18, Batch: 611/842, Loss: 0.4598, Time: 0.74s\n",
      "Epoch: 18, Batch: 621/842, Loss: 0.5294, Time: 0.73s\n",
      "Epoch: 18, Batch: 631/842, Loss: 0.4702, Time: 0.73s\n",
      "Epoch: 18, Batch: 641/842, Loss: 0.5246, Time: 0.75s\n",
      "Epoch: 18, Batch: 651/842, Loss: 0.4405, Time: 0.75s\n",
      "Epoch: 18, Batch: 661/842, Loss: 0.4678, Time: 0.77s\n",
      "Epoch: 18, Batch: 671/842, Loss: 0.7129, Time: 0.76s\n",
      "Epoch: 18, Batch: 681/842, Loss: 0.4746, Time: 0.74s\n",
      "Epoch: 18, Batch: 691/842, Loss: 0.4643, Time: 0.73s\n",
      "Epoch: 18, Batch: 701/842, Loss: 0.5025, Time: 0.73s\n",
      "Epoch: 18, Batch: 711/842, Loss: 0.6485, Time: 0.72s\n",
      "Epoch: 18, Batch: 721/842, Loss: 0.6068, Time: 0.71s\n",
      "Epoch: 18, Batch: 731/842, Loss: 0.4430, Time: 0.71s\n",
      "Epoch: 18, Batch: 741/842, Loss: 0.5904, Time: 0.71s\n",
      "Epoch: 18, Batch: 751/842, Loss: 0.4592, Time: 0.71s\n",
      "Epoch: 18, Batch: 761/842, Loss: 0.5645, Time: 0.71s\n",
      "Epoch: 18, Batch: 771/842, Loss: 0.5499, Time: 0.71s\n",
      "Epoch: 18, Batch: 781/842, Loss: 0.4187, Time: 0.71s\n",
      "Epoch: 18, Batch: 791/842, Loss: 0.5624, Time: 0.71s\n",
      "Epoch: 18, Batch: 801/842, Loss: 0.7107, Time: 0.81s\n",
      "Epoch: 18, Batch: 811/842, Loss: 0.6213, Time: 0.81s\n",
      "Epoch: 18, Batch: 821/842, Loss: 0.4513, Time: 0.81s\n",
      "Epoch: 18, Batch: 831/842, Loss: 0.5098, Time: 0.76s\n",
      "Epoch: 18, Batch: 841/842, Loss: 0.5951, Time: 0.71s\n",
      "Epoch 18/100: Train Loss: 0.5506, Val Loss: 0.5533, mIoU: 0.5860, F1: 0.7277, OA: 0.9547\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7422, F1=0.8520, OA=0.8457, Precision=0.7922, Recall=0.9215\n",
      "    Class 1: IoU=0.4060, F1=0.5775, OA=0.9887, Precision=0.4776, Recall=0.7302\n",
      "    Class 2: IoU=0.8548, F1=0.9217, OA=0.9694, Precision=0.9585, Recall=0.8876\n",
      "    Class 3: IoU=0.3995, F1=0.5709, OA=0.9550, Precision=0.6794, Recall=0.4923\n",
      "    Class 4: IoU=0.6854, F1=0.8133, OA=0.9766, Precision=0.8915, Recall=0.7478\n",
      "    Class 5: IoU=0.5642, F1=0.7214, OA=0.9954, Precision=0.7853, Recall=0.6670\n",
      "    Class 6: IoU=0.5029, F1=0.6693, OA=0.9476, Precision=0.7382, Recall=0.6121\n",
      "    Class 7: IoU=0.5330, F1=0.6954, OA=0.9590, Precision=0.8576, Recall=0.5848\n",
      "Epoch: 19, Batch: 1/842, Loss: 0.4045, Time: 1.87s\n",
      "Epoch: 19, Batch: 11/842, Loss: 0.4484, Time: 0.82s\n",
      "Epoch: 19, Batch: 21/842, Loss: 0.4854, Time: 0.87s\n",
      "Epoch: 19, Batch: 31/842, Loss: 0.4466, Time: 0.87s\n",
      "Epoch: 19, Batch: 41/842, Loss: 0.4842, Time: 0.87s\n",
      "Epoch: 19, Batch: 51/842, Loss: 0.8007, Time: 0.84s\n",
      "Epoch: 19, Batch: 61/842, Loss: 0.6048, Time: 0.86s\n",
      "Epoch: 19, Batch: 71/842, Loss: 0.5063, Time: 0.83s\n",
      "Epoch: 19, Batch: 81/842, Loss: 0.4494, Time: 0.75s\n",
      "Epoch: 19, Batch: 91/842, Loss: 0.5660, Time: 0.81s\n",
      "Epoch: 19, Batch: 101/842, Loss: 0.6308, Time: 0.78s\n",
      "Epoch: 19, Batch: 111/842, Loss: 0.4523, Time: 0.77s\n",
      "Epoch: 19, Batch: 121/842, Loss: 0.3987, Time: 0.78s\n",
      "Epoch: 19, Batch: 131/842, Loss: 0.5452, Time: 0.76s\n",
      "Epoch: 19, Batch: 141/842, Loss: 0.6113, Time: 0.72s\n",
      "Epoch: 19, Batch: 151/842, Loss: 0.4982, Time: 0.70s\n",
      "Epoch: 19, Batch: 161/842, Loss: 0.5370, Time: 0.69s\n",
      "Epoch: 19, Batch: 171/842, Loss: 0.5112, Time: 0.79s\n",
      "Epoch: 19, Batch: 181/842, Loss: 0.5124, Time: 0.79s\n",
      "Epoch: 19, Batch: 191/842, Loss: 0.3361, Time: 0.74s\n",
      "Epoch: 19, Batch: 201/842, Loss: 0.3931, Time: 0.76s\n",
      "Epoch: 19, Batch: 211/842, Loss: 0.5306, Time: 0.73s\n",
      "Epoch: 19, Batch: 221/842, Loss: 0.5373, Time: 0.72s\n",
      "Epoch: 19, Batch: 231/842, Loss: 0.4689, Time: 0.74s\n",
      "Epoch: 19, Batch: 241/842, Loss: 0.5686, Time: 0.78s\n",
      "Epoch: 19, Batch: 251/842, Loss: 0.6197, Time: 0.75s\n",
      "Epoch: 19, Batch: 261/842, Loss: 0.5431, Time: 0.77s\n",
      "Epoch: 19, Batch: 271/842, Loss: 0.5948, Time: 0.79s\n",
      "Epoch: 19, Batch: 281/842, Loss: 0.7301, Time: 0.72s\n",
      "Epoch: 19, Batch: 291/842, Loss: 0.5234, Time: 0.73s\n",
      "Epoch: 19, Batch: 301/842, Loss: 0.5802, Time: 0.71s\n",
      "Epoch: 19, Batch: 311/842, Loss: 0.5971, Time: 0.72s\n",
      "Epoch: 19, Batch: 321/842, Loss: 0.6017, Time: 0.71s\n",
      "Epoch: 19, Batch: 331/842, Loss: 0.4710, Time: 0.71s\n",
      "Epoch: 19, Batch: 341/842, Loss: 0.6644, Time: 0.71s\n",
      "Epoch: 19, Batch: 351/842, Loss: 0.5227, Time: 0.71s\n",
      "Epoch: 19, Batch: 361/842, Loss: 0.5159, Time: 0.71s\n",
      "Epoch: 19, Batch: 371/842, Loss: 0.6707, Time: 0.71s\n",
      "Epoch: 19, Batch: 381/842, Loss: 0.5340, Time: 0.70s\n",
      "Epoch: 19, Batch: 391/842, Loss: 0.5628, Time: 0.71s\n",
      "Epoch: 19, Batch: 401/842, Loss: 0.5554, Time: 0.72s\n",
      "Epoch: 19, Batch: 411/842, Loss: 0.6492, Time: 0.71s\n",
      "Epoch: 19, Batch: 421/842, Loss: 0.6974, Time: 0.71s\n",
      "Epoch: 19, Batch: 431/842, Loss: 0.4440, Time: 0.71s\n",
      "Epoch: 19, Batch: 441/842, Loss: 0.5865, Time: 0.71s\n",
      "Epoch: 19, Batch: 451/842, Loss: 0.4931, Time: 0.72s\n",
      "Epoch: 19, Batch: 461/842, Loss: 0.5760, Time: 0.75s\n",
      "Epoch: 19, Batch: 471/842, Loss: 0.5623, Time: 0.74s\n",
      "Epoch: 19, Batch: 481/842, Loss: 0.4387, Time: 0.74s\n",
      "Epoch: 19, Batch: 491/842, Loss: 0.4977, Time: 0.73s\n",
      "Epoch: 19, Batch: 501/842, Loss: 0.4387, Time: 0.73s\n",
      "Epoch: 19, Batch: 511/842, Loss: 0.4281, Time: 0.72s\n",
      "Epoch: 19, Batch: 521/842, Loss: 0.4736, Time: 0.72s\n",
      "Epoch: 19, Batch: 531/842, Loss: 0.6070, Time: 0.72s\n",
      "Epoch: 19, Batch: 541/842, Loss: 0.4705, Time: 0.72s\n",
      "Epoch: 19, Batch: 551/842, Loss: 0.5932, Time: 0.72s\n",
      "Epoch: 19, Batch: 561/842, Loss: 0.5229, Time: 0.77s\n",
      "Epoch: 19, Batch: 571/842, Loss: 0.6149, Time: 0.77s\n",
      "Epoch: 19, Batch: 581/842, Loss: 0.4683, Time: 0.74s\n",
      "Epoch: 19, Batch: 591/842, Loss: 0.3991, Time: 0.72s\n",
      "Epoch: 19, Batch: 601/842, Loss: 0.5403, Time: 0.72s\n",
      "Epoch: 19, Batch: 611/842, Loss: 0.5289, Time: 0.74s\n",
      "Epoch: 19, Batch: 621/842, Loss: 0.4924, Time: 0.79s\n",
      "Epoch: 19, Batch: 631/842, Loss: 0.3790, Time: 0.72s\n",
      "Epoch: 19, Batch: 641/842, Loss: 0.5239, Time: 0.72s\n",
      "Epoch: 19, Batch: 651/842, Loss: 0.6262, Time: 0.80s\n",
      "Epoch: 19, Batch: 661/842, Loss: 0.7238, Time: 0.80s\n",
      "Epoch: 19, Batch: 671/842, Loss: 0.4363, Time: 0.81s\n",
      "Epoch: 19, Batch: 681/842, Loss: 0.5445, Time: 0.81s\n",
      "Epoch: 19, Batch: 691/842, Loss: 0.5217, Time: 0.79s\n",
      "Epoch: 19, Batch: 701/842, Loss: 0.5963, Time: 0.82s\n",
      "Epoch: 19, Batch: 711/842, Loss: 0.6159, Time: 0.82s\n",
      "Epoch: 19, Batch: 721/842, Loss: 0.4628, Time: 0.81s\n",
      "Epoch: 19, Batch: 731/842, Loss: 0.6316, Time: 0.76s\n",
      "Epoch: 19, Batch: 741/842, Loss: 0.5798, Time: 0.80s\n",
      "Epoch: 19, Batch: 751/842, Loss: 0.6083, Time: 0.77s\n",
      "Epoch: 19, Batch: 761/842, Loss: 0.5785, Time: 0.78s\n",
      "Epoch: 19, Batch: 771/842, Loss: 0.5049, Time: 0.77s\n",
      "Epoch: 19, Batch: 781/842, Loss: 0.5960, Time: 0.74s\n",
      "Epoch: 19, Batch: 791/842, Loss: 0.5094, Time: 0.75s\n",
      "Epoch: 19, Batch: 801/842, Loss: 0.5529, Time: 0.75s\n",
      "Epoch: 19, Batch: 811/842, Loss: 0.5189, Time: 0.77s\n",
      "Epoch: 19, Batch: 821/842, Loss: 0.4417, Time: 0.75s\n",
      "Epoch: 19, Batch: 831/842, Loss: 0.6170, Time: 0.76s\n",
      "Epoch: 19, Batch: 841/842, Loss: 0.6417, Time: 0.72s\n",
      "Epoch 19/100: Train Loss: 0.5471, Val Loss: 0.5210, mIoU: 0.6188, F1: 0.7535, OA: 0.9574\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7434, F1=0.8528, OA=0.8475, Precision=0.7975, Recall=0.9163\n",
      "    Class 1: IoU=0.6397, F1=0.7803, OA=0.9957, Precision=0.8454, Recall=0.7245\n",
      "    Class 2: IoU=0.8992, F1=0.9469, OA=0.9786, Precision=0.9513, Recall=0.9426\n",
      "    Class 3: IoU=0.3586, F1=0.5279, OA=0.9579, Precision=0.8305, Recall=0.3869\n",
      "    Class 4: IoU=0.6742, F1=0.8054, OA=0.9756, Precision=0.8835, Recall=0.7399\n",
      "    Class 5: IoU=0.5414, F1=0.7025, OA=0.9953, Precision=0.8075, Recall=0.6216\n",
      "    Class 6: IoU=0.5020, F1=0.6685, OA=0.9463, Precision=0.7192, Recall=0.6244\n",
      "    Class 7: IoU=0.5917, F1=0.7435, OA=0.9619, Precision=0.8054, Recall=0.6904\n",
      "Saving best model with mIoU: 0.6188\n",
      "Epoch: 20, Batch: 1/842, Loss: 0.5812, Time: 1.72s\n",
      "Epoch: 20, Batch: 11/842, Loss: 0.4955, Time: 0.80s\n",
      "Epoch: 20, Batch: 21/842, Loss: 0.7117, Time: 0.83s\n",
      "Epoch: 20, Batch: 31/842, Loss: 0.4671, Time: 0.76s\n",
      "Epoch: 20, Batch: 41/842, Loss: 0.4853, Time: 0.72s\n",
      "Epoch: 20, Batch: 51/842, Loss: 0.5807, Time: 0.72s\n",
      "Epoch: 20, Batch: 61/842, Loss: 0.5060, Time: 0.70s\n",
      "Epoch: 20, Batch: 71/842, Loss: 0.6770, Time: 0.69s\n",
      "Epoch: 20, Batch: 81/842, Loss: 0.6359, Time: 0.71s\n",
      "Epoch: 20, Batch: 91/842, Loss: 0.5647, Time: 0.73s\n",
      "Epoch: 20, Batch: 101/842, Loss: 0.4963, Time: 0.93s\n",
      "Epoch: 20, Batch: 111/842, Loss: 0.4231, Time: 0.84s\n",
      "Epoch: 20, Batch: 121/842, Loss: 0.4240, Time: 0.79s\n",
      "Epoch: 20, Batch: 131/842, Loss: 0.5271, Time: 0.78s\n",
      "Epoch: 20, Batch: 141/842, Loss: 0.4678, Time: 0.80s\n",
      "Epoch: 20, Batch: 151/842, Loss: 0.5890, Time: 0.82s\n",
      "Epoch: 20, Batch: 161/842, Loss: 0.5662, Time: 0.79s\n",
      "Epoch: 20, Batch: 171/842, Loss: 0.6439, Time: 0.73s\n",
      "Epoch: 20, Batch: 181/842, Loss: 0.5082, Time: 0.72s\n",
      "Epoch: 20, Batch: 191/842, Loss: 0.5433, Time: 0.73s\n",
      "Epoch: 20, Batch: 201/842, Loss: 0.4973, Time: 0.72s\n",
      "Epoch: 20, Batch: 211/842, Loss: 0.4633, Time: 0.72s\n",
      "Epoch: 20, Batch: 221/842, Loss: 0.6382, Time: 0.72s\n",
      "Epoch: 20, Batch: 231/842, Loss: 0.7562, Time: 0.73s\n",
      "Epoch: 20, Batch: 241/842, Loss: 0.4982, Time: 0.72s\n",
      "Epoch: 20, Batch: 251/842, Loss: 0.5237, Time: 0.74s\n",
      "Epoch: 20, Batch: 261/842, Loss: 0.7908, Time: 0.76s\n",
      "Epoch: 20, Batch: 271/842, Loss: 0.5179, Time: 0.73s\n",
      "Epoch: 20, Batch: 281/842, Loss: 0.7448, Time: 0.74s\n",
      "Epoch: 20, Batch: 291/842, Loss: 0.5455, Time: 0.75s\n",
      "Epoch: 20, Batch: 301/842, Loss: 0.2989, Time: 0.76s\n",
      "Epoch: 20, Batch: 311/842, Loss: 0.5320, Time: 0.74s\n",
      "Epoch: 20, Batch: 321/842, Loss: 0.7262, Time: 0.74s\n",
      "Epoch: 20, Batch: 331/842, Loss: 0.4575, Time: 0.74s\n",
      "Epoch: 20, Batch: 341/842, Loss: 0.5266, Time: 0.77s\n",
      "Epoch: 20, Batch: 351/842, Loss: 0.4624, Time: 0.75s\n",
      "Epoch: 20, Batch: 361/842, Loss: 0.5501, Time: 0.73s\n",
      "Epoch: 20, Batch: 371/842, Loss: 0.4598, Time: 0.74s\n",
      "Epoch: 20, Batch: 381/842, Loss: 0.4237, Time: 0.78s\n",
      "Epoch: 20, Batch: 391/842, Loss: 0.5972, Time: 0.74s\n",
      "Epoch: 20, Batch: 401/842, Loss: 0.5724, Time: 0.77s\n",
      "Epoch: 20, Batch: 411/842, Loss: 0.5032, Time: 0.75s\n",
      "Epoch: 20, Batch: 421/842, Loss: 0.4961, Time: 0.74s\n",
      "Epoch: 20, Batch: 431/842, Loss: 0.4966, Time: 0.75s\n",
      "Epoch: 20, Batch: 441/842, Loss: 0.4613, Time: 0.74s\n",
      "Epoch: 20, Batch: 451/842, Loss: 0.4090, Time: 0.75s\n",
      "Epoch: 20, Batch: 461/842, Loss: 0.5898, Time: 0.76s\n",
      "Epoch: 20, Batch: 471/842, Loss: 0.4732, Time: 0.76s\n",
      "Epoch: 20, Batch: 481/842, Loss: 0.4326, Time: 0.77s\n",
      "Epoch: 20, Batch: 491/842, Loss: 0.6242, Time: 0.75s\n",
      "Epoch: 20, Batch: 501/842, Loss: 0.5600, Time: 0.75s\n",
      "Epoch: 20, Batch: 511/842, Loss: 0.5606, Time: 0.76s\n",
      "Epoch: 20, Batch: 521/842, Loss: 0.5367, Time: 0.76s\n",
      "Epoch: 20, Batch: 531/842, Loss: 0.4670, Time: 0.73s\n",
      "Epoch: 20, Batch: 541/842, Loss: 0.4671, Time: 0.75s\n",
      "Epoch: 20, Batch: 551/842, Loss: 0.5721, Time: 0.75s\n",
      "Epoch: 20, Batch: 561/842, Loss: 0.5976, Time: 0.75s\n",
      "Epoch: 20, Batch: 571/842, Loss: 0.6642, Time: 0.75s\n",
      "Epoch: 20, Batch: 581/842, Loss: 0.5606, Time: 0.75s\n",
      "Epoch: 20, Batch: 591/842, Loss: 0.6203, Time: 0.74s\n",
      "Epoch: 20, Batch: 601/842, Loss: 0.4984, Time: 0.76s\n",
      "Epoch: 20, Batch: 611/842, Loss: 0.5885, Time: 0.77s\n",
      "Epoch: 20, Batch: 621/842, Loss: 0.5231, Time: 0.75s\n",
      "Epoch: 20, Batch: 631/842, Loss: 0.5268, Time: 0.74s\n",
      "Epoch: 20, Batch: 641/842, Loss: 0.5541, Time: 0.78s\n",
      "Epoch: 20, Batch: 651/842, Loss: 0.7095, Time: 0.75s\n",
      "Epoch: 20, Batch: 661/842, Loss: 0.4364, Time: 0.75s\n",
      "Epoch: 20, Batch: 671/842, Loss: 0.7473, Time: 0.77s\n",
      "Epoch: 20, Batch: 681/842, Loss: 0.5413, Time: 0.79s\n",
      "Epoch: 20, Batch: 691/842, Loss: 0.6213, Time: 0.76s\n",
      "Epoch: 20, Batch: 701/842, Loss: 0.5833, Time: 0.71s\n",
      "Epoch: 20, Batch: 711/842, Loss: 0.5629, Time: 0.76s\n",
      "Epoch: 20, Batch: 721/842, Loss: 0.4234, Time: 0.78s\n",
      "Epoch: 20, Batch: 731/842, Loss: 0.4980, Time: 0.78s\n",
      "Epoch: 20, Batch: 741/842, Loss: 0.5395, Time: 0.77s\n",
      "Epoch: 20, Batch: 751/842, Loss: 0.4112, Time: 0.79s\n",
      "Epoch: 20, Batch: 761/842, Loss: 0.3290, Time: 0.78s\n",
      "Epoch: 20, Batch: 771/842, Loss: 0.6811, Time: 0.76s\n",
      "Epoch: 20, Batch: 781/842, Loss: 0.6004, Time: 0.75s\n",
      "Epoch: 20, Batch: 791/842, Loss: 0.6017, Time: 0.72s\n",
      "Epoch: 20, Batch: 801/842, Loss: 0.5389, Time: 0.74s\n",
      "Epoch: 20, Batch: 811/842, Loss: 0.5188, Time: 0.74s\n",
      "Epoch: 20, Batch: 821/842, Loss: 0.5045, Time: 0.73s\n",
      "Epoch: 20, Batch: 831/842, Loss: 0.4749, Time: 0.74s\n",
      "Epoch: 20, Batch: 841/842, Loss: 0.5271, Time: 0.71s\n",
      "Epoch 20/100: Train Loss: 0.5433, Val Loss: 0.5151, mIoU: 0.6234, F1: 0.7576, OA: 0.9573\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7420, F1=0.8519, OA=0.8474, Precision=0.8007, Recall=0.9101\n",
      "    Class 1: IoU=0.6537, F1=0.7906, OA=0.9958, Precision=0.8483, Recall=0.7403\n",
      "    Class 2: IoU=0.9008, F1=0.9478, OA=0.9788, Precision=0.9446, Recall=0.9510\n",
      "    Class 3: IoU=0.3742, F1=0.5446, OA=0.9578, Precision=0.7933, Recall=0.4146\n",
      "    Class 4: IoU=0.6855, F1=0.8134, OA=0.9755, Precision=0.8460, Recall=0.7832\n",
      "    Class 5: IoU=0.5624, F1=0.7199, OA=0.9956, Precision=0.8215, Recall=0.6407\n",
      "    Class 6: IoU=0.5157, F1=0.6805, OA=0.9465, Precision=0.7048, Recall=0.6578\n",
      "    Class 7: IoU=0.5528, F1=0.7120, OA=0.9609, Precision=0.8655, Recall=0.6047\n",
      "Saving best model with mIoU: 0.6234\n",
      "Epoch: 21, Batch: 1/842, Loss: 0.5551, Time: 1.48s\n",
      "Epoch: 21, Batch: 11/842, Loss: 0.5948, Time: 0.79s\n",
      "Epoch: 21, Batch: 21/842, Loss: 0.4025, Time: 0.79s\n",
      "Epoch: 21, Batch: 31/842, Loss: 0.4998, Time: 0.78s\n",
      "Epoch: 21, Batch: 41/842, Loss: 0.6271, Time: 0.76s\n",
      "Epoch: 21, Batch: 51/842, Loss: 0.5155, Time: 0.82s\n",
      "Epoch: 21, Batch: 61/842, Loss: 0.5908, Time: 0.75s\n",
      "Epoch: 21, Batch: 71/842, Loss: 0.5021, Time: 0.70s\n",
      "Epoch: 21, Batch: 81/842, Loss: 0.5400, Time: 0.68s\n",
      "Epoch: 21, Batch: 91/842, Loss: 0.4682, Time: 0.70s\n",
      "Epoch: 21, Batch: 101/842, Loss: 0.5255, Time: 0.69s\n",
      "Epoch: 21, Batch: 111/842, Loss: 0.3662, Time: 0.69s\n",
      "Epoch: 21, Batch: 121/842, Loss: 0.4771, Time: 0.70s\n",
      "Epoch: 21, Batch: 131/842, Loss: 0.6115, Time: 0.70s\n",
      "Epoch: 21, Batch: 141/842, Loss: 0.5022, Time: 0.69s\n",
      "Epoch: 21, Batch: 151/842, Loss: 0.6086, Time: 0.69s\n",
      "Epoch: 21, Batch: 161/842, Loss: 0.4264, Time: 0.68s\n",
      "Epoch: 21, Batch: 171/842, Loss: 0.5935, Time: 0.70s\n",
      "Epoch: 21, Batch: 181/842, Loss: 0.6449, Time: 0.68s\n",
      "Epoch: 21, Batch: 191/842, Loss: 0.6867, Time: 0.69s\n",
      "Epoch: 21, Batch: 201/842, Loss: 0.4201, Time: 0.69s\n",
      "Epoch: 21, Batch: 211/842, Loss: 0.5699, Time: 0.76s\n",
      "Epoch: 21, Batch: 221/842, Loss: 0.4717, Time: 0.81s\n",
      "Epoch: 21, Batch: 231/842, Loss: 0.5177, Time: 0.85s\n",
      "Epoch: 21, Batch: 241/842, Loss: 0.3881, Time: 0.78s\n",
      "Epoch: 21, Batch: 251/842, Loss: 0.5634, Time: 0.83s\n",
      "Epoch: 21, Batch: 261/842, Loss: 0.5315, Time: 0.83s\n",
      "Epoch: 21, Batch: 271/842, Loss: 0.6351, Time: 0.83s\n",
      "Epoch: 21, Batch: 281/842, Loss: 0.5483, Time: 0.83s\n",
      "Epoch: 21, Batch: 291/842, Loss: 0.3930, Time: 0.86s\n",
      "Epoch: 21, Batch: 301/842, Loss: 0.4840, Time: 0.85s\n",
      "Epoch: 21, Batch: 311/842, Loss: 0.5920, Time: 0.83s\n",
      "Epoch: 21, Batch: 321/842, Loss: 0.4504, Time: 1.46s\n",
      "Epoch: 21, Batch: 331/842, Loss: 0.5895, Time: 0.85s\n",
      "Epoch: 21, Batch: 341/842, Loss: 0.3207, Time: 0.84s\n",
      "Epoch: 21, Batch: 351/842, Loss: 0.5082, Time: 0.76s\n",
      "Epoch: 21, Batch: 361/842, Loss: 0.5312, Time: 0.75s\n",
      "Epoch: 21, Batch: 371/842, Loss: 0.5293, Time: 0.74s\n",
      "Epoch: 21, Batch: 381/842, Loss: 0.5161, Time: 0.74s\n",
      "Epoch: 21, Batch: 391/842, Loss: 0.4752, Time: 0.73s\n",
      "Epoch: 21, Batch: 401/842, Loss: 0.4868, Time: 0.73s\n",
      "Epoch: 21, Batch: 411/842, Loss: 0.5110, Time: 0.74s\n",
      "Epoch: 21, Batch: 421/842, Loss: 0.5466, Time: 0.73s\n",
      "Epoch: 21, Batch: 431/842, Loss: 0.6160, Time: 0.73s\n",
      "Epoch: 21, Batch: 441/842, Loss: 0.5134, Time: 0.73s\n",
      "Epoch: 21, Batch: 451/842, Loss: 0.4546, Time: 0.72s\n",
      "Epoch: 21, Batch: 461/842, Loss: 0.6832, Time: 0.72s\n",
      "Epoch: 21, Batch: 471/842, Loss: 0.5631, Time: 0.72s\n",
      "Epoch: 21, Batch: 481/842, Loss: 0.5543, Time: 0.73s\n",
      "Epoch: 21, Batch: 491/842, Loss: 0.5782, Time: 0.76s\n",
      "Epoch: 21, Batch: 501/842, Loss: 0.5485, Time: 0.78s\n",
      "Epoch: 21, Batch: 511/842, Loss: 0.6641, Time: 0.76s\n",
      "Epoch: 21, Batch: 521/842, Loss: 0.5153, Time: 0.74s\n",
      "Epoch: 21, Batch: 531/842, Loss: 0.3732, Time: 0.73s\n",
      "Epoch: 21, Batch: 541/842, Loss: 0.4435, Time: 0.72s\n",
      "Epoch: 21, Batch: 551/842, Loss: 0.4168, Time: 0.72s\n",
      "Epoch: 21, Batch: 561/842, Loss: 0.6033, Time: 0.72s\n",
      "Epoch: 21, Batch: 571/842, Loss: 0.4175, Time: 0.72s\n",
      "Epoch: 21, Batch: 581/842, Loss: 0.5261, Time: 0.72s\n",
      "Epoch: 21, Batch: 591/842, Loss: 0.4955, Time: 0.72s\n",
      "Epoch: 21, Batch: 601/842, Loss: 0.4777, Time: 0.76s\n",
      "Epoch: 21, Batch: 611/842, Loss: 0.4837, Time: 0.77s\n",
      "Epoch: 21, Batch: 621/842, Loss: 0.4088, Time: 0.78s\n",
      "Epoch: 21, Batch: 631/842, Loss: 0.4413, Time: 0.82s\n",
      "Epoch: 21, Batch: 641/842, Loss: 0.5312, Time: 0.84s\n",
      "Epoch: 21, Batch: 651/842, Loss: 0.4453, Time: 0.80s\n",
      "Epoch: 21, Batch: 661/842, Loss: 0.5475, Time: 0.82s\n",
      "Epoch: 21, Batch: 671/842, Loss: 0.5144, Time: 0.80s\n",
      "Epoch: 21, Batch: 681/842, Loss: 0.4369, Time: 0.78s\n",
      "Epoch: 21, Batch: 691/842, Loss: 0.5838, Time: 0.77s\n",
      "Epoch: 21, Batch: 701/842, Loss: 0.3465, Time: 0.76s\n",
      "Epoch: 21, Batch: 711/842, Loss: 0.6246, Time: 0.76s\n",
      "Epoch: 21, Batch: 721/842, Loss: 0.5169, Time: 0.76s\n",
      "Epoch: 21, Batch: 731/842, Loss: 0.5511, Time: 0.75s\n",
      "Epoch: 21, Batch: 741/842, Loss: 0.3738, Time: 0.77s\n",
      "Epoch: 21, Batch: 751/842, Loss: 0.5897, Time: 0.74s\n",
      "Epoch: 21, Batch: 761/842, Loss: 0.6944, Time: 0.76s\n",
      "Epoch: 21, Batch: 771/842, Loss: 0.4563, Time: 0.74s\n",
      "Epoch: 21, Batch: 781/842, Loss: 0.4985, Time: 0.75s\n",
      "Epoch: 21, Batch: 791/842, Loss: 0.5399, Time: 0.74s\n",
      "Epoch: 21, Batch: 801/842, Loss: 0.6379, Time: 0.76s\n",
      "Epoch: 21, Batch: 811/842, Loss: 0.4866, Time: 0.76s\n",
      "Epoch: 21, Batch: 821/842, Loss: 0.5290, Time: 0.75s\n",
      "Epoch: 21, Batch: 831/842, Loss: 0.5094, Time: 0.73s\n",
      "Epoch: 21, Batch: 841/842, Loss: 0.4723, Time: 0.72s\n",
      "Epoch 21/100: Train Loss: 0.5370, Val Loss: 0.5539, mIoU: 0.5921, F1: 0.7354, OA: 0.9543\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7347, F1=0.8471, OA=0.8378, Precision=0.7765, Recall=0.9318\n",
      "    Class 1: IoU=0.5093, F1=0.6749, OA=0.9926, Precision=0.6319, Recall=0.7242\n",
      "    Class 2: IoU=0.8555, F1=0.9221, OA=0.9696, Precision=0.9587, Recall=0.8883\n",
      "    Class 3: IoU=0.3977, F1=0.5691, OA=0.9578, Precision=0.7497, Recall=0.4586\n",
      "    Class 4: IoU=0.5834, F1=0.7369, OA=0.9700, Precision=0.9151, Recall=0.6168\n",
      "    Class 5: IoU=0.5711, F1=0.7270, OA=0.9953, Precision=0.7574, Recall=0.6989\n",
      "    Class 6: IoU=0.5172, F1=0.6818, OA=0.9515, Precision=0.7910, Recall=0.5990\n",
      "    Class 7: IoU=0.5675, F1=0.7241, OA=0.9601, Precision=0.8112, Recall=0.6538\n",
      "Epoch: 22, Batch: 1/842, Loss: 0.5477, Time: 1.64s\n",
      "Epoch: 22, Batch: 11/842, Loss: 0.5642, Time: 0.74s\n",
      "Epoch: 22, Batch: 21/842, Loss: 0.4388, Time: 0.75s\n",
      "Epoch: 22, Batch: 31/842, Loss: 0.4284, Time: 0.73s\n",
      "Epoch: 22, Batch: 41/842, Loss: 0.5308, Time: 0.74s\n",
      "Epoch: 22, Batch: 51/842, Loss: 0.5896, Time: 0.75s\n",
      "Epoch: 22, Batch: 61/842, Loss: 0.5905, Time: 0.73s\n",
      "Epoch: 22, Batch: 71/842, Loss: 0.5209, Time: 0.73s\n",
      "Epoch: 22, Batch: 81/842, Loss: 0.4008, Time: 0.78s\n",
      "Epoch: 22, Batch: 91/842, Loss: 0.4456, Time: 0.79s\n",
      "Epoch: 22, Batch: 101/842, Loss: 0.5294, Time: 0.78s\n",
      "Epoch: 22, Batch: 111/842, Loss: 0.6487, Time: 0.75s\n",
      "Epoch: 22, Batch: 121/842, Loss: 0.5887, Time: 0.75s\n",
      "Epoch: 22, Batch: 131/842, Loss: 0.5968, Time: 0.76s\n",
      "Epoch: 22, Batch: 141/842, Loss: 0.5455, Time: 0.75s\n",
      "Epoch: 22, Batch: 151/842, Loss: 0.4341, Time: 0.73s\n",
      "Epoch: 22, Batch: 161/842, Loss: 0.4820, Time: 0.73s\n",
      "Epoch: 22, Batch: 171/842, Loss: 0.4269, Time: 0.74s\n",
      "Epoch: 22, Batch: 181/842, Loss: 0.4964, Time: 0.74s\n",
      "Epoch: 22, Batch: 191/842, Loss: 0.5538, Time: 0.73s\n",
      "Epoch: 22, Batch: 201/842, Loss: 0.6019, Time: 0.74s\n",
      "Epoch: 22, Batch: 211/842, Loss: 0.4362, Time: 0.73s\n",
      "Epoch: 22, Batch: 221/842, Loss: 0.3535, Time: 0.73s\n",
      "Epoch: 22, Batch: 231/842, Loss: 0.5390, Time: 0.77s\n",
      "Epoch: 22, Batch: 241/842, Loss: 0.5592, Time: 0.85s\n",
      "Epoch: 22, Batch: 251/842, Loss: 0.4818, Time: 0.84s\n",
      "Epoch: 22, Batch: 261/842, Loss: 0.5104, Time: 0.83s\n",
      "Epoch: 22, Batch: 271/842, Loss: 0.5515, Time: 0.83s\n",
      "Epoch: 22, Batch: 281/842, Loss: 0.6690, Time: 0.84s\n",
      "Epoch: 22, Batch: 291/842, Loss: 0.5141, Time: 0.87s\n",
      "Epoch: 22, Batch: 301/842, Loss: 0.4919, Time: 0.84s\n",
      "Epoch: 22, Batch: 311/842, Loss: 0.4613, Time: 0.73s\n",
      "Epoch: 22, Batch: 321/842, Loss: 0.5285, Time: 0.76s\n",
      "Epoch: 22, Batch: 331/842, Loss: 0.4513, Time: 0.72s\n",
      "Epoch: 22, Batch: 341/842, Loss: 0.4702, Time: 0.73s\n",
      "Epoch: 22, Batch: 351/842, Loss: 0.5878, Time: 0.72s\n",
      "Epoch: 22, Batch: 361/842, Loss: 0.5201, Time: 0.79s\n",
      "Epoch: 22, Batch: 371/842, Loss: 0.6165, Time: 0.73s\n",
      "Epoch: 22, Batch: 381/842, Loss: 0.6449, Time: 0.75s\n",
      "Epoch: 22, Batch: 391/842, Loss: 0.4921, Time: 0.73s\n",
      "Epoch: 22, Batch: 401/842, Loss: 0.5712, Time: 0.73s\n",
      "Epoch: 22, Batch: 411/842, Loss: 0.4712, Time: 0.73s\n",
      "Epoch: 22, Batch: 421/842, Loss: 0.4395, Time: 0.74s\n",
      "Epoch: 22, Batch: 431/842, Loss: 0.5780, Time: 0.74s\n",
      "Epoch: 22, Batch: 441/842, Loss: 0.5581, Time: 0.74s\n",
      "Epoch: 22, Batch: 451/842, Loss: 0.5130, Time: 0.74s\n",
      "Epoch: 22, Batch: 461/842, Loss: 0.5144, Time: 0.75s\n",
      "Epoch: 22, Batch: 471/842, Loss: 0.4603, Time: 0.74s\n",
      "Epoch: 22, Batch: 481/842, Loss: 0.5704, Time: 0.74s\n",
      "Epoch: 22, Batch: 491/842, Loss: 0.5366, Time: 0.74s\n",
      "Epoch: 22, Batch: 501/842, Loss: 0.6988, Time: 0.75s\n",
      "Epoch: 22, Batch: 511/842, Loss: 0.5479, Time: 0.75s\n",
      "Epoch: 22, Batch: 521/842, Loss: 0.5836, Time: 0.78s\n",
      "Epoch: 22, Batch: 531/842, Loss: 0.5374, Time: 0.78s\n",
      "Epoch: 22, Batch: 541/842, Loss: 0.4636, Time: 0.74s\n",
      "Epoch: 22, Batch: 551/842, Loss: 0.5077, Time: 0.75s\n",
      "Epoch: 22, Batch: 561/842, Loss: 0.6090, Time: 0.74s\n",
      "Epoch: 22, Batch: 571/842, Loss: 0.3642, Time: 0.73s\n",
      "Epoch: 22, Batch: 581/842, Loss: 0.4890, Time: 0.74s\n",
      "Epoch: 22, Batch: 591/842, Loss: 0.5860, Time: 0.76s\n",
      "Epoch: 22, Batch: 601/842, Loss: 0.5452, Time: 0.76s\n",
      "Epoch: 22, Batch: 611/842, Loss: 0.7112, Time: 0.75s\n",
      "Epoch: 22, Batch: 621/842, Loss: 0.6303, Time: 0.76s\n",
      "Epoch: 22, Batch: 631/842, Loss: 0.5206, Time: 0.75s\n",
      "Epoch: 22, Batch: 641/842, Loss: 0.5454, Time: 0.73s\n",
      "Epoch: 22, Batch: 651/842, Loss: 0.4493, Time: 0.73s\n",
      "Epoch: 22, Batch: 661/842, Loss: 0.6666, Time: 0.73s\n",
      "Epoch: 22, Batch: 671/842, Loss: 0.6058, Time: 0.76s\n",
      "Epoch: 22, Batch: 681/842, Loss: 0.5325, Time: 0.77s\n",
      "Epoch: 22, Batch: 691/842, Loss: 0.5523, Time: 0.79s\n",
      "Epoch: 22, Batch: 701/842, Loss: 0.5754, Time: 0.75s\n",
      "Epoch: 22, Batch: 711/842, Loss: 0.4161, Time: 0.77s\n",
      "Epoch: 22, Batch: 721/842, Loss: 0.4576, Time: 0.75s\n",
      "Epoch: 22, Batch: 731/842, Loss: 0.4393, Time: 0.76s\n",
      "Epoch: 22, Batch: 741/842, Loss: 0.6408, Time: 0.77s\n",
      "Epoch: 22, Batch: 751/842, Loss: 0.4618, Time: 0.80s\n",
      "Epoch: 22, Batch: 761/842, Loss: 0.4106, Time: 0.76s\n",
      "Epoch: 22, Batch: 771/842, Loss: 0.4672, Time: 0.76s\n",
      "Epoch: 22, Batch: 781/842, Loss: 0.3701, Time: 0.76s\n",
      "Epoch: 22, Batch: 791/842, Loss: 0.5610, Time: 0.78s\n",
      "Epoch: 22, Batch: 801/842, Loss: 0.4997, Time: 0.75s\n",
      "Epoch: 22, Batch: 811/842, Loss: 0.5462, Time: 0.81s\n",
      "Epoch: 22, Batch: 821/842, Loss: 0.4852, Time: 0.78s\n",
      "Epoch: 22, Batch: 831/842, Loss: 0.6143, Time: 0.75s\n",
      "Epoch: 22, Batch: 841/842, Loss: 0.5950, Time: 0.72s\n",
      "Epoch 22/100: Train Loss: 0.5367, Val Loss: 0.7162, mIoU: 0.5002, F1: 0.6485, OA: 0.9415\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.6868, F1=0.8143, OA=0.8014, Precision=0.7414, Recall=0.9031\n",
      "    Class 1: IoU=0.6083, F1=0.7565, OA=0.9952, Precision=0.8211, Recall=0.7013\n",
      "    Class 2: IoU=0.8039, F1=0.8913, OA=0.9590, Precision=0.9623, Recall=0.8300\n",
      "    Class 3: IoU=0.3927, F1=0.5639, OA=0.9567, Precision=0.7273, Recall=0.4605\n",
      "    Class 4: IoU=0.5461, F1=0.7064, OA=0.9526, Precision=0.6113, Recall=0.8366\n",
      "    Class 5: IoU=0.2723, F1=0.4281, OA=0.9928, Precision=0.7434, Recall=0.3006\n",
      "    Class 6: IoU=0.3589, F1=0.5282, OA=0.9324, Precision=0.6686, Recall=0.4365\n",
      "    Class 7: IoU=0.3329, F1=0.4995, OA=0.9415, Precision=0.7913, Recall=0.3649\n",
      "Epoch: 23, Batch: 1/842, Loss: 0.6517, Time: 1.70s\n",
      "Epoch: 23, Batch: 11/842, Loss: 0.6327, Time: 0.73s\n",
      "Epoch: 23, Batch: 21/842, Loss: 0.5508, Time: 0.72s\n",
      "Epoch: 23, Batch: 31/842, Loss: 0.5573, Time: 0.73s\n",
      "Epoch: 23, Batch: 41/842, Loss: 0.5082, Time: 0.72s\n",
      "Epoch: 23, Batch: 51/842, Loss: 0.4838, Time: 0.72s\n",
      "Epoch: 23, Batch: 61/842, Loss: 0.6290, Time: 0.72s\n",
      "Epoch: 23, Batch: 71/842, Loss: 0.5738, Time: 0.72s\n",
      "Epoch: 23, Batch: 81/842, Loss: 0.5021, Time: 0.72s\n",
      "Epoch: 23, Batch: 91/842, Loss: 0.4244, Time: 0.78s\n",
      "Epoch: 23, Batch: 101/842, Loss: 0.7301, Time: 0.76s\n",
      "Epoch: 23, Batch: 111/842, Loss: 0.3568, Time: 0.77s\n",
      "Epoch: 23, Batch: 121/842, Loss: 0.4755, Time: 0.76s\n",
      "Epoch: 23, Batch: 131/842, Loss: 0.4552, Time: 0.77s\n",
      "Epoch: 23, Batch: 141/842, Loss: 0.5234, Time: 0.76s\n",
      "Epoch: 23, Batch: 151/842, Loss: 0.5241, Time: 0.74s\n",
      "Epoch: 23, Batch: 161/842, Loss: 0.5739, Time: 0.74s\n",
      "Epoch: 23, Batch: 171/842, Loss: 0.4046, Time: 0.74s\n",
      "Epoch: 23, Batch: 181/842, Loss: 0.4342, Time: 0.74s\n",
      "Epoch: 23, Batch: 191/842, Loss: 0.5020, Time: 0.79s\n",
      "Epoch: 23, Batch: 201/842, Loss: 0.4800, Time: 0.78s\n",
      "Epoch: 23, Batch: 211/842, Loss: 0.5117, Time: 0.75s\n",
      "Epoch: 23, Batch: 221/842, Loss: 0.5699, Time: 0.77s\n",
      "Epoch: 23, Batch: 231/842, Loss: 0.5517, Time: 0.73s\n",
      "Epoch: 23, Batch: 241/842, Loss: 0.5491, Time: 0.75s\n",
      "Epoch: 23, Batch: 251/842, Loss: 0.5623, Time: 0.83s\n",
      "Epoch: 23, Batch: 261/842, Loss: 0.4439, Time: 0.80s\n",
      "Epoch: 23, Batch: 271/842, Loss: 0.5111, Time: 0.80s\n",
      "Epoch: 23, Batch: 281/842, Loss: 0.6001, Time: 0.79s\n",
      "Epoch: 23, Batch: 291/842, Loss: 0.5096, Time: 0.78s\n",
      "Epoch: 23, Batch: 301/842, Loss: 0.4737, Time: 0.71s\n",
      "Epoch: 23, Batch: 311/842, Loss: 0.6571, Time: 0.71s\n",
      "Epoch: 23, Batch: 321/842, Loss: 0.5911, Time: 0.77s\n",
      "Epoch: 23, Batch: 331/842, Loss: 0.5856, Time: 0.82s\n",
      "Epoch: 23, Batch: 341/842, Loss: 0.4094, Time: 0.78s\n",
      "Epoch: 23, Batch: 351/842, Loss: 0.5267, Time: 0.76s\n",
      "Epoch: 23, Batch: 361/842, Loss: 0.4542, Time: 0.75s\n",
      "Epoch: 23, Batch: 371/842, Loss: 0.3351, Time: 0.77s\n",
      "Epoch: 23, Batch: 381/842, Loss: 0.4744, Time: 0.74s\n",
      "Epoch: 23, Batch: 391/842, Loss: 0.4898, Time: 0.82s\n",
      "Epoch: 23, Batch: 401/842, Loss: 0.7079, Time: 0.83s\n",
      "Epoch: 23, Batch: 411/842, Loss: 0.5323, Time: 0.80s\n",
      "Epoch: 23, Batch: 421/842, Loss: 0.4822, Time: 0.78s\n",
      "Epoch: 23, Batch: 431/842, Loss: 0.4348, Time: 0.75s\n",
      "Epoch: 23, Batch: 441/842, Loss: 0.4871, Time: 0.76s\n",
      "Epoch: 23, Batch: 451/842, Loss: 0.5106, Time: 0.76s\n",
      "Epoch: 23, Batch: 461/842, Loss: 0.5264, Time: 0.78s\n",
      "Epoch: 23, Batch: 471/842, Loss: 0.6418, Time: 0.83s\n",
      "Epoch: 23, Batch: 481/842, Loss: 0.7201, Time: 0.81s\n",
      "Epoch: 23, Batch: 491/842, Loss: 0.5226, Time: 0.81s\n",
      "Epoch: 23, Batch: 501/842, Loss: 0.5990, Time: 0.77s\n",
      "Epoch: 23, Batch: 511/842, Loss: 0.4487, Time: 0.81s\n",
      "Epoch: 23, Batch: 521/842, Loss: 0.4598, Time: 0.81s\n",
      "Epoch: 23, Batch: 531/842, Loss: 0.4124, Time: 0.75s\n",
      "Epoch: 23, Batch: 541/842, Loss: 0.3729, Time: 0.73s\n",
      "Epoch: 23, Batch: 551/842, Loss: 0.3768, Time: 0.78s\n",
      "Epoch: 23, Batch: 561/842, Loss: 0.4063, Time: 0.81s\n",
      "Epoch: 23, Batch: 571/842, Loss: 0.5897, Time: 0.81s\n",
      "Epoch: 23, Batch: 581/842, Loss: 0.5235, Time: 0.77s\n",
      "Epoch: 23, Batch: 591/842, Loss: 0.6044, Time: 0.76s\n",
      "Epoch: 23, Batch: 601/842, Loss: 0.5351, Time: 0.76s\n",
      "Epoch: 23, Batch: 611/842, Loss: 0.5280, Time: 0.80s\n",
      "Epoch: 23, Batch: 621/842, Loss: 0.2728, Time: 0.83s\n",
      "Epoch: 23, Batch: 631/842, Loss: 0.4674, Time: 0.83s\n",
      "Epoch: 23, Batch: 641/842, Loss: 0.7669, Time: 0.75s\n",
      "Epoch: 23, Batch: 651/842, Loss: 0.7566, Time: 0.79s\n",
      "Epoch: 23, Batch: 661/842, Loss: 0.4769, Time: 0.84s\n",
      "Epoch: 23, Batch: 671/842, Loss: 0.4830, Time: 0.78s\n",
      "Epoch: 23, Batch: 681/842, Loss: 0.4118, Time: 0.81s\n",
      "Epoch: 23, Batch: 691/842, Loss: 0.5644, Time: 0.79s\n",
      "Epoch: 23, Batch: 701/842, Loss: 0.5384, Time: 0.77s\n",
      "Epoch: 23, Batch: 711/842, Loss: 0.4690, Time: 0.76s\n",
      "Epoch: 23, Batch: 721/842, Loss: 0.4849, Time: 0.76s\n",
      "Epoch: 23, Batch: 731/842, Loss: 0.4397, Time: 0.79s\n",
      "Epoch: 23, Batch: 741/842, Loss: 0.5811, Time: 0.75s\n",
      "Epoch: 23, Batch: 751/842, Loss: 0.5252, Time: 0.74s\n",
      "Epoch: 23, Batch: 761/842, Loss: 0.3730, Time: 0.73s\n",
      "Epoch: 23, Batch: 771/842, Loss: 0.6519, Time: 0.75s\n",
      "Epoch: 23, Batch: 781/842, Loss: 0.4461, Time: 0.75s\n",
      "Epoch: 23, Batch: 791/842, Loss: 0.5437, Time: 0.75s\n",
      "Epoch: 23, Batch: 801/842, Loss: 0.4762, Time: 0.74s\n",
      "Epoch: 23, Batch: 811/842, Loss: 0.5098, Time: 0.73s\n",
      "Epoch: 23, Batch: 821/842, Loss: 0.6391, Time: 0.73s\n",
      "Epoch: 23, Batch: 831/842, Loss: 0.6379, Time: 0.73s\n",
      "Epoch: 23, Batch: 841/842, Loss: 0.4213, Time: 0.72s\n",
      "Epoch 23/100: Train Loss: 0.5360, Val Loss: 0.5211, mIoU: 0.6220, F1: 0.7592, OA: 0.9566\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7384, F1=0.8495, OA=0.8418, Precision=0.7846, Recall=0.9260\n",
      "    Class 1: IoU=0.6273, F1=0.7710, OA=0.9951, Precision=0.7617, Recall=0.7805\n",
      "    Class 2: IoU=0.8789, F1=0.9355, OA=0.9745, Precision=0.9598, Recall=0.9125\n",
      "    Class 3: IoU=0.4226, F1=0.5942, OA=0.9600, Precision=0.7763, Recall=0.4812\n",
      "    Class 4: IoU=0.6269, F1=0.7707, OA=0.9723, Precision=0.8829, Recall=0.6838\n",
      "    Class 5: IoU=0.5859, F1=0.7389, OA=0.9953, Precision=0.7309, Recall=0.7471\n",
      "    Class 6: IoU=0.5008, F1=0.6673, OA=0.9529, Precision=0.8604, Recall=0.5450\n",
      "    Class 7: IoU=0.5951, F1=0.7462, OA=0.9612, Precision=0.7826, Recall=0.7130\n",
      "Epoch: 24, Batch: 1/842, Loss: 0.4982, Time: 1.72s\n",
      "Epoch: 24, Batch: 11/842, Loss: 0.5144, Time: 0.80s\n",
      "Epoch: 24, Batch: 21/842, Loss: 0.6252, Time: 0.82s\n",
      "Epoch: 24, Batch: 31/842, Loss: 0.7199, Time: 0.81s\n",
      "Epoch: 24, Batch: 41/842, Loss: 0.4851, Time: 0.81s\n",
      "Epoch: 24, Batch: 51/842, Loss: 0.5304, Time: 0.76s\n",
      "Epoch: 24, Batch: 61/842, Loss: 0.6023, Time: 0.77s\n",
      "Epoch: 24, Batch: 71/842, Loss: 0.5648, Time: 0.77s\n",
      "Epoch: 24, Batch: 81/842, Loss: 0.6037, Time: 0.73s\n",
      "Epoch: 24, Batch: 91/842, Loss: 0.5030, Time: 0.72s\n",
      "Epoch: 24, Batch: 101/842, Loss: 0.5504, Time: 0.73s\n",
      "Epoch: 24, Batch: 111/842, Loss: 0.5133, Time: 0.72s\n",
      "Epoch: 24, Batch: 121/842, Loss: 0.5555, Time: 0.73s\n",
      "Epoch: 24, Batch: 131/842, Loss: 0.6450, Time: 0.73s\n",
      "Epoch: 24, Batch: 141/842, Loss: 0.4292, Time: 0.74s\n",
      "Epoch: 24, Batch: 151/842, Loss: 0.5179, Time: 0.74s\n",
      "Epoch: 24, Batch: 161/842, Loss: 0.4590, Time: 0.73s\n",
      "Epoch: 24, Batch: 171/842, Loss: 0.5933, Time: 0.73s\n",
      "Epoch: 24, Batch: 181/842, Loss: 0.4861, Time: 0.74s\n",
      "Epoch: 24, Batch: 191/842, Loss: 0.4828, Time: 0.73s\n",
      "Epoch: 24, Batch: 201/842, Loss: 0.5457, Time: 0.73s\n",
      "Epoch: 24, Batch: 211/842, Loss: 0.5293, Time: 0.82s\n",
      "Epoch: 24, Batch: 221/842, Loss: 0.4603, Time: 0.76s\n",
      "Epoch: 24, Batch: 231/842, Loss: 0.4775, Time: 0.74s\n",
      "Epoch: 24, Batch: 241/842, Loss: 0.6742, Time: 0.76s\n",
      "Epoch: 24, Batch: 251/842, Loss: 0.6272, Time: 0.74s\n",
      "Epoch: 24, Batch: 261/842, Loss: 0.5914, Time: 0.74s\n",
      "Epoch: 24, Batch: 271/842, Loss: 0.5469, Time: 0.73s\n",
      "Epoch: 24, Batch: 281/842, Loss: 0.4216, Time: 0.75s\n",
      "Epoch: 24, Batch: 291/842, Loss: 0.5378, Time: 0.77s\n",
      "Epoch: 24, Batch: 301/842, Loss: 0.5530, Time: 0.79s\n",
      "Epoch: 24, Batch: 311/842, Loss: 0.7216, Time: 0.77s\n",
      "Epoch: 24, Batch: 321/842, Loss: 0.5646, Time: 0.73s\n",
      "Epoch: 24, Batch: 331/842, Loss: 0.4677, Time: 0.74s\n",
      "Epoch: 24, Batch: 341/842, Loss: 0.5625, Time: 0.76s\n",
      "Epoch: 24, Batch: 351/842, Loss: 0.4571, Time: 0.74s\n",
      "Epoch: 24, Batch: 361/842, Loss: 0.5395, Time: 0.73s\n",
      "Epoch: 24, Batch: 371/842, Loss: 0.5859, Time: 0.75s\n",
      "Epoch: 24, Batch: 381/842, Loss: 0.3919, Time: 0.74s\n",
      "Epoch: 24, Batch: 391/842, Loss: 0.6315, Time: 0.74s\n",
      "Epoch: 24, Batch: 401/842, Loss: 0.5412, Time: 0.73s\n",
      "Epoch: 24, Batch: 411/842, Loss: 0.4803, Time: 0.74s\n",
      "Epoch: 24, Batch: 421/842, Loss: 0.5904, Time: 0.73s\n",
      "Epoch: 24, Batch: 431/842, Loss: 0.5220, Time: 0.74s\n",
      "Epoch: 24, Batch: 441/842, Loss: 0.5092, Time: 0.75s\n",
      "Epoch: 24, Batch: 451/842, Loss: 0.5575, Time: 0.74s\n",
      "Epoch: 24, Batch: 461/842, Loss: 0.5154, Time: 0.74s\n",
      "Epoch: 24, Batch: 471/842, Loss: 0.6040, Time: 0.73s\n",
      "Epoch: 24, Batch: 481/842, Loss: 0.4440, Time: 0.74s\n",
      "Epoch: 24, Batch: 491/842, Loss: 0.7535, Time: 0.74s\n",
      "Epoch: 24, Batch: 501/842, Loss: 0.4413, Time: 0.74s\n",
      "Epoch: 24, Batch: 511/842, Loss: 0.6387, Time: 0.74s\n",
      "Epoch: 24, Batch: 521/842, Loss: 0.5378, Time: 0.75s\n",
      "Epoch: 24, Batch: 531/842, Loss: 0.6229, Time: 0.74s\n",
      "Epoch: 24, Batch: 541/842, Loss: 0.5142, Time: 0.75s\n",
      "Epoch: 24, Batch: 551/842, Loss: 0.5143, Time: 0.75s\n",
      "Epoch: 24, Batch: 561/842, Loss: 0.5661, Time: 0.75s\n",
      "Epoch: 24, Batch: 571/842, Loss: 0.4832, Time: 0.73s\n",
      "Epoch: 24, Batch: 581/842, Loss: 0.5601, Time: 0.73s\n",
      "Epoch: 24, Batch: 591/842, Loss: 0.4340, Time: 0.80s\n",
      "Epoch: 24, Batch: 601/842, Loss: 0.4518, Time: 0.90s\n",
      "Epoch: 24, Batch: 611/842, Loss: 0.4346, Time: 0.92s\n",
      "Epoch: 24, Batch: 621/842, Loss: 0.4267, Time: 0.81s\n",
      "Epoch: 24, Batch: 631/842, Loss: 0.5431, Time: 0.75s\n",
      "Epoch: 24, Batch: 641/842, Loss: 0.3008, Time: 0.73s\n",
      "Epoch: 24, Batch: 651/842, Loss: 0.5772, Time: 0.73s\n",
      "Epoch: 24, Batch: 661/842, Loss: 0.4595, Time: 0.72s\n",
      "Epoch: 24, Batch: 671/842, Loss: 0.4719, Time: 0.72s\n",
      "Epoch: 24, Batch: 681/842, Loss: 0.5064, Time: 0.72s\n",
      "Epoch: 24, Batch: 691/842, Loss: 0.4227, Time: 0.72s\n",
      "Epoch: 24, Batch: 701/842, Loss: 0.6156, Time: 0.72s\n",
      "Epoch: 24, Batch: 711/842, Loss: 0.5465, Time: 0.72s\n",
      "Epoch: 24, Batch: 721/842, Loss: 0.6218, Time: 0.72s\n",
      "Epoch: 24, Batch: 731/842, Loss: 0.5745, Time: 0.72s\n",
      "Epoch: 24, Batch: 741/842, Loss: 0.4561, Time: 0.74s\n",
      "Epoch: 24, Batch: 751/842, Loss: 0.7337, Time: 0.80s\n",
      "Epoch: 24, Batch: 761/842, Loss: 0.6550, Time: 0.78s\n",
      "Epoch: 24, Batch: 771/842, Loss: 0.5755, Time: 0.76s\n",
      "Epoch: 24, Batch: 781/842, Loss: 0.3276, Time: 0.76s\n",
      "Epoch: 24, Batch: 791/842, Loss: 0.5035, Time: 1.31s\n",
      "Epoch: 24, Batch: 801/842, Loss: 0.6769, Time: 0.77s\n",
      "Epoch: 24, Batch: 811/842, Loss: 0.5820, Time: 0.75s\n",
      "Epoch: 24, Batch: 821/842, Loss: 0.5268, Time: 0.78s\n",
      "Epoch: 24, Batch: 831/842, Loss: 0.4311, Time: 0.77s\n",
      "Epoch: 24, Batch: 841/842, Loss: 0.5631, Time: 0.73s\n",
      "Epoch 24/100: Train Loss: 0.5278, Val Loss: 0.4975, mIoU: 0.6349, F1: 0.7666, OA: 0.9593\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7517, F1=0.8583, OA=0.8508, Precision=0.7920, Recall=0.9367\n",
      "    Class 1: IoU=0.6574, F1=0.7933, OA=0.9959, Precision=0.8612, Recall=0.7353\n",
      "    Class 2: IoU=0.9034, F1=0.9492, OA=0.9796, Precision=0.9586, Recall=0.9400\n",
      "    Class 3: IoU=0.3717, F1=0.5419, OA=0.9590, Precision=0.8440, Recall=0.3991\n",
      "    Class 4: IoU=0.6876, F1=0.8149, OA=0.9768, Precision=0.8924, Recall=0.7497\n",
      "    Class 5: IoU=0.5845, F1=0.7378, OA=0.9955, Precision=0.7750, Recall=0.7039\n",
      "    Class 6: IoU=0.5299, F1=0.6928, OA=0.9537, Precision=0.8159, Recall=0.6019\n",
      "    Class 7: IoU=0.5929, F1=0.7444, OA=0.9632, Precision=0.8384, Recall=0.6693\n",
      "Saving best model with mIoU: 0.6349\n",
      "Epoch: 25, Batch: 1/842, Loss: 0.5677, Time: 1.61s\n",
      "Epoch: 25, Batch: 11/842, Loss: 0.3533, Time: 0.78s\n",
      "Epoch: 25, Batch: 21/842, Loss: 0.6994, Time: 0.74s\n",
      "Epoch: 25, Batch: 31/842, Loss: 0.5179, Time: 0.77s\n",
      "Epoch: 25, Batch: 41/842, Loss: 0.5573, Time: 0.74s\n",
      "Epoch: 25, Batch: 51/842, Loss: 0.6316, Time: 0.75s\n",
      "Epoch: 25, Batch: 61/842, Loss: 0.6960, Time: 0.73s\n",
      "Epoch: 25, Batch: 71/842, Loss: 0.5347, Time: 0.74s\n",
      "Epoch: 25, Batch: 81/842, Loss: 0.4462, Time: 0.74s\n",
      "Epoch: 25, Batch: 91/842, Loss: 0.4580, Time: 0.73s\n",
      "Epoch: 25, Batch: 101/842, Loss: 0.5803, Time: 0.72s\n",
      "Epoch: 25, Batch: 111/842, Loss: 0.5836, Time: 0.69s\n",
      "Epoch: 25, Batch: 121/842, Loss: 0.6656, Time: 0.68s\n",
      "Epoch: 25, Batch: 131/842, Loss: 0.4362, Time: 0.68s\n",
      "Epoch: 25, Batch: 141/842, Loss: 0.4431, Time: 0.68s\n",
      "Epoch: 25, Batch: 151/842, Loss: 0.6394, Time: 0.68s\n",
      "Epoch: 25, Batch: 161/842, Loss: 0.5150, Time: 0.68s\n",
      "Epoch: 25, Batch: 171/842, Loss: 0.4349, Time: 0.68s\n",
      "Epoch: 25, Batch: 181/842, Loss: 0.5382, Time: 0.68s\n",
      "Epoch: 25, Batch: 191/842, Loss: 0.6744, Time: 0.68s\n",
      "Epoch: 25, Batch: 201/842, Loss: 0.7855, Time: 0.72s\n",
      "Epoch: 25, Batch: 211/842, Loss: 0.4236, Time: 0.75s\n",
      "Epoch: 25, Batch: 221/842, Loss: 0.4846, Time: 0.73s\n",
      "Epoch: 25, Batch: 231/842, Loss: 0.5669, Time: 0.73s\n",
      "Epoch: 25, Batch: 241/842, Loss: 0.5029, Time: 0.72s\n",
      "Epoch: 25, Batch: 251/842, Loss: 0.4705, Time: 0.74s\n",
      "Epoch: 25, Batch: 261/842, Loss: 0.5485, Time: 0.76s\n",
      "Epoch: 25, Batch: 271/842, Loss: 0.5966, Time: 0.83s\n",
      "Epoch: 25, Batch: 281/842, Loss: 0.3883, Time: 0.80s\n",
      "Epoch: 25, Batch: 291/842, Loss: 0.5261, Time: 0.77s\n",
      "Epoch: 25, Batch: 301/842, Loss: 0.5807, Time: 0.74s\n",
      "Epoch: 25, Batch: 311/842, Loss: 0.5255, Time: 0.77s\n",
      "Epoch: 25, Batch: 321/842, Loss: 0.5306, Time: 0.75s\n",
      "Epoch: 25, Batch: 331/842, Loss: 0.5265, Time: 0.74s\n",
      "Epoch: 25, Batch: 341/842, Loss: 0.5046, Time: 0.74s\n",
      "Epoch: 25, Batch: 351/842, Loss: 0.6243, Time: 0.75s\n",
      "Epoch: 25, Batch: 361/842, Loss: 0.5145, Time: 0.75s\n",
      "Epoch: 25, Batch: 371/842, Loss: 0.6199, Time: 0.72s\n",
      "Epoch: 25, Batch: 381/842, Loss: 0.5524, Time: 0.71s\n",
      "Epoch: 25, Batch: 391/842, Loss: 0.5038, Time: 0.72s\n",
      "Epoch: 25, Batch: 401/842, Loss: 0.6089, Time: 0.71s\n",
      "Epoch: 25, Batch: 411/842, Loss: 0.6092, Time: 0.71s\n",
      "Epoch: 25, Batch: 421/842, Loss: 0.5028, Time: 0.71s\n",
      "Epoch: 25, Batch: 431/842, Loss: 0.6256, Time: 0.73s\n",
      "Epoch: 25, Batch: 441/842, Loss: 0.4780, Time: 0.72s\n",
      "Epoch: 25, Batch: 451/842, Loss: 0.5161, Time: 0.71s\n",
      "Epoch: 25, Batch: 461/842, Loss: 0.4463, Time: 0.72s\n",
      "Epoch: 25, Batch: 471/842, Loss: 0.6046, Time: 0.72s\n",
      "Epoch: 25, Batch: 481/842, Loss: 0.4140, Time: 0.74s\n",
      "Epoch: 25, Batch: 491/842, Loss: 0.5494, Time: 0.72s\n",
      "Epoch: 25, Batch: 501/842, Loss: 0.5621, Time: 0.73s\n",
      "Epoch: 25, Batch: 511/842, Loss: 0.6379, Time: 0.73s\n",
      "Epoch: 25, Batch: 521/842, Loss: 0.6046, Time: 0.71s\n",
      "Epoch: 25, Batch: 531/842, Loss: 0.6452, Time: 0.72s\n",
      "Epoch: 25, Batch: 541/842, Loss: 0.6140, Time: 0.72s\n",
      "Epoch: 25, Batch: 551/842, Loss: 0.5739, Time: 0.71s\n",
      "Epoch: 25, Batch: 561/842, Loss: 0.4641, Time: 0.72s\n",
      "Epoch: 25, Batch: 571/842, Loss: 0.4957, Time: 0.72s\n",
      "Epoch: 25, Batch: 581/842, Loss: 0.3841, Time: 0.71s\n",
      "Epoch: 25, Batch: 591/842, Loss: 0.4042, Time: 0.73s\n",
      "Epoch: 25, Batch: 601/842, Loss: 0.4763, Time: 0.74s\n",
      "Epoch: 25, Batch: 611/842, Loss: 0.5968, Time: 0.73s\n",
      "Epoch: 25, Batch: 621/842, Loss: 0.6097, Time: 0.74s\n",
      "Epoch: 25, Batch: 631/842, Loss: 0.5306, Time: 0.74s\n",
      "Epoch: 25, Batch: 641/842, Loss: 0.4427, Time: 0.74s\n",
      "Epoch: 25, Batch: 651/842, Loss: 0.5727, Time: 0.74s\n",
      "Epoch: 25, Batch: 661/842, Loss: 0.5660, Time: 0.73s\n",
      "Epoch: 25, Batch: 671/842, Loss: 0.5394, Time: 0.74s\n",
      "Epoch: 25, Batch: 681/842, Loss: 0.5709, Time: 0.74s\n",
      "Epoch: 25, Batch: 691/842, Loss: 0.6196, Time: 0.75s\n",
      "Epoch: 25, Batch: 701/842, Loss: 0.6209, Time: 0.76s\n",
      "Epoch: 25, Batch: 711/842, Loss: 0.4992, Time: 0.75s\n",
      "Epoch: 25, Batch: 721/842, Loss: 0.6493, Time: 0.75s\n",
      "Epoch: 25, Batch: 731/842, Loss: 0.5394, Time: 0.74s\n",
      "Epoch: 25, Batch: 741/842, Loss: 0.6060, Time: 0.74s\n",
      "Epoch: 25, Batch: 751/842, Loss: 0.4552, Time: 0.72s\n",
      "Epoch: 25, Batch: 761/842, Loss: 0.4698, Time: 0.73s\n",
      "Epoch: 25, Batch: 771/842, Loss: 0.3663, Time: 0.73s\n",
      "Epoch: 25, Batch: 781/842, Loss: 0.5691, Time: 0.73s\n",
      "Epoch: 25, Batch: 791/842, Loss: 0.4485, Time: 0.75s\n",
      "Epoch: 25, Batch: 801/842, Loss: 0.5953, Time: 0.77s\n",
      "Epoch: 25, Batch: 811/842, Loss: 0.5403, Time: 0.73s\n",
      "Epoch: 25, Batch: 821/842, Loss: 0.4409, Time: 0.71s\n",
      "Epoch: 25, Batch: 831/842, Loss: 0.5061, Time: 0.67s\n",
      "Epoch: 25, Batch: 841/842, Loss: 0.4292, Time: 0.67s\n",
      "Epoch 25/100: Train Loss: 0.5297, Val Loss: 0.5068, mIoU: 0.6288, F1: 0.7630, OA: 0.9585\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7509, F1=0.8578, OA=0.8557, Precision=0.8171, Recall=0.9026\n",
      "    Class 1: IoU=0.6443, F1=0.7837, OA=0.9957, Precision=0.8365, Recall=0.7371\n",
      "    Class 2: IoU=0.8998, F1=0.9473, OA=0.9786, Precision=0.9479, Recall=0.9466\n",
      "    Class 3: IoU=0.4221, F1=0.5936, OA=0.9582, Precision=0.7257, Recall=0.5022\n",
      "    Class 4: IoU=0.6918, F1=0.8179, OA=0.9750, Precision=0.8136, Recall=0.8221\n",
      "    Class 5: IoU=0.5390, F1=0.7004, OA=0.9943, Precision=0.6595, Recall=0.7467\n",
      "    Class 6: IoU=0.5050, F1=0.6711, OA=0.9501, Precision=0.7839, Recall=0.5866\n",
      "    Class 7: IoU=0.5779, F1=0.7325, OA=0.9599, Precision=0.7863, Recall=0.6855\n",
      "Epoch: 26, Batch: 1/842, Loss: 0.5684, Time: 1.79s\n",
      "Epoch: 26, Batch: 11/842, Loss: 0.4345, Time: 0.76s\n",
      "Epoch: 26, Batch: 21/842, Loss: 0.5338, Time: 0.72s\n",
      "Epoch: 26, Batch: 31/842, Loss: 0.4367, Time: 0.72s\n",
      "Epoch: 26, Batch: 41/842, Loss: 0.4991, Time: 0.72s\n",
      "Epoch: 26, Batch: 51/842, Loss: 0.6250, Time: 0.73s\n",
      "Epoch: 26, Batch: 61/842, Loss: 0.5786, Time: 0.73s\n",
      "Epoch: 26, Batch: 71/842, Loss: 0.7790, Time: 0.73s\n",
      "Epoch: 26, Batch: 81/842, Loss: 0.5112, Time: 0.72s\n",
      "Epoch: 26, Batch: 91/842, Loss: 0.4762, Time: 0.72s\n",
      "Epoch: 26, Batch: 101/842, Loss: 0.5526, Time: 0.74s\n",
      "Epoch: 26, Batch: 111/842, Loss: 0.5210, Time: 0.75s\n",
      "Epoch: 26, Batch: 121/842, Loss: 0.6009, Time: 0.72s\n",
      "Epoch: 26, Batch: 131/842, Loss: 0.4659, Time: 0.72s\n",
      "Epoch: 26, Batch: 141/842, Loss: 0.5023, Time: 0.73s\n",
      "Epoch: 26, Batch: 151/842, Loss: 0.2915, Time: 0.73s\n",
      "Epoch: 26, Batch: 161/842, Loss: 0.4469, Time: 0.73s\n",
      "Epoch: 26, Batch: 171/842, Loss: 0.5166, Time: 0.74s\n",
      "Epoch: 26, Batch: 181/842, Loss: 0.5047, Time: 0.79s\n",
      "Epoch: 26, Batch: 191/842, Loss: 0.3802, Time: 0.79s\n",
      "Epoch: 26, Batch: 201/842, Loss: 0.5149, Time: 0.77s\n",
      "Epoch: 26, Batch: 211/842, Loss: 0.4248, Time: 0.74s\n",
      "Epoch: 26, Batch: 221/842, Loss: 0.5343, Time: 0.75s\n",
      "Epoch: 26, Batch: 231/842, Loss: 0.5852, Time: 0.73s\n",
      "Epoch: 26, Batch: 241/842, Loss: 0.5484, Time: 0.75s\n",
      "Epoch: 26, Batch: 251/842, Loss: 0.4161, Time: 0.73s\n",
      "Epoch: 26, Batch: 261/842, Loss: 0.5994, Time: 0.73s\n",
      "Epoch: 26, Batch: 271/842, Loss: 0.4794, Time: 0.72s\n",
      "Epoch: 26, Batch: 281/842, Loss: 0.5494, Time: 0.72s\n",
      "Epoch: 26, Batch: 291/842, Loss: 0.5870, Time: 0.75s\n",
      "Epoch: 26, Batch: 301/842, Loss: 0.5330, Time: 0.73s\n",
      "Epoch: 26, Batch: 311/842, Loss: 0.3944, Time: 0.72s\n",
      "Epoch: 26, Batch: 321/842, Loss: 0.5416, Time: 0.75s\n",
      "Epoch: 26, Batch: 331/842, Loss: 0.4624, Time: 0.72s\n",
      "Epoch: 26, Batch: 341/842, Loss: 0.4722, Time: 0.73s\n",
      "Epoch: 26, Batch: 351/842, Loss: 0.4794, Time: 0.73s\n",
      "Epoch: 26, Batch: 361/842, Loss: 0.5807, Time: 0.72s\n",
      "Epoch: 26, Batch: 371/842, Loss: 0.4399, Time: 0.73s\n",
      "Epoch: 26, Batch: 381/842, Loss: 0.6608, Time: 0.72s\n",
      "Epoch: 26, Batch: 391/842, Loss: 0.4429, Time: 0.74s\n",
      "Epoch: 26, Batch: 401/842, Loss: 0.4505, Time: 0.90s\n",
      "Epoch: 26, Batch: 411/842, Loss: 0.6727, Time: 0.79s\n",
      "Epoch: 26, Batch: 421/842, Loss: 0.5462, Time: 0.77s\n",
      "Epoch: 26, Batch: 431/842, Loss: 0.4385, Time: 0.78s\n",
      "Epoch: 26, Batch: 441/842, Loss: 0.3762, Time: 0.81s\n",
      "Epoch: 26, Batch: 451/842, Loss: 0.4823, Time: 0.77s\n",
      "Epoch: 26, Batch: 461/842, Loss: 0.6034, Time: 0.74s\n",
      "Epoch: 26, Batch: 471/842, Loss: 0.4458, Time: 0.74s\n",
      "Epoch: 26, Batch: 481/842, Loss: 0.5680, Time: 0.73s\n",
      "Epoch: 26, Batch: 491/842, Loss: 0.5207, Time: 0.76s\n",
      "Epoch: 26, Batch: 501/842, Loss: 0.4625, Time: 0.76s\n",
      "Epoch: 26, Batch: 511/842, Loss: 0.4537, Time: 0.79s\n",
      "Epoch: 26, Batch: 521/842, Loss: 0.4395, Time: 0.74s\n",
      "Epoch: 26, Batch: 531/842, Loss: 0.5215, Time: 0.73s\n",
      "Epoch: 26, Batch: 541/842, Loss: 0.5316, Time: 0.75s\n",
      "Epoch: 26, Batch: 551/842, Loss: 0.4447, Time: 0.76s\n",
      "Epoch: 26, Batch: 561/842, Loss: 0.5665, Time: 0.75s\n",
      "Epoch: 26, Batch: 571/842, Loss: 0.7243, Time: 0.79s\n",
      "Epoch: 26, Batch: 581/842, Loss: 0.6079, Time: 0.75s\n",
      "Epoch: 26, Batch: 591/842, Loss: 0.5842, Time: 0.74s\n",
      "Epoch: 26, Batch: 601/842, Loss: 0.7027, Time: 0.76s\n",
      "Epoch: 26, Batch: 611/842, Loss: 0.4815, Time: 0.80s\n",
      "Epoch: 26, Batch: 621/842, Loss: 0.3425, Time: 0.74s\n",
      "Epoch: 26, Batch: 631/842, Loss: 0.6281, Time: 0.76s\n",
      "Epoch: 26, Batch: 641/842, Loss: 0.4016, Time: 0.75s\n",
      "Epoch: 26, Batch: 651/842, Loss: 0.7442, Time: 0.75s\n",
      "Epoch: 26, Batch: 661/842, Loss: 0.4737, Time: 0.71s\n",
      "Epoch: 26, Batch: 671/842, Loss: 0.3965, Time: 0.72s\n",
      "Epoch: 26, Batch: 681/842, Loss: 0.6097, Time: 0.72s\n",
      "Epoch: 26, Batch: 691/842, Loss: 0.5987, Time: 0.83s\n",
      "Epoch: 26, Batch: 701/842, Loss: 0.6002, Time: 0.84s\n",
      "Epoch: 26, Batch: 711/842, Loss: 0.5197, Time: 0.80s\n",
      "Epoch: 26, Batch: 721/842, Loss: 0.6305, Time: 0.82s\n",
      "Epoch: 26, Batch: 731/842, Loss: 0.5662, Time: 0.80s\n",
      "Epoch: 26, Batch: 741/842, Loss: 0.4934, Time: 0.79s\n",
      "Epoch: 26, Batch: 751/842, Loss: 0.4905, Time: 0.81s\n",
      "Epoch: 26, Batch: 761/842, Loss: 0.4516, Time: 0.78s\n",
      "Epoch: 26, Batch: 771/842, Loss: 0.5485, Time: 0.81s\n",
      "Epoch: 26, Batch: 781/842, Loss: 0.5567, Time: 0.80s\n",
      "Epoch: 26, Batch: 791/842, Loss: 0.5241, Time: 0.79s\n",
      "Epoch: 26, Batch: 801/842, Loss: 0.5565, Time: 0.79s\n",
      "Epoch: 26, Batch: 811/842, Loss: 0.6106, Time: 0.80s\n",
      "Epoch: 26, Batch: 821/842, Loss: 0.6130, Time: 0.79s\n",
      "Epoch: 26, Batch: 831/842, Loss: 0.5085, Time: 0.80s\n",
      "Epoch: 26, Batch: 841/842, Loss: 0.7828, Time: 0.80s\n",
      "Epoch 26/100: Train Loss: 0.5251, Val Loss: 0.5337, mIoU: 0.6016, F1: 0.7415, OA: 0.9556\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7426, F1=0.8523, OA=0.8477, Precision=0.8004, Recall=0.9114\n",
      "    Class 1: IoU=0.4952, F1=0.6624, OA=0.9917, Precision=0.5826, Recall=0.7676\n",
      "    Class 2: IoU=0.8687, F1=0.9297, OA=0.9726, Precision=0.9670, Recall=0.8952\n",
      "    Class 3: IoU=0.4219, F1=0.5934, OA=0.9560, Precision=0.6763, Recall=0.5286\n",
      "    Class 4: IoU=0.7120, F1=0.8318, OA=0.9776, Precision=0.8517, Recall=0.8127\n",
      "    Class 5: IoU=0.5513, F1=0.7107, OA=0.9944, Precision=0.6551, Recall=0.7767\n",
      "    Class 6: IoU=0.5170, F1=0.6816, OA=0.9474, Precision=0.7172, Recall=0.6494\n",
      "    Class 7: IoU=0.5038, F1=0.6701, OA=0.9578, Precision=0.8966, Recall=0.5349\n",
      "Epoch: 27, Batch: 1/842, Loss: 0.5828, Time: 1.69s\n",
      "Epoch: 27, Batch: 11/842, Loss: 0.4671, Time: 0.76s\n",
      "Epoch: 27, Batch: 21/842, Loss: 0.5125, Time: 0.76s\n",
      "Epoch: 27, Batch: 31/842, Loss: 0.6058, Time: 0.74s\n",
      "Epoch: 27, Batch: 41/842, Loss: 0.5938, Time: 0.73s\n",
      "Epoch: 27, Batch: 51/842, Loss: 0.5374, Time: 0.76s\n",
      "Epoch: 27, Batch: 61/842, Loss: 0.4309, Time: 0.77s\n",
      "Epoch: 27, Batch: 71/842, Loss: 0.5467, Time: 0.84s\n",
      "Epoch: 27, Batch: 81/842, Loss: 0.4493, Time: 0.82s\n",
      "Epoch: 27, Batch: 91/842, Loss: 0.6153, Time: 0.78s\n",
      "Epoch: 27, Batch: 101/842, Loss: 0.4347, Time: 0.77s\n",
      "Epoch: 27, Batch: 111/842, Loss: 0.4822, Time: 0.81s\n",
      "Epoch: 27, Batch: 121/842, Loss: 0.4932, Time: 0.81s\n",
      "Epoch: 27, Batch: 131/842, Loss: 0.3380, Time: 0.79s\n",
      "Epoch: 27, Batch: 141/842, Loss: 0.5485, Time: 0.79s\n",
      "Epoch: 27, Batch: 151/842, Loss: 0.5320, Time: 0.79s\n",
      "Epoch: 27, Batch: 161/842, Loss: 0.5637, Time: 0.79s\n",
      "Epoch: 27, Batch: 171/842, Loss: 0.5109, Time: 0.75s\n",
      "Epoch: 27, Batch: 181/842, Loss: 0.5053, Time: 0.78s\n",
      "Epoch: 27, Batch: 191/842, Loss: 0.6273, Time: 0.81s\n",
      "Epoch: 27, Batch: 201/842, Loss: 0.5435, Time: 0.80s\n",
      "Epoch: 27, Batch: 211/842, Loss: 0.4056, Time: 0.76s\n",
      "Epoch: 27, Batch: 221/842, Loss: 0.4663, Time: 0.74s\n",
      "Epoch: 27, Batch: 231/842, Loss: 0.6375, Time: 0.73s\n",
      "Epoch: 27, Batch: 241/842, Loss: 0.5003, Time: 0.73s\n",
      "Epoch: 27, Batch: 251/842, Loss: 0.4607, Time: 0.77s\n",
      "Epoch: 27, Batch: 261/842, Loss: 0.4957, Time: 0.79s\n",
      "Epoch: 27, Batch: 271/842, Loss: 0.5718, Time: 0.80s\n",
      "Epoch: 27, Batch: 281/842, Loss: 0.4961, Time: 0.77s\n",
      "Epoch: 27, Batch: 291/842, Loss: 0.6179, Time: 0.70s\n",
      "Epoch: 27, Batch: 301/842, Loss: 0.5093, Time: 0.70s\n",
      "Epoch: 27, Batch: 311/842, Loss: 0.3690, Time: 0.70s\n",
      "Epoch: 27, Batch: 321/842, Loss: 0.4769, Time: 0.76s\n",
      "Epoch: 27, Batch: 331/842, Loss: 0.5348, Time: 0.70s\n",
      "Epoch: 27, Batch: 341/842, Loss: 0.4382, Time: 0.72s\n",
      "Epoch: 27, Batch: 351/842, Loss: 0.3590, Time: 0.76s\n",
      "Epoch: 27, Batch: 361/842, Loss: 0.4820, Time: 0.76s\n",
      "Epoch: 27, Batch: 371/842, Loss: 0.4806, Time: 0.73s\n",
      "Epoch: 27, Batch: 381/842, Loss: 0.5305, Time: 0.73s\n",
      "Epoch: 27, Batch: 391/842, Loss: 0.3919, Time: 0.73s\n",
      "Epoch: 27, Batch: 401/842, Loss: 0.6060, Time: 0.76s\n",
      "Epoch: 27, Batch: 411/842, Loss: 0.4884, Time: 0.78s\n",
      "Epoch: 27, Batch: 421/842, Loss: 0.4492, Time: 0.74s\n",
      "Epoch: 27, Batch: 431/842, Loss: 0.4480, Time: 0.74s\n",
      "Epoch: 27, Batch: 441/842, Loss: 0.5429, Time: 0.74s\n",
      "Epoch: 27, Batch: 451/842, Loss: 0.5319, Time: 0.80s\n",
      "Epoch: 27, Batch: 461/842, Loss: 0.4697, Time: 0.75s\n",
      "Epoch: 27, Batch: 471/842, Loss: 0.4974, Time: 0.76s\n",
      "Epoch: 27, Batch: 481/842, Loss: 0.4351, Time: 0.78s\n",
      "Epoch: 27, Batch: 491/842, Loss: 0.6944, Time: 0.74s\n",
      "Epoch: 27, Batch: 501/842, Loss: 0.6353, Time: 0.73s\n",
      "Epoch: 27, Batch: 511/842, Loss: 0.4825, Time: 0.73s\n",
      "Epoch: 27, Batch: 521/842, Loss: 0.4186, Time: 0.73s\n",
      "Epoch: 27, Batch: 531/842, Loss: 0.5638, Time: 0.75s\n",
      "Epoch: 27, Batch: 541/842, Loss: 0.4683, Time: 0.73s\n",
      "Epoch: 27, Batch: 551/842, Loss: 0.5908, Time: 0.75s\n",
      "Epoch: 27, Batch: 561/842, Loss: 0.4711, Time: 0.75s\n",
      "Epoch: 27, Batch: 571/842, Loss: 0.4664, Time: 0.79s\n",
      "Epoch: 27, Batch: 581/842, Loss: 0.4982, Time: 0.87s\n",
      "Epoch: 27, Batch: 591/842, Loss: 0.4589, Time: 0.80s\n",
      "Epoch: 27, Batch: 601/842, Loss: 0.6216, Time: 0.72s\n",
      "Epoch: 27, Batch: 611/842, Loss: 0.5431, Time: 0.74s\n",
      "Epoch: 27, Batch: 621/842, Loss: 0.4537, Time: 0.73s\n",
      "Epoch: 27, Batch: 631/842, Loss: 0.5013, Time: 0.73s\n",
      "Epoch: 27, Batch: 641/842, Loss: 0.4572, Time: 0.77s\n",
      "Epoch: 27, Batch: 651/842, Loss: 0.5704, Time: 0.84s\n",
      "Epoch: 27, Batch: 661/842, Loss: 0.5429, Time: 0.78s\n",
      "Epoch: 27, Batch: 671/842, Loss: 0.5955, Time: 0.79s\n",
      "Epoch: 27, Batch: 681/842, Loss: 0.5049, Time: 0.78s\n",
      "Epoch: 27, Batch: 691/842, Loss: 0.3612, Time: 0.74s\n",
      "Epoch: 27, Batch: 701/842, Loss: 0.5311, Time: 0.77s\n",
      "Epoch: 27, Batch: 711/842, Loss: 0.6167, Time: 0.75s\n",
      "Epoch: 27, Batch: 721/842, Loss: 0.4986, Time: 0.75s\n",
      "Epoch: 27, Batch: 731/842, Loss: 0.4930, Time: 0.74s\n",
      "Epoch: 27, Batch: 741/842, Loss: 0.6571, Time: 0.75s\n",
      "Epoch: 27, Batch: 751/842, Loss: 0.5431, Time: 0.74s\n",
      "Epoch: 27, Batch: 761/842, Loss: 0.7157, Time: 0.74s\n",
      "Epoch: 27, Batch: 771/842, Loss: 0.5239, Time: 0.73s\n",
      "Epoch: 27, Batch: 781/842, Loss: 0.5407, Time: 0.73s\n",
      "Epoch: 27, Batch: 791/842, Loss: 0.5375, Time: 0.75s\n",
      "Epoch: 27, Batch: 801/842, Loss: 0.3713, Time: 0.75s\n",
      "Epoch: 27, Batch: 811/842, Loss: 0.4977, Time: 0.75s\n",
      "Epoch: 27, Batch: 821/842, Loss: 0.5417, Time: 0.74s\n",
      "Epoch: 27, Batch: 831/842, Loss: 0.5176, Time: 0.73s\n",
      "Epoch: 27, Batch: 841/842, Loss: 0.6238, Time: 0.72s\n",
      "Epoch 27/100: Train Loss: 0.5182, Val Loss: 0.5340, mIoU: 0.5949, F1: 0.7337, OA: 0.9561\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7390, F1=0.8499, OA=0.8410, Precision=0.7796, Recall=0.9342\n",
      "    Class 1: IoU=0.6070, F1=0.7554, OA=0.9952, Precision=0.8323, Recall=0.6916\n",
      "    Class 2: IoU=0.8974, F1=0.9459, OA=0.9783, Precision=0.9533, Recall=0.9387\n",
      "    Class 3: IoU=0.3618, F1=0.5313, OA=0.9564, Precision=0.7662, Recall=0.4067\n",
      "    Class 4: IoU=0.6718, F1=0.8037, OA=0.9758, Precision=0.9000, Recall=0.7260\n",
      "    Class 5: IoU=0.4540, F1=0.6245, OA=0.9947, Precision=0.8539, Recall=0.4923\n",
      "    Class 6: IoU=0.5098, F1=0.6753, OA=0.9484, Precision=0.7430, Recall=0.6189\n",
      "    Class 7: IoU=0.5187, F1=0.6831, OA=0.9590, Precision=0.8962, Recall=0.5519\n",
      "Epoch: 28, Batch: 1/842, Loss: 0.4834, Time: 1.69s\n",
      "Epoch: 28, Batch: 11/842, Loss: 0.4429, Time: 0.83s\n",
      "Epoch: 28, Batch: 21/842, Loss: 0.5948, Time: 0.82s\n",
      "Epoch: 28, Batch: 31/842, Loss: 0.5354, Time: 0.82s\n",
      "Epoch: 28, Batch: 41/842, Loss: 0.7430, Time: 0.74s\n",
      "Epoch: 28, Batch: 51/842, Loss: 0.5114, Time: 0.72s\n",
      "Epoch: 28, Batch: 61/842, Loss: 0.6016, Time: 0.72s\n",
      "Epoch: 28, Batch: 71/842, Loss: 0.5417, Time: 0.77s\n",
      "Epoch: 28, Batch: 81/842, Loss: 0.6310, Time: 0.76s\n",
      "Epoch: 28, Batch: 91/842, Loss: 0.5552, Time: 0.79s\n",
      "Epoch: 28, Batch: 101/842, Loss: 0.5545, Time: 0.77s\n",
      "Epoch: 28, Batch: 111/842, Loss: 0.5917, Time: 0.76s\n",
      "Epoch: 28, Batch: 121/842, Loss: 0.4642, Time: 0.77s\n",
      "Epoch: 28, Batch: 131/842, Loss: 0.3971, Time: 0.72s\n",
      "Epoch: 28, Batch: 141/842, Loss: 0.4960, Time: 0.74s\n",
      "Epoch: 28, Batch: 151/842, Loss: 0.6292, Time: 0.74s\n",
      "Epoch: 28, Batch: 161/842, Loss: 0.5315, Time: 0.86s\n",
      "Epoch: 28, Batch: 171/842, Loss: 0.4116, Time: 0.84s\n",
      "Epoch: 28, Batch: 181/842, Loss: 0.3243, Time: 0.82s\n",
      "Epoch: 28, Batch: 191/842, Loss: 0.5703, Time: 0.77s\n",
      "Epoch: 28, Batch: 201/842, Loss: 0.4370, Time: 0.73s\n",
      "Epoch: 28, Batch: 211/842, Loss: 0.6477, Time: 0.73s\n",
      "Epoch: 28, Batch: 221/842, Loss: 0.6874, Time: 0.74s\n",
      "Epoch: 28, Batch: 231/842, Loss: 0.5518, Time: 0.72s\n",
      "Epoch: 28, Batch: 241/842, Loss: 0.5126, Time: 0.73s\n",
      "Epoch: 28, Batch: 251/842, Loss: 0.4698, Time: 0.82s\n",
      "Epoch: 28, Batch: 261/842, Loss: 0.6427, Time: 1.35s\n",
      "Epoch: 28, Batch: 271/842, Loss: 0.5924, Time: 0.82s\n",
      "Epoch: 28, Batch: 281/842, Loss: 0.4902, Time: 0.81s\n",
      "Epoch: 28, Batch: 291/842, Loss: 0.6247, Time: 0.81s\n",
      "Epoch: 28, Batch: 301/842, Loss: 0.5648, Time: 0.83s\n",
      "Epoch: 28, Batch: 311/842, Loss: 0.4384, Time: 0.76s\n",
      "Epoch: 28, Batch: 321/842, Loss: 0.4367, Time: 0.73s\n",
      "Epoch: 28, Batch: 331/842, Loss: 0.4569, Time: 0.78s\n",
      "Epoch: 28, Batch: 341/842, Loss: 0.5657, Time: 0.77s\n",
      "Epoch: 28, Batch: 351/842, Loss: 0.6051, Time: 0.78s\n",
      "Epoch: 28, Batch: 361/842, Loss: 0.5098, Time: 0.76s\n",
      "Epoch: 28, Batch: 371/842, Loss: 0.4221, Time: 0.77s\n",
      "Epoch: 28, Batch: 381/842, Loss: 0.6069, Time: 0.74s\n",
      "Epoch: 28, Batch: 391/842, Loss: 0.5166, Time: 0.74s\n",
      "Epoch: 28, Batch: 401/842, Loss: 0.6142, Time: 0.73s\n",
      "Epoch: 28, Batch: 411/842, Loss: 0.4323, Time: 0.74s\n",
      "Epoch: 28, Batch: 421/842, Loss: 0.4512, Time: 0.74s\n",
      "Epoch: 28, Batch: 431/842, Loss: 0.5826, Time: 0.73s\n",
      "Epoch: 28, Batch: 441/842, Loss: 0.6014, Time: 0.77s\n",
      "Epoch: 28, Batch: 451/842, Loss: 0.4520, Time: 0.75s\n",
      "Epoch: 28, Batch: 461/842, Loss: 0.5147, Time: 0.75s\n",
      "Epoch: 28, Batch: 471/842, Loss: 0.5075, Time: 0.73s\n",
      "Epoch: 28, Batch: 481/842, Loss: 0.4142, Time: 0.73s\n",
      "Epoch: 28, Batch: 491/842, Loss: 0.4430, Time: 0.74s\n",
      "Epoch: 28, Batch: 501/842, Loss: 0.4974, Time: 0.75s\n",
      "Epoch: 28, Batch: 511/842, Loss: 0.4724, Time: 0.74s\n",
      "Epoch: 28, Batch: 521/842, Loss: 0.4811, Time: 0.73s\n",
      "Epoch: 28, Batch: 531/842, Loss: 0.5090, Time: 0.74s\n",
      "Epoch: 28, Batch: 541/842, Loss: 0.7277, Time: 0.83s\n",
      "Epoch: 28, Batch: 551/842, Loss: 0.6280, Time: 0.86s\n",
      "Epoch: 28, Batch: 561/842, Loss: 0.3534, Time: 0.84s\n",
      "Epoch: 28, Batch: 571/842, Loss: 0.3413, Time: 0.84s\n",
      "Epoch: 28, Batch: 581/842, Loss: 0.5801, Time: 0.84s\n",
      "Epoch: 28, Batch: 591/842, Loss: 0.4201, Time: 0.83s\n",
      "Epoch: 28, Batch: 601/842, Loss: 0.4289, Time: 0.82s\n",
      "Epoch: 28, Batch: 611/842, Loss: 0.5038, Time: 0.87s\n",
      "Epoch: 28, Batch: 621/842, Loss: 0.6514, Time: 0.83s\n",
      "Epoch: 28, Batch: 631/842, Loss: 0.5311, Time: 0.83s\n",
      "Epoch: 28, Batch: 641/842, Loss: 0.4772, Time: 0.82s\n",
      "Epoch: 28, Batch: 651/842, Loss: 0.4575, Time: 0.82s\n",
      "Epoch: 28, Batch: 661/842, Loss: 0.4568, Time: 0.83s\n",
      "Epoch: 28, Batch: 671/842, Loss: 0.5379, Time: 0.76s\n",
      "Epoch: 28, Batch: 681/842, Loss: 0.5356, Time: 0.72s\n",
      "Epoch: 28, Batch: 691/842, Loss: 0.5777, Time: 0.72s\n",
      "Epoch: 28, Batch: 701/842, Loss: 0.5306, Time: 0.72s\n",
      "Epoch: 28, Batch: 711/842, Loss: 0.4578, Time: 0.73s\n",
      "Epoch: 28, Batch: 721/842, Loss: 0.4389, Time: 0.76s\n",
      "Epoch: 28, Batch: 731/842, Loss: 0.5306, Time: 0.70s\n",
      "Epoch: 28, Batch: 741/842, Loss: 0.5683, Time: 0.74s\n",
      "Epoch: 28, Batch: 751/842, Loss: 0.4991, Time: 0.76s\n",
      "Epoch: 28, Batch: 761/842, Loss: 0.5079, Time: 0.81s\n",
      "Epoch: 28, Batch: 771/842, Loss: 0.5225, Time: 0.77s\n",
      "Epoch: 28, Batch: 781/842, Loss: 0.4379, Time: 0.77s\n",
      "Epoch: 28, Batch: 791/842, Loss: 0.5302, Time: 0.77s\n",
      "Epoch: 28, Batch: 801/842, Loss: 0.5523, Time: 0.74s\n",
      "Epoch: 28, Batch: 811/842, Loss: 0.3703, Time: 0.75s\n",
      "Epoch: 28, Batch: 821/842, Loss: 0.7080, Time: 0.75s\n",
      "Epoch: 28, Batch: 831/842, Loss: 0.5261, Time: 0.73s\n",
      "Epoch: 28, Batch: 841/842, Loss: 0.6082, Time: 0.72s\n",
      "Epoch 28/100: Train Loss: 0.5183, Val Loss: 0.4869, mIoU: 0.6473, F1: 0.7774, OA: 0.9604\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7586, F1=0.8628, OA=0.8580, Precision=0.8078, Recall=0.9257\n",
      "    Class 1: IoU=0.6586, F1=0.7941, OA=0.9960, Precision=0.8676, Recall=0.7322\n",
      "    Class 2: IoU=0.8997, F1=0.9472, OA=0.9788, Precision=0.9557, Recall=0.9389\n",
      "    Class 3: IoU=0.4195, F1=0.5910, OA=0.9602, Precision=0.7883, Recall=0.4727\n",
      "    Class 4: IoU=0.7153, F1=0.8340, OA=0.9788, Precision=0.8942, Recall=0.7814\n",
      "    Class 5: IoU=0.5947, F1=0.7458, OA=0.9954, Precision=0.7372, Recall=0.7547\n",
      "    Class 6: IoU=0.5277, F1=0.6909, OA=0.9523, Precision=0.7880, Recall=0.6151\n",
      "    Class 7: IoU=0.6044, F1=0.7534, OA=0.9638, Precision=0.8290, Recall=0.6905\n",
      "Saving best model with mIoU: 0.6473\n",
      "Epoch: 29, Batch: 1/842, Loss: 0.4303, Time: 2.08s\n",
      "Epoch: 29, Batch: 11/842, Loss: 0.6177, Time: 0.80s\n",
      "Epoch: 29, Batch: 21/842, Loss: 0.3820, Time: 0.82s\n",
      "Epoch: 29, Batch: 31/842, Loss: 0.4220, Time: 0.81s\n",
      "Epoch: 29, Batch: 41/842, Loss: 0.4380, Time: 0.79s\n",
      "Epoch: 29, Batch: 51/842, Loss: 0.4887, Time: 0.77s\n",
      "Epoch: 29, Batch: 61/842, Loss: 0.5530, Time: 0.78s\n",
      "Epoch: 29, Batch: 71/842, Loss: 0.3340, Time: 0.78s\n",
      "Epoch: 29, Batch: 81/842, Loss: 0.5276, Time: 0.76s\n",
      "Epoch: 29, Batch: 91/842, Loss: 0.5155, Time: 0.76s\n",
      "Epoch: 29, Batch: 101/842, Loss: 0.5896, Time: 0.76s\n",
      "Epoch: 29, Batch: 111/842, Loss: 0.5005, Time: 0.75s\n",
      "Epoch: 29, Batch: 121/842, Loss: 0.4889, Time: 0.72s\n",
      "Epoch: 29, Batch: 131/842, Loss: 0.4318, Time: 0.73s\n",
      "Epoch: 29, Batch: 141/842, Loss: 0.5414, Time: 0.74s\n",
      "Epoch: 29, Batch: 151/842, Loss: 0.5633, Time: 0.74s\n",
      "Epoch: 29, Batch: 161/842, Loss: 0.5223, Time: 0.74s\n",
      "Epoch: 29, Batch: 171/842, Loss: 0.4864, Time: 0.78s\n",
      "Epoch: 29, Batch: 181/842, Loss: 0.5500, Time: 0.80s\n",
      "Epoch: 29, Batch: 191/842, Loss: 0.3661, Time: 0.80s\n",
      "Epoch: 29, Batch: 201/842, Loss: 0.4269, Time: 0.78s\n",
      "Epoch: 29, Batch: 211/842, Loss: 0.6370, Time: 0.76s\n",
      "Epoch: 29, Batch: 221/842, Loss: 0.4250, Time: 0.75s\n",
      "Epoch: 29, Batch: 231/842, Loss: 0.6704, Time: 0.73s\n",
      "Epoch: 29, Batch: 241/842, Loss: 0.5617, Time: 0.76s\n",
      "Epoch: 29, Batch: 251/842, Loss: 0.4249, Time: 0.73s\n",
      "Epoch: 29, Batch: 261/842, Loss: 0.4889, Time: 0.73s\n",
      "Epoch: 29, Batch: 271/842, Loss: 0.5091, Time: 0.73s\n",
      "Epoch: 29, Batch: 281/842, Loss: 0.5207, Time: 0.74s\n",
      "Epoch: 29, Batch: 291/842, Loss: 0.5381, Time: 0.73s\n",
      "Epoch: 29, Batch: 301/842, Loss: 0.7203, Time: 0.74s\n",
      "Epoch: 29, Batch: 311/842, Loss: 0.4315, Time: 0.82s\n",
      "Epoch: 29, Batch: 321/842, Loss: 0.4388, Time: 0.77s\n",
      "Epoch: 29, Batch: 331/842, Loss: 0.4249, Time: 0.75s\n",
      "Epoch: 29, Batch: 341/842, Loss: 0.5663, Time: 0.74s\n",
      "Epoch: 29, Batch: 351/842, Loss: 0.4127, Time: 0.72s\n",
      "Epoch: 29, Batch: 361/842, Loss: 0.5727, Time: 0.71s\n",
      "Epoch: 29, Batch: 371/842, Loss: 0.3738, Time: 0.72s\n",
      "Epoch: 29, Batch: 381/842, Loss: 0.5866, Time: 0.71s\n",
      "Epoch: 29, Batch: 391/842, Loss: 0.7094, Time: 0.71s\n",
      "Epoch: 29, Batch: 401/842, Loss: 0.4893, Time: 0.72s\n",
      "Epoch: 29, Batch: 411/842, Loss: 0.5939, Time: 0.73s\n",
      "Epoch: 29, Batch: 421/842, Loss: 0.5266, Time: 0.72s\n",
      "Epoch: 29, Batch: 431/842, Loss: 0.4426, Time: 0.72s\n",
      "Epoch: 29, Batch: 441/842, Loss: 0.6618, Time: 0.72s\n",
      "Epoch: 29, Batch: 451/842, Loss: 0.3881, Time: 0.81s\n",
      "Epoch: 29, Batch: 461/842, Loss: 0.5091, Time: 0.85s\n",
      "Epoch: 29, Batch: 471/842, Loss: 0.6244, Time: 0.79s\n",
      "Epoch: 29, Batch: 481/842, Loss: 0.4172, Time: 0.88s\n",
      "Epoch: 29, Batch: 491/842, Loss: 0.5457, Time: 0.89s\n",
      "Epoch: 29, Batch: 501/842, Loss: 0.5909, Time: 0.92s\n",
      "Epoch: 29, Batch: 511/842, Loss: 0.5262, Time: 1.00s\n",
      "Epoch: 29, Batch: 521/842, Loss: 0.5321, Time: 0.97s\n",
      "Epoch: 29, Batch: 531/842, Loss: 0.4053, Time: 1.01s\n",
      "Epoch: 29, Batch: 541/842, Loss: 0.5366, Time: 0.91s\n",
      "Epoch: 29, Batch: 551/842, Loss: 0.5489, Time: 0.86s\n",
      "Epoch: 29, Batch: 561/842, Loss: 0.4992, Time: 0.81s\n",
      "Epoch: 29, Batch: 571/842, Loss: 0.5115, Time: 0.78s\n",
      "Epoch: 29, Batch: 581/842, Loss: 0.5586, Time: 0.84s\n",
      "Epoch: 29, Batch: 591/842, Loss: 0.4031, Time: 0.78s\n",
      "Epoch: 29, Batch: 601/842, Loss: 0.4294, Time: 0.83s\n",
      "Epoch: 29, Batch: 611/842, Loss: 0.5347, Time: 0.87s\n",
      "Epoch: 29, Batch: 621/842, Loss: 0.4861, Time: 0.82s\n",
      "Epoch: 29, Batch: 631/842, Loss: 0.6304, Time: 0.73s\n",
      "Epoch: 29, Batch: 641/842, Loss: 0.5384, Time: 0.73s\n",
      "Epoch: 29, Batch: 651/842, Loss: 0.5185, Time: 0.73s\n",
      "Epoch: 29, Batch: 661/842, Loss: 0.5325, Time: 0.73s\n",
      "Epoch: 29, Batch: 671/842, Loss: 0.3952, Time: 0.73s\n",
      "Epoch: 29, Batch: 681/842, Loss: 0.5294, Time: 0.72s\n",
      "Epoch: 29, Batch: 691/842, Loss: 0.5814, Time: 0.73s\n",
      "Epoch: 29, Batch: 701/842, Loss: 0.5595, Time: 0.72s\n",
      "Epoch: 29, Batch: 711/842, Loss: 0.4866, Time: 0.72s\n",
      "Epoch: 29, Batch: 721/842, Loss: 0.6043, Time: 0.72s\n",
      "Epoch: 29, Batch: 731/842, Loss: 0.4895, Time: 0.71s\n",
      "Epoch: 29, Batch: 741/842, Loss: 0.6237, Time: 0.72s\n",
      "Epoch: 29, Batch: 751/842, Loss: 0.5928, Time: 0.72s\n",
      "Epoch: 29, Batch: 761/842, Loss: 0.5544, Time: 0.71s\n",
      "Epoch: 29, Batch: 771/842, Loss: 0.4856, Time: 0.72s\n",
      "Epoch: 29, Batch: 781/842, Loss: 0.6515, Time: 0.73s\n",
      "Epoch: 29, Batch: 791/842, Loss: 0.5366, Time: 0.71s\n",
      "Epoch: 29, Batch: 801/842, Loss: 0.4604, Time: 0.71s\n",
      "Epoch: 29, Batch: 811/842, Loss: 0.5284, Time: 0.71s\n",
      "Epoch: 29, Batch: 821/842, Loss: 0.5353, Time: 0.71s\n",
      "Epoch: 29, Batch: 831/842, Loss: 0.5551, Time: 0.71s\n",
      "Epoch: 29, Batch: 841/842, Loss: 0.3366, Time: 0.72s\n",
      "Epoch 29/100: Train Loss: 0.5192, Val Loss: 0.4989, mIoU: 0.6328, F1: 0.7665, OA: 0.9591\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7515, F1=0.8581, OA=0.8565, Precision=0.8198, Recall=0.9001\n",
      "    Class 1: IoU=0.5772, F1=0.7319, OA=0.9951, Precision=0.8725, Recall=0.6304\n",
      "    Class 2: IoU=0.8946, F1=0.9444, OA=0.9775, Precision=0.9474, Recall=0.9413\n",
      "    Class 3: IoU=0.4122, F1=0.5838, OA=0.9610, Precision=0.8309, Recall=0.4500\n",
      "    Class 4: IoU=0.7015, F1=0.8246, OA=0.9759, Precision=0.8177, Recall=0.8315\n",
      "    Class 5: IoU=0.5945, F1=0.7457, OA=0.9959, Precision=0.8359, Recall=0.6731\n",
      "    Class 6: IoU=0.5279, F1=0.6910, OA=0.9498, Precision=0.7411, Recall=0.6473\n",
      "    Class 7: IoU=0.6027, F1=0.7521, OA=0.9612, Precision=0.7695, Recall=0.7355\n",
      "Epoch: 30, Batch: 1/842, Loss: 0.5403, Time: 1.68s\n",
      "Epoch: 30, Batch: 11/842, Loss: 0.5447, Time: 0.75s\n",
      "Epoch: 30, Batch: 21/842, Loss: 0.7056, Time: 0.75s\n",
      "Epoch: 30, Batch: 31/842, Loss: 0.4914, Time: 0.74s\n",
      "Epoch: 30, Batch: 41/842, Loss: 0.8364, Time: 0.72s\n",
      "Epoch: 30, Batch: 51/842, Loss: 0.3742, Time: 0.74s\n",
      "Epoch: 30, Batch: 61/842, Loss: 0.6679, Time: 0.72s\n",
      "Epoch: 30, Batch: 71/842, Loss: 0.4577, Time: 0.73s\n",
      "Epoch: 30, Batch: 81/842, Loss: 0.4032, Time: 0.74s\n",
      "Epoch: 30, Batch: 91/842, Loss: 0.5686, Time: 0.72s\n",
      "Epoch: 30, Batch: 101/842, Loss: 0.5417, Time: 0.73s\n",
      "Epoch: 30, Batch: 111/842, Loss: 0.4888, Time: 0.72s\n",
      "Epoch: 30, Batch: 121/842, Loss: 0.5263, Time: 0.74s\n",
      "Epoch: 30, Batch: 131/842, Loss: 0.6551, Time: 0.73s\n",
      "Epoch: 30, Batch: 141/842, Loss: 0.5972, Time: 0.83s\n",
      "Epoch: 30, Batch: 151/842, Loss: 0.5682, Time: 0.74s\n",
      "Epoch: 30, Batch: 161/842, Loss: 0.6152, Time: 0.76s\n",
      "Epoch: 30, Batch: 171/842, Loss: 0.4483, Time: 0.72s\n",
      "Epoch: 30, Batch: 181/842, Loss: 0.4260, Time: 0.71s\n",
      "Epoch: 30, Batch: 191/842, Loss: 0.4792, Time: 0.71s\n",
      "Epoch: 30, Batch: 201/842, Loss: 0.4462, Time: 0.75s\n",
      "Epoch: 30, Batch: 211/842, Loss: 0.5249, Time: 0.74s\n",
      "Epoch: 30, Batch: 221/842, Loss: 0.5034, Time: 0.78s\n",
      "Epoch: 30, Batch: 231/842, Loss: 0.4403, Time: 0.79s\n",
      "Epoch: 30, Batch: 241/842, Loss: 0.5899, Time: 0.76s\n",
      "Epoch: 30, Batch: 251/842, Loss: 0.4358, Time: 0.75s\n",
      "Epoch: 30, Batch: 261/842, Loss: 0.5331, Time: 0.75s\n",
      "Epoch: 30, Batch: 271/842, Loss: 0.4184, Time: 0.74s\n",
      "Epoch: 30, Batch: 281/842, Loss: 0.4687, Time: 0.75s\n",
      "Epoch: 30, Batch: 291/842, Loss: 0.6110, Time: 0.74s\n",
      "Epoch: 30, Batch: 301/842, Loss: 0.4772, Time: 0.75s\n",
      "Epoch: 30, Batch: 311/842, Loss: 0.3894, Time: 0.77s\n",
      "Epoch: 30, Batch: 321/842, Loss: 0.5828, Time: 0.77s\n",
      "Epoch: 30, Batch: 331/842, Loss: 0.5865, Time: 0.73s\n",
      "Epoch: 30, Batch: 341/842, Loss: 0.7108, Time: 0.73s\n",
      "Epoch: 30, Batch: 351/842, Loss: 0.4369, Time: 0.73s\n",
      "Epoch: 30, Batch: 361/842, Loss: 0.6034, Time: 0.73s\n",
      "Epoch: 30, Batch: 371/842, Loss: 0.6184, Time: 0.73s\n",
      "Epoch: 30, Batch: 381/842, Loss: 0.4334, Time: 0.73s\n",
      "Epoch: 30, Batch: 391/842, Loss: 0.5146, Time: 0.76s\n",
      "Epoch: 30, Batch: 401/842, Loss: 0.4959, Time: 0.74s\n",
      "Epoch: 30, Batch: 411/842, Loss: 0.5881, Time: 0.77s\n",
      "Epoch: 30, Batch: 421/842, Loss: 0.4951, Time: 0.72s\n",
      "Epoch: 30, Batch: 431/842, Loss: 0.6142, Time: 0.74s\n",
      "Epoch: 30, Batch: 441/842, Loss: 0.5449, Time: 0.73s\n",
      "Epoch: 30, Batch: 451/842, Loss: 0.4932, Time: 0.72s\n",
      "Epoch: 30, Batch: 461/842, Loss: 0.4985, Time: 0.73s\n",
      "Epoch: 30, Batch: 471/842, Loss: 0.4886, Time: 0.73s\n",
      "Epoch: 30, Batch: 481/842, Loss: 0.4733, Time: 0.73s\n",
      "Epoch: 30, Batch: 491/842, Loss: 0.3994, Time: 0.74s\n",
      "Epoch: 30, Batch: 501/842, Loss: 0.4819, Time: 0.76s\n",
      "Epoch: 30, Batch: 511/842, Loss: 0.5050, Time: 0.74s\n",
      "Epoch: 30, Batch: 521/842, Loss: 0.5616, Time: 0.75s\n",
      "Epoch: 30, Batch: 531/842, Loss: 0.5308, Time: 0.74s\n",
      "Epoch: 30, Batch: 541/842, Loss: 0.4304, Time: 0.72s\n",
      "Epoch: 30, Batch: 551/842, Loss: 0.6090, Time: 0.74s\n",
      "Epoch: 30, Batch: 561/842, Loss: 0.5752, Time: 0.73s\n",
      "Epoch: 30, Batch: 571/842, Loss: 0.5519, Time: 0.73s\n",
      "Epoch: 30, Batch: 581/842, Loss: 0.5245, Time: 0.74s\n",
      "Epoch: 30, Batch: 591/842, Loss: 0.4809, Time: 0.72s\n",
      "Epoch: 30, Batch: 601/842, Loss: 0.6010, Time: 0.72s\n",
      "Epoch: 30, Batch: 611/842, Loss: 0.5484, Time: 0.73s\n",
      "Epoch: 30, Batch: 621/842, Loss: 0.6049, Time: 0.73s\n",
      "Epoch: 30, Batch: 631/842, Loss: 0.5381, Time: 0.73s\n",
      "Epoch: 30, Batch: 641/842, Loss: 0.4116, Time: 0.73s\n",
      "Epoch: 30, Batch: 651/842, Loss: 0.5234, Time: 0.75s\n",
      "Epoch: 30, Batch: 661/842, Loss: 0.5425, Time: 0.73s\n",
      "Epoch: 30, Batch: 671/842, Loss: 0.5457, Time: 0.80s\n",
      "Epoch: 30, Batch: 681/842, Loss: 0.3918, Time: 0.81s\n",
      "Epoch: 30, Batch: 691/842, Loss: 0.4516, Time: 0.80s\n",
      "Epoch: 30, Batch: 701/842, Loss: 0.3468, Time: 0.80s\n",
      "Epoch: 30, Batch: 711/842, Loss: 0.4699, Time: 0.75s\n",
      "Epoch: 30, Batch: 721/842, Loss: 0.5297, Time: 0.72s\n",
      "Epoch: 30, Batch: 731/842, Loss: 0.3420, Time: 0.72s\n",
      "Epoch: 30, Batch: 741/842, Loss: 0.4379, Time: 0.72s\n",
      "Epoch: 30, Batch: 751/842, Loss: 0.5783, Time: 0.71s\n",
      "Epoch: 30, Batch: 761/842, Loss: 0.4622, Time: 0.72s\n",
      "Epoch: 30, Batch: 771/842, Loss: 0.4100, Time: 0.71s\n",
      "Epoch: 30, Batch: 781/842, Loss: 0.5276, Time: 0.74s\n",
      "Epoch: 30, Batch: 791/842, Loss: 0.3976, Time: 0.76s\n",
      "Epoch: 30, Batch: 801/842, Loss: 0.3776, Time: 0.88s\n",
      "Epoch: 30, Batch: 811/842, Loss: 0.6397, Time: 0.83s\n",
      "Epoch: 30, Batch: 821/842, Loss: 0.3754, Time: 0.82s\n",
      "Epoch: 30, Batch: 831/842, Loss: 0.5224, Time: 0.86s\n",
      "Epoch: 30, Batch: 841/842, Loss: 0.4414, Time: 0.78s\n",
      "Epoch 30/100: Train Loss: 0.5158, Val Loss: 0.4809, mIoU: 0.6518, F1: 0.7812, OA: 0.9607\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7592, F1=0.8631, OA=0.8596, Precision=0.8142, Recall=0.9182\n",
      "    Class 1: IoU=0.6706, F1=0.8028, OA=0.9962, Precision=0.8947, Recall=0.7281\n",
      "    Class 2: IoU=0.8997, F1=0.9472, OA=0.9787, Precision=0.9513, Recall=0.9432\n",
      "    Class 3: IoU=0.4366, F1=0.6079, OA=0.9604, Precision=0.7647, Recall=0.5044\n",
      "    Class 4: IoU=0.7146, F1=0.8335, OA=0.9783, Precision=0.8755, Recall=0.7954\n",
      "    Class 5: IoU=0.5924, F1=0.7441, OA=0.9954, Precision=0.7310, Recall=0.7576\n",
      "    Class 6: IoU=0.5303, F1=0.6931, OA=0.9538, Precision=0.8159, Recall=0.6024\n",
      "    Class 7: IoU=0.6106, F1=0.7582, OA=0.9634, Precision=0.8038, Recall=0.7176\n",
      "Saving best model with mIoU: 0.6518\n",
      "Epoch: 31, Batch: 1/842, Loss: 0.4118, Time: 1.50s\n",
      "Epoch: 31, Batch: 11/842, Loss: 0.5065, Time: 0.82s\n",
      "Epoch: 31, Batch: 21/842, Loss: 0.5115, Time: 0.76s\n",
      "Epoch: 31, Batch: 31/842, Loss: 0.6170, Time: 0.74s\n",
      "Epoch: 31, Batch: 41/842, Loss: 0.4817, Time: 0.74s\n",
      "Epoch: 31, Batch: 51/842, Loss: 0.4448, Time: 0.72s\n",
      "Epoch: 31, Batch: 61/842, Loss: 0.5639, Time: 0.74s\n",
      "Epoch: 31, Batch: 71/842, Loss: 0.4303, Time: 0.72s\n",
      "Epoch: 31, Batch: 81/842, Loss: 0.5630, Time: 0.72s\n",
      "Epoch: 31, Batch: 91/842, Loss: 0.4170, Time: 0.74s\n",
      "Epoch: 31, Batch: 101/842, Loss: 0.5277, Time: 0.81s\n",
      "Epoch: 31, Batch: 111/842, Loss: 0.4904, Time: 0.81s\n",
      "Epoch: 31, Batch: 121/842, Loss: 0.4095, Time: 0.80s\n",
      "Epoch: 31, Batch: 131/842, Loss: 0.5152, Time: 0.80s\n",
      "Epoch: 31, Batch: 141/842, Loss: 0.3427, Time: 0.79s\n",
      "Epoch: 31, Batch: 151/842, Loss: 0.5386, Time: 0.79s\n",
      "Epoch: 31, Batch: 161/842, Loss: 0.6519, Time: 0.78s\n",
      "Epoch: 31, Batch: 171/842, Loss: 0.5546, Time: 0.79s\n",
      "Epoch: 31, Batch: 181/842, Loss: 0.4889, Time: 0.75s\n",
      "Epoch: 31, Batch: 191/842, Loss: 0.4214, Time: 0.74s\n",
      "Epoch: 31, Batch: 201/842, Loss: 0.3918, Time: 0.73s\n",
      "Epoch: 31, Batch: 211/842, Loss: 0.5477, Time: 0.73s\n",
      "Epoch: 31, Batch: 221/842, Loss: 0.5555, Time: 0.73s\n",
      "Epoch: 31, Batch: 231/842, Loss: 0.6343, Time: 0.76s\n",
      "Epoch: 31, Batch: 241/842, Loss: 0.4296, Time: 0.74s\n",
      "Epoch: 31, Batch: 251/842, Loss: 0.5560, Time: 0.78s\n",
      "Epoch: 31, Batch: 261/842, Loss: 0.5960, Time: 0.77s\n",
      "Epoch: 31, Batch: 271/842, Loss: 0.5486, Time: 0.76s\n",
      "Epoch: 31, Batch: 281/842, Loss: 0.4348, Time: 0.73s\n",
      "Epoch: 31, Batch: 291/842, Loss: 0.5939, Time: 0.75s\n",
      "Epoch: 31, Batch: 301/842, Loss: 0.4693, Time: 0.75s\n",
      "Epoch: 31, Batch: 311/842, Loss: 0.6378, Time: 0.75s\n",
      "Epoch: 31, Batch: 321/842, Loss: 0.3400, Time: 0.75s\n",
      "Epoch: 31, Batch: 331/842, Loss: 0.4186, Time: 0.74s\n",
      "Epoch: 31, Batch: 341/842, Loss: 0.5145, Time: 0.74s\n",
      "Epoch: 31, Batch: 351/842, Loss: 0.4112, Time: 0.74s\n",
      "Epoch: 31, Batch: 361/842, Loss: 0.5595, Time: 0.73s\n",
      "Epoch: 31, Batch: 371/842, Loss: 0.4520, Time: 0.72s\n",
      "Epoch: 31, Batch: 381/842, Loss: 0.3676, Time: 0.75s\n",
      "Epoch: 31, Batch: 391/842, Loss: 0.7120, Time: 0.77s\n",
      "Epoch: 31, Batch: 401/842, Loss: 0.6349, Time: 0.75s\n",
      "Epoch: 31, Batch: 411/842, Loss: 0.4832, Time: 0.72s\n",
      "Epoch: 31, Batch: 421/842, Loss: 0.4782, Time: 0.72s\n",
      "Epoch: 31, Batch: 431/842, Loss: 0.4458, Time: 0.73s\n",
      "Epoch: 31, Batch: 441/842, Loss: 0.3464, Time: 0.72s\n",
      "Epoch: 31, Batch: 451/842, Loss: 0.5675, Time: 0.73s\n",
      "Epoch: 31, Batch: 461/842, Loss: 0.5612, Time: 0.73s\n",
      "Epoch: 31, Batch: 471/842, Loss: 0.4711, Time: 0.71s\n",
      "Epoch: 31, Batch: 481/842, Loss: 0.3668, Time: 0.71s\n",
      "Epoch: 31, Batch: 491/842, Loss: 0.5381, Time: 0.73s\n",
      "Epoch: 31, Batch: 501/842, Loss: 0.5842, Time: 1.27s\n",
      "Epoch: 31, Batch: 511/842, Loss: 0.5134, Time: 0.73s\n",
      "Epoch: 31, Batch: 521/842, Loss: 0.5867, Time: 0.74s\n",
      "Epoch: 31, Batch: 531/842, Loss: 0.4554, Time: 0.73s\n",
      "Epoch: 31, Batch: 541/842, Loss: 0.4457, Time: 0.73s\n",
      "Epoch: 31, Batch: 551/842, Loss: 0.5539, Time: 0.74s\n",
      "Epoch: 31, Batch: 561/842, Loss: 0.3802, Time: 0.73s\n",
      "Epoch: 31, Batch: 571/842, Loss: 0.4558, Time: 0.73s\n",
      "Epoch: 31, Batch: 581/842, Loss: 0.5155, Time: 0.75s\n",
      "Epoch: 31, Batch: 591/842, Loss: 0.5171, Time: 0.75s\n",
      "Epoch: 31, Batch: 601/842, Loss: 0.4791, Time: 0.75s\n",
      "Epoch: 31, Batch: 611/842, Loss: 0.4385, Time: 0.73s\n",
      "Epoch: 31, Batch: 621/842, Loss: 0.5361, Time: 0.73s\n",
      "Epoch: 31, Batch: 631/842, Loss: 0.5180, Time: 0.73s\n",
      "Epoch: 31, Batch: 641/842, Loss: 0.6925, Time: 0.72s\n",
      "Epoch: 31, Batch: 651/842, Loss: 0.5216, Time: 0.73s\n",
      "Epoch: 31, Batch: 661/842, Loss: 0.4946, Time: 0.72s\n",
      "Epoch: 31, Batch: 671/842, Loss: 0.5483, Time: 0.74s\n",
      "Epoch: 31, Batch: 681/842, Loss: 0.6712, Time: 0.78s\n",
      "Epoch: 31, Batch: 691/842, Loss: 0.4935, Time: 0.76s\n",
      "Epoch: 31, Batch: 701/842, Loss: 0.4031, Time: 0.72s\n",
      "Epoch: 31, Batch: 711/842, Loss: 0.5720, Time: 0.72s\n",
      "Epoch: 31, Batch: 721/842, Loss: 0.5043, Time: 0.72s\n",
      "Epoch: 31, Batch: 731/842, Loss: 0.4597, Time: 0.72s\n",
      "Epoch: 31, Batch: 741/842, Loss: 0.3249, Time: 0.72s\n",
      "Epoch: 31, Batch: 751/842, Loss: 0.4541, Time: 0.72s\n",
      "Epoch: 31, Batch: 761/842, Loss: 0.5621, Time: 0.72s\n",
      "Epoch: 31, Batch: 771/842, Loss: 0.5805, Time: 0.72s\n",
      "Epoch: 31, Batch: 781/842, Loss: 0.5486, Time: 0.72s\n",
      "Epoch: 31, Batch: 791/842, Loss: 0.6474, Time: 0.72s\n",
      "Epoch: 31, Batch: 801/842, Loss: 0.4777, Time: 0.71s\n",
      "Epoch: 31, Batch: 811/842, Loss: 0.6383, Time: 0.72s\n",
      "Epoch: 31, Batch: 821/842, Loss: 0.6523, Time: 0.72s\n",
      "Epoch: 31, Batch: 831/842, Loss: 0.4676, Time: 0.72s\n",
      "Epoch: 31, Batch: 841/842, Loss: 0.4682, Time: 0.70s\n",
      "Epoch 31/100: Train Loss: 0.5190, Val Loss: 0.4788, mIoU: 0.6476, F1: 0.7777, OA: 0.9611\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7611, F1=0.8643, OA=0.8613, Precision=0.8180, Recall=0.9162\n",
      "    Class 1: IoU=0.6690, F1=0.8017, OA=0.9960, Precision=0.8444, Recall=0.7631\n",
      "    Class 2: IoU=0.9024, F1=0.9487, OA=0.9793, Precision=0.9529, Recall=0.9445\n",
      "    Class 3: IoU=0.4373, F1=0.6085, OA=0.9605, Precision=0.7645, Recall=0.5054\n",
      "    Class 4: IoU=0.7157, F1=0.8343, OA=0.9774, Precision=0.8338, Recall=0.8348\n",
      "    Class 5: IoU=0.5438, F1=0.7045, OA=0.9943, Precision=0.6530, Recall=0.7648\n",
      "    Class 6: IoU=0.5286, F1=0.6917, OA=0.9547, Precision=0.8449, Recall=0.5855\n",
      "    Class 7: IoU=0.6230, F1=0.7677, OA=0.9650, Precision=0.8191, Recall=0.7225\n",
      "Epoch: 32, Batch: 1/842, Loss: 0.5239, Time: 1.67s\n",
      "Epoch: 32, Batch: 11/842, Loss: 0.5270, Time: 0.84s\n",
      "Epoch: 32, Batch: 21/842, Loss: 0.5507, Time: 0.82s\n",
      "Epoch: 32, Batch: 31/842, Loss: 0.4914, Time: 0.82s\n",
      "Epoch: 32, Batch: 41/842, Loss: 0.3778, Time: 0.83s\n",
      "Epoch: 32, Batch: 51/842, Loss: 0.5560, Time: 0.85s\n",
      "Epoch: 32, Batch: 61/842, Loss: 0.5267, Time: 0.85s\n",
      "Epoch: 32, Batch: 71/842, Loss: 0.4803, Time: 0.87s\n",
      "Epoch: 32, Batch: 81/842, Loss: 0.3960, Time: 0.85s\n",
      "Epoch: 32, Batch: 91/842, Loss: 0.5169, Time: 0.80s\n",
      "Epoch: 32, Batch: 101/842, Loss: 0.6046, Time: 0.80s\n",
      "Epoch: 32, Batch: 111/842, Loss: 0.4513, Time: 0.81s\n",
      "Epoch: 32, Batch: 121/842, Loss: 0.4716, Time: 0.79s\n",
      "Epoch: 32, Batch: 131/842, Loss: 0.5837, Time: 0.78s\n",
      "Epoch: 32, Batch: 141/842, Loss: 0.4930, Time: 0.80s\n",
      "Epoch: 32, Batch: 151/842, Loss: 0.4979, Time: 0.79s\n",
      "Epoch: 32, Batch: 161/842, Loss: 0.4151, Time: 0.78s\n",
      "Epoch: 32, Batch: 171/842, Loss: 0.5701, Time: 0.79s\n",
      "Epoch: 32, Batch: 181/842, Loss: 0.5734, Time: 0.78s\n",
      "Epoch: 32, Batch: 191/842, Loss: 0.4712, Time: 0.78s\n",
      "Epoch: 32, Batch: 201/842, Loss: 0.5013, Time: 0.78s\n",
      "Epoch: 32, Batch: 211/842, Loss: 0.6228, Time: 0.78s\n",
      "Epoch: 32, Batch: 221/842, Loss: 0.6091, Time: 0.79s\n",
      "Epoch: 32, Batch: 231/842, Loss: 0.3959, Time: 0.79s\n",
      "Epoch: 32, Batch: 241/842, Loss: 0.4647, Time: 0.76s\n",
      "Epoch: 32, Batch: 251/842, Loss: 0.5843, Time: 0.81s\n",
      "Epoch: 32, Batch: 261/842, Loss: 0.5194, Time: 0.82s\n",
      "Epoch: 32, Batch: 271/842, Loss: 0.4424, Time: 0.80s\n",
      "Epoch: 32, Batch: 281/842, Loss: 0.4362, Time: 0.79s\n",
      "Epoch: 32, Batch: 291/842, Loss: 0.5175, Time: 0.81s\n",
      "Epoch: 32, Batch: 301/842, Loss: 0.4369, Time: 0.81s\n",
      "Epoch: 32, Batch: 311/842, Loss: 0.5241, Time: 0.80s\n",
      "Epoch: 32, Batch: 321/842, Loss: 0.5050, Time: 0.77s\n",
      "Epoch: 32, Batch: 331/842, Loss: 0.5709, Time: 0.76s\n",
      "Epoch: 32, Batch: 341/842, Loss: 0.4190, Time: 0.78s\n",
      "Epoch: 32, Batch: 351/842, Loss: 0.4815, Time: 0.86s\n",
      "Epoch: 32, Batch: 361/842, Loss: 0.4388, Time: 0.84s\n",
      "Epoch: 32, Batch: 371/842, Loss: 0.5199, Time: 0.82s\n",
      "Epoch: 32, Batch: 381/842, Loss: 0.4843, Time: 0.83s\n",
      "Epoch: 32, Batch: 391/842, Loss: 0.5998, Time: 0.93s\n",
      "Epoch: 32, Batch: 401/842, Loss: 0.4385, Time: 0.92s\n",
      "Epoch: 32, Batch: 411/842, Loss: 0.3639, Time: 0.89s\n",
      "Epoch: 32, Batch: 421/842, Loss: 0.5959, Time: 0.88s\n",
      "Epoch: 32, Batch: 431/842, Loss: 0.4297, Time: 0.85s\n",
      "Epoch: 32, Batch: 441/842, Loss: 0.4660, Time: 0.90s\n",
      "Epoch: 32, Batch: 451/842, Loss: 0.5207, Time: 0.83s\n",
      "Epoch: 32, Batch: 461/842, Loss: 0.4173, Time: 0.80s\n",
      "Epoch: 32, Batch: 471/842, Loss: 0.4981, Time: 0.79s\n",
      "Epoch: 32, Batch: 481/842, Loss: 0.7324, Time: 0.79s\n",
      "Epoch: 32, Batch: 491/842, Loss: 0.4570, Time: 0.79s\n",
      "Epoch: 32, Batch: 501/842, Loss: 0.5837, Time: 0.81s\n",
      "Epoch: 32, Batch: 511/842, Loss: 0.4393, Time: 0.79s\n",
      "Epoch: 32, Batch: 521/842, Loss: 0.4903, Time: 0.74s\n",
      "Epoch: 32, Batch: 531/842, Loss: 0.5399, Time: 0.74s\n",
      "Epoch: 32, Batch: 541/842, Loss: 0.4882, Time: 0.72s\n",
      "Epoch: 32, Batch: 551/842, Loss: 0.4551, Time: 0.72s\n",
      "Epoch: 32, Batch: 561/842, Loss: 0.4549, Time: 0.72s\n",
      "Epoch: 32, Batch: 571/842, Loss: 0.6983, Time: 0.78s\n",
      "Epoch: 32, Batch: 581/842, Loss: 0.6645, Time: 0.75s\n",
      "Epoch: 32, Batch: 591/842, Loss: 0.4679, Time: 0.75s\n",
      "Epoch: 32, Batch: 601/842, Loss: 0.5025, Time: 0.74s\n",
      "Epoch: 32, Batch: 611/842, Loss: 0.4024, Time: 0.73s\n",
      "Epoch: 32, Batch: 621/842, Loss: 0.5675, Time: 0.74s\n",
      "Epoch: 32, Batch: 631/842, Loss: 0.5607, Time: 0.74s\n",
      "Epoch: 32, Batch: 641/842, Loss: 0.4380, Time: 0.75s\n",
      "Epoch: 32, Batch: 651/842, Loss: 0.4388, Time: 0.73s\n",
      "Epoch: 32, Batch: 661/842, Loss: 0.5018, Time: 0.73s\n",
      "Epoch: 32, Batch: 671/842, Loss: 0.6474, Time: 0.74s\n",
      "Epoch: 32, Batch: 681/842, Loss: 0.5942, Time: 0.74s\n",
      "Epoch: 32, Batch: 691/842, Loss: 0.4919, Time: 0.74s\n",
      "Epoch: 32, Batch: 701/842, Loss: 0.5920, Time: 0.74s\n",
      "Epoch: 32, Batch: 711/842, Loss: 0.4698, Time: 0.74s\n",
      "Epoch: 32, Batch: 721/842, Loss: 0.4326, Time: 0.74s\n",
      "Epoch: 32, Batch: 731/842, Loss: 0.4082, Time: 0.73s\n",
      "Epoch: 32, Batch: 741/842, Loss: 0.4340, Time: 0.74s\n",
      "Epoch: 32, Batch: 751/842, Loss: 0.4229, Time: 0.72s\n",
      "Epoch: 32, Batch: 761/842, Loss: 0.4537, Time: 0.74s\n",
      "Epoch: 32, Batch: 771/842, Loss: 0.5208, Time: 0.74s\n",
      "Epoch: 32, Batch: 781/842, Loss: 0.5537, Time: 0.73s\n",
      "Epoch: 32, Batch: 791/842, Loss: 0.5459, Time: 0.73s\n",
      "Epoch: 32, Batch: 801/842, Loss: 0.4898, Time: 0.74s\n",
      "Epoch: 32, Batch: 811/842, Loss: 0.4260, Time: 0.71s\n",
      "Epoch: 32, Batch: 821/842, Loss: 0.5329, Time: 0.72s\n",
      "Epoch: 32, Batch: 831/842, Loss: 0.5194, Time: 0.77s\n",
      "Epoch: 32, Batch: 841/842, Loss: 0.5748, Time: 0.75s\n",
      "Epoch 32/100: Train Loss: 0.5130, Val Loss: 0.4748, mIoU: 0.6528, F1: 0.7820, OA: 0.9610\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7615, F1=0.8646, OA=0.8613, Precision=0.8166, Recall=0.9186\n",
      "    Class 1: IoU=0.6499, F1=0.7878, OA=0.9958, Precision=0.8574, Recall=0.7287\n",
      "    Class 2: IoU=0.9062, F1=0.9508, OA=0.9801, Precision=0.9531, Recall=0.9485\n",
      "    Class 3: IoU=0.4340, F1=0.6053, OA=0.9616, Precision=0.8078, Recall=0.4840\n",
      "    Class 4: IoU=0.7133, F1=0.8326, OA=0.9776, Precision=0.8485, Recall=0.8173\n",
      "    Class 5: IoU=0.6240, F1=0.7685, OA=0.9960, Precision=0.8035, Recall=0.7364\n",
      "    Class 6: IoU=0.5363, F1=0.6981, OA=0.9513, Precision=0.7548, Recall=0.6494\n",
      "    Class 7: IoU=0.5975, F1=0.7480, OA=0.9642, Precision=0.8561, Recall=0.6642\n",
      "Saving best model with mIoU: 0.6528\n",
      "Epoch: 33, Batch: 1/842, Loss: 0.5446, Time: 1.95s\n",
      "Epoch: 33, Batch: 11/842, Loss: 0.5623, Time: 0.86s\n",
      "Epoch: 33, Batch: 21/842, Loss: 0.6396, Time: 0.88s\n",
      "Epoch: 33, Batch: 31/842, Loss: 0.6480, Time: 0.89s\n",
      "Epoch: 33, Batch: 41/842, Loss: 0.4787, Time: 0.89s\n",
      "Epoch: 33, Batch: 51/842, Loss: 0.6352, Time: 0.90s\n",
      "Epoch: 33, Batch: 61/842, Loss: 0.2880, Time: 0.90s\n",
      "Epoch: 33, Batch: 71/842, Loss: 0.4958, Time: 0.83s\n",
      "Epoch: 33, Batch: 81/842, Loss: 0.5380, Time: 0.78s\n",
      "Epoch: 33, Batch: 91/842, Loss: 0.4772, Time: 0.74s\n",
      "Epoch: 33, Batch: 101/842, Loss: 0.6109, Time: 0.78s\n",
      "Epoch: 33, Batch: 111/842, Loss: 0.6919, Time: 0.77s\n",
      "Epoch: 33, Batch: 121/842, Loss: 0.4306, Time: 0.79s\n",
      "Epoch: 33, Batch: 131/842, Loss: 0.4795, Time: 0.77s\n",
      "Epoch: 33, Batch: 141/842, Loss: 0.5486, Time: 0.81s\n",
      "Epoch: 33, Batch: 151/842, Loss: 0.5086, Time: 0.75s\n",
      "Epoch: 33, Batch: 161/842, Loss: 0.5426, Time: 0.78s\n",
      "Epoch: 33, Batch: 171/842, Loss: 0.4158, Time: 0.74s\n",
      "Epoch: 33, Batch: 181/842, Loss: 0.4808, Time: 0.77s\n",
      "Epoch: 33, Batch: 191/842, Loss: 0.4743, Time: 0.75s\n",
      "Epoch: 33, Batch: 201/842, Loss: 0.5563, Time: 0.74s\n",
      "Epoch: 33, Batch: 211/842, Loss: 0.7275, Time: 0.74s\n",
      "Epoch: 33, Batch: 221/842, Loss: 0.6357, Time: 0.77s\n",
      "Epoch: 33, Batch: 231/842, Loss: 0.7179, Time: 0.69s\n",
      "Epoch: 33, Batch: 241/842, Loss: 0.4497, Time: 0.68s\n",
      "Epoch: 33, Batch: 251/842, Loss: 0.5646, Time: 0.68s\n",
      "Epoch: 33, Batch: 261/842, Loss: 0.4081, Time: 0.68s\n",
      "Epoch: 33, Batch: 271/842, Loss: 0.5065, Time: 0.68s\n",
      "Epoch: 33, Batch: 281/842, Loss: 0.4941, Time: 0.68s\n",
      "Epoch: 33, Batch: 291/842, Loss: 0.5227, Time: 0.69s\n",
      "Epoch: 33, Batch: 301/842, Loss: 0.4308, Time: 0.68s\n",
      "Epoch: 33, Batch: 311/842, Loss: 0.5737, Time: 0.69s\n",
      "Epoch: 33, Batch: 321/842, Loss: 0.5584, Time: 0.70s\n",
      "Epoch: 33, Batch: 331/842, Loss: 0.4533, Time: 0.72s\n",
      "Epoch: 33, Batch: 341/842, Loss: 0.4428, Time: 0.81s\n",
      "Epoch: 33, Batch: 351/842, Loss: 0.5849, Time: 0.74s\n",
      "Epoch: 33, Batch: 361/842, Loss: 0.4858, Time: 0.71s\n",
      "Epoch: 33, Batch: 371/842, Loss: 0.4908, Time: 0.71s\n",
      "Epoch: 33, Batch: 381/842, Loss: 0.6406, Time: 0.76s\n",
      "Epoch: 33, Batch: 391/842, Loss: 0.5566, Time: 0.73s\n",
      "Epoch: 33, Batch: 401/842, Loss: 0.5119, Time: 0.74s\n",
      "Epoch: 33, Batch: 411/842, Loss: 0.3993, Time: 0.74s\n",
      "Epoch: 33, Batch: 421/842, Loss: 0.3994, Time: 0.77s\n",
      "Epoch: 33, Batch: 431/842, Loss: 0.4605, Time: 0.74s\n",
      "Epoch: 33, Batch: 441/842, Loss: 0.4650, Time: 0.75s\n",
      "Epoch: 33, Batch: 451/842, Loss: 0.4220, Time: 0.74s\n",
      "Epoch: 33, Batch: 461/842, Loss: 0.4146, Time: 0.76s\n",
      "Epoch: 33, Batch: 471/842, Loss: 0.3656, Time: 0.76s\n",
      "Epoch: 33, Batch: 481/842, Loss: 0.6111, Time: 0.77s\n",
      "Epoch: 33, Batch: 491/842, Loss: 0.5226, Time: 0.80s\n",
      "Epoch: 33, Batch: 501/842, Loss: 0.5359, Time: 0.80s\n",
      "Epoch: 33, Batch: 511/842, Loss: 0.4682, Time: 0.78s\n",
      "Epoch: 33, Batch: 521/842, Loss: 0.5967, Time: 0.80s\n",
      "Epoch: 33, Batch: 531/842, Loss: 0.4734, Time: 0.73s\n",
      "Epoch: 33, Batch: 541/842, Loss: 0.4894, Time: 0.74s\n",
      "Epoch: 33, Batch: 551/842, Loss: 0.5712, Time: 0.76s\n",
      "Epoch: 33, Batch: 561/842, Loss: 0.5615, Time: 0.72s\n",
      "Epoch: 33, Batch: 571/842, Loss: 0.5245, Time: 0.77s\n",
      "Epoch: 33, Batch: 581/842, Loss: 0.4713, Time: 0.73s\n",
      "Epoch: 33, Batch: 591/842, Loss: 0.6441, Time: 0.76s\n",
      "Epoch: 33, Batch: 601/842, Loss: 0.6339, Time: 0.74s\n",
      "Epoch: 33, Batch: 611/842, Loss: 0.5301, Time: 0.72s\n",
      "Epoch: 33, Batch: 621/842, Loss: 0.6055, Time: 0.75s\n",
      "Epoch: 33, Batch: 631/842, Loss: 0.5545, Time: 0.72s\n",
      "Epoch: 33, Batch: 641/842, Loss: 0.4529, Time: 0.75s\n",
      "Epoch: 33, Batch: 651/842, Loss: 0.6112, Time: 0.75s\n",
      "Epoch: 33, Batch: 661/842, Loss: 0.3323, Time: 0.73s\n",
      "Epoch: 33, Batch: 671/842, Loss: 0.4638, Time: 0.72s\n",
      "Epoch: 33, Batch: 681/842, Loss: 0.4649, Time: 0.69s\n",
      "Epoch: 33, Batch: 691/842, Loss: 0.5626, Time: 0.71s\n",
      "Epoch: 33, Batch: 701/842, Loss: 0.5721, Time: 0.71s\n",
      "Epoch: 33, Batch: 711/842, Loss: 0.5602, Time: 0.70s\n",
      "Epoch: 33, Batch: 721/842, Loss: 0.4701, Time: 0.69s\n",
      "Epoch: 33, Batch: 731/842, Loss: 0.5551, Time: 0.69s\n",
      "Epoch: 33, Batch: 741/842, Loss: 0.5819, Time: 0.70s\n",
      "Epoch: 33, Batch: 751/842, Loss: 0.4670, Time: 0.68s\n",
      "Epoch: 33, Batch: 761/842, Loss: 0.6039, Time: 0.69s\n",
      "Epoch: 33, Batch: 771/842, Loss: 0.4218, Time: 0.69s\n",
      "Epoch: 33, Batch: 781/842, Loss: 0.3800, Time: 0.69s\n",
      "Epoch: 33, Batch: 791/842, Loss: 0.4434, Time: 0.69s\n",
      "Epoch: 33, Batch: 801/842, Loss: 0.4953, Time: 0.69s\n",
      "Epoch: 33, Batch: 811/842, Loss: 0.5578, Time: 0.68s\n",
      "Epoch: 33, Batch: 821/842, Loss: 0.5169, Time: 0.70s\n",
      "Epoch: 33, Batch: 831/842, Loss: 0.6343, Time: 0.68s\n",
      "Epoch: 33, Batch: 841/842, Loss: 0.4974, Time: 0.67s\n",
      "Epoch 33/100: Train Loss: 0.5099, Val Loss: 0.5847, mIoU: 0.5602, F1: 0.7042, OA: 0.9516\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7448, F1=0.8538, OA=0.8487, Precision=0.7995, Recall=0.9158\n",
      "    Class 1: IoU=0.5262, F1=0.6895, OA=0.9946, Precision=0.8864, Recall=0.5642\n",
      "    Class 2: IoU=0.8579, F1=0.9235, OA=0.9702, Precision=0.9614, Recall=0.8885\n",
      "    Class 3: IoU=0.4084, F1=0.5799, OA=0.9593, Precision=0.7787, Recall=0.4620\n",
      "    Class 4: IoU=0.6841, F1=0.8124, OA=0.9763, Precision=0.8841, Recall=0.7515\n",
      "    Class 5: IoU=0.4229, F1=0.5944, OA=0.9902, Precision=0.4711, Recall=0.8052\n",
      "    Class 6: IoU=0.4319, F1=0.6032, OA=0.9232, Precision=0.5463, Recall=0.6734\n",
      "    Class 7: IoU=0.4053, F1=0.5768, OA=0.9502, Precision=0.9027, Recall=0.4238\n",
      "Epoch: 34, Batch: 1/842, Loss: 0.5338, Time: 2.25s\n",
      "Epoch: 34, Batch: 11/842, Loss: 0.5827, Time: 0.74s\n",
      "Epoch: 34, Batch: 21/842, Loss: 0.6220, Time: 0.69s\n",
      "Epoch: 34, Batch: 31/842, Loss: 0.5548, Time: 0.69s\n",
      "Epoch: 34, Batch: 41/842, Loss: 0.4416, Time: 0.69s\n",
      "Epoch: 34, Batch: 51/842, Loss: 0.4735, Time: 0.68s\n",
      "Epoch: 34, Batch: 61/842, Loss: 0.5721, Time: 0.69s\n",
      "Epoch: 34, Batch: 71/842, Loss: 0.6162, Time: 0.70s\n",
      "Epoch: 34, Batch: 81/842, Loss: 0.7249, Time: 0.71s\n",
      "Epoch: 34, Batch: 91/842, Loss: 0.5195, Time: 0.71s\n",
      "Epoch: 34, Batch: 101/842, Loss: 0.4818, Time: 0.71s\n",
      "Epoch: 34, Batch: 111/842, Loss: 0.5728, Time: 0.71s\n",
      "Epoch: 34, Batch: 121/842, Loss: 0.5712, Time: 0.71s\n",
      "Epoch: 34, Batch: 131/842, Loss: 0.3969, Time: 0.69s\n",
      "Epoch: 34, Batch: 141/842, Loss: 0.5400, Time: 0.71s\n",
      "Epoch: 34, Batch: 151/842, Loss: 0.6681, Time: 0.68s\n",
      "Epoch: 34, Batch: 161/842, Loss: 0.5026, Time: 0.68s\n",
      "Epoch: 34, Batch: 171/842, Loss: 0.6191, Time: 0.68s\n",
      "Epoch: 34, Batch: 181/842, Loss: 0.5395, Time: 0.68s\n",
      "Epoch: 34, Batch: 191/842, Loss: 0.4872, Time: 0.68s\n",
      "Epoch: 34, Batch: 201/842, Loss: 0.4214, Time: 0.68s\n",
      "Epoch: 34, Batch: 211/842, Loss: 0.5543, Time: 0.69s\n",
      "Epoch: 34, Batch: 221/842, Loss: 0.3981, Time: 0.69s\n",
      "Epoch: 34, Batch: 231/842, Loss: 0.5748, Time: 0.68s\n",
      "Epoch: 34, Batch: 241/842, Loss: 0.7028, Time: 0.68s\n",
      "Epoch: 34, Batch: 251/842, Loss: 0.5036, Time: 0.69s\n",
      "Epoch: 34, Batch: 261/842, Loss: 0.5730, Time: 0.68s\n",
      "Epoch: 34, Batch: 271/842, Loss: 0.5913, Time: 0.68s\n",
      "Epoch: 34, Batch: 281/842, Loss: 0.5252, Time: 0.67s\n",
      "Epoch: 34, Batch: 291/842, Loss: 0.4182, Time: 0.69s\n",
      "Epoch: 34, Batch: 301/842, Loss: 0.5798, Time: 0.67s\n",
      "Epoch: 34, Batch: 311/842, Loss: 0.3735, Time: 0.69s\n",
      "Epoch: 34, Batch: 321/842, Loss: 0.3908, Time: 0.69s\n",
      "Epoch: 34, Batch: 331/842, Loss: 0.4738, Time: 0.69s\n",
      "Epoch: 34, Batch: 341/842, Loss: 0.4405, Time: 0.69s\n",
      "Epoch: 34, Batch: 351/842, Loss: 0.5498, Time: 0.68s\n",
      "Epoch: 34, Batch: 361/842, Loss: 0.5796, Time: 0.68s\n",
      "Epoch: 34, Batch: 371/842, Loss: 0.4145, Time: 0.69s\n",
      "Epoch: 34, Batch: 381/842, Loss: 0.5207, Time: 0.68s\n",
      "Epoch: 34, Batch: 391/842, Loss: 0.5188, Time: 0.68s\n",
      "Epoch: 34, Batch: 401/842, Loss: 0.5352, Time: 0.68s\n",
      "Epoch: 34, Batch: 411/842, Loss: 0.6200, Time: 0.68s\n",
      "Epoch: 34, Batch: 421/842, Loss: 0.4243, Time: 0.68s\n",
      "Epoch: 34, Batch: 431/842, Loss: 0.6556, Time: 0.68s\n",
      "Epoch: 34, Batch: 441/842, Loss: 0.5508, Time: 0.68s\n",
      "Epoch: 34, Batch: 451/842, Loss: 0.3820, Time: 0.72s\n",
      "Epoch: 34, Batch: 461/842, Loss: 0.3910, Time: 0.74s\n",
      "Epoch: 34, Batch: 471/842, Loss: 0.4562, Time: 0.75s\n",
      "Epoch: 34, Batch: 481/842, Loss: 0.5638, Time: 0.79s\n",
      "Epoch: 34, Batch: 491/842, Loss: 0.4944, Time: 0.75s\n",
      "Epoch: 34, Batch: 501/842, Loss: 0.3861, Time: 0.74s\n",
      "Epoch: 34, Batch: 511/842, Loss: 0.4440, Time: 0.74s\n",
      "Epoch: 34, Batch: 521/842, Loss: 0.4978, Time: 0.75s\n",
      "Epoch: 34, Batch: 531/842, Loss: 0.4213, Time: 0.75s\n",
      "Epoch: 34, Batch: 541/842, Loss: 0.5346, Time: 0.75s\n",
      "Epoch: 34, Batch: 551/842, Loss: 0.4104, Time: 0.74s\n",
      "Epoch: 34, Batch: 561/842, Loss: 0.5296, Time: 0.74s\n",
      "Epoch: 34, Batch: 571/842, Loss: 0.4904, Time: 0.74s\n",
      "Epoch: 34, Batch: 581/842, Loss: 0.5506, Time: 0.75s\n",
      "Epoch: 34, Batch: 591/842, Loss: 0.5952, Time: 0.75s\n",
      "Epoch: 34, Batch: 601/842, Loss: 0.4754, Time: 0.76s\n",
      "Epoch: 34, Batch: 611/842, Loss: 0.5571, Time: 0.75s\n",
      "Epoch: 34, Batch: 621/842, Loss: 0.4230, Time: 0.75s\n",
      "Epoch: 34, Batch: 631/842, Loss: 0.4140, Time: 0.76s\n",
      "Epoch: 34, Batch: 641/842, Loss: 0.4389, Time: 0.74s\n",
      "Epoch: 34, Batch: 651/842, Loss: 0.5577, Time: 0.75s\n",
      "Epoch: 34, Batch: 661/842, Loss: 0.4664, Time: 0.74s\n",
      "Epoch: 34, Batch: 671/842, Loss: 0.4864, Time: 0.76s\n",
      "Epoch: 34, Batch: 681/842, Loss: 0.5626, Time: 0.74s\n",
      "Epoch: 34, Batch: 691/842, Loss: 0.4381, Time: 0.75s\n",
      "Epoch: 34, Batch: 701/842, Loss: 0.5808, Time: 0.73s\n",
      "Epoch: 34, Batch: 711/842, Loss: 0.6204, Time: 0.74s\n",
      "Epoch: 34, Batch: 721/842, Loss: 0.4058, Time: 0.76s\n",
      "Epoch: 34, Batch: 731/842, Loss: 0.4862, Time: 0.76s\n",
      "Epoch: 34, Batch: 741/842, Loss: 0.6016, Time: 0.75s\n",
      "Epoch: 34, Batch: 751/842, Loss: 0.4712, Time: 0.76s\n",
      "Epoch: 34, Batch: 761/842, Loss: 0.6448, Time: 0.75s\n",
      "Epoch: 34, Batch: 771/842, Loss: 0.4951, Time: 0.75s\n",
      "Epoch: 34, Batch: 781/842, Loss: 0.4803, Time: 0.78s\n",
      "Epoch: 34, Batch: 791/842, Loss: 0.6113, Time: 0.76s\n",
      "Epoch: 34, Batch: 801/842, Loss: 0.5469, Time: 0.74s\n",
      "Epoch: 34, Batch: 811/842, Loss: 0.5864, Time: 0.74s\n",
      "Epoch: 34, Batch: 821/842, Loss: 0.5062, Time: 0.75s\n",
      "Epoch: 34, Batch: 831/842, Loss: 0.4056, Time: 0.76s\n",
      "Epoch: 34, Batch: 841/842, Loss: 0.4563, Time: 0.73s\n",
      "Epoch 34/100: Train Loss: 0.5102, Val Loss: 0.4685, mIoU: 0.6642, F1: 0.7906, OA: 0.9620\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7647, F1=0.8666, OA=0.8629, Precision=0.8161, Recall=0.9239\n",
      "    Class 1: IoU=0.6922, F1=0.8181, OA=0.9963, Precision=0.8520, Recall=0.7868\n",
      "    Class 2: IoU=0.9074, F1=0.9515, OA=0.9804, Precision=0.9566, Recall=0.9464\n",
      "    Class 3: IoU=0.4450, F1=0.6159, OA=0.9601, Precision=0.7413, Recall=0.5268\n",
      "    Class 4: IoU=0.7279, F1=0.8426, OA=0.9790, Precision=0.8625, Recall=0.8235\n",
      "    Class 5: IoU=0.6208, F1=0.7660, OA=0.9957, Precision=0.7516, Recall=0.7810\n",
      "    Class 6: IoU=0.5465, F1=0.7067, OA=0.9562, Precision=0.8427, Recall=0.6086\n",
      "    Class 7: IoU=0.6092, F1=0.7572, OA=0.9651, Precision=0.8539, Recall=0.6801\n",
      "Saving best model with mIoU: 0.6642\n",
      "Epoch: 35, Batch: 1/842, Loss: 0.5543, Time: 2.23s\n",
      "Epoch: 35, Batch: 11/842, Loss: 0.4884, Time: 0.75s\n",
      "Epoch: 35, Batch: 21/842, Loss: 0.4260, Time: 0.75s\n",
      "Epoch: 35, Batch: 31/842, Loss: 0.6205, Time: 0.74s\n",
      "Epoch: 35, Batch: 41/842, Loss: 0.5387, Time: 0.73s\n",
      "Epoch: 35, Batch: 51/842, Loss: 0.5268, Time: 0.74s\n",
      "Epoch: 35, Batch: 61/842, Loss: 0.5354, Time: 0.79s\n",
      "Epoch: 35, Batch: 71/842, Loss: 0.5576, Time: 0.73s\n",
      "Epoch: 35, Batch: 81/842, Loss: 0.5332, Time: 0.72s\n",
      "Epoch: 35, Batch: 91/842, Loss: 0.4867, Time: 0.72s\n",
      "Epoch: 35, Batch: 101/842, Loss: 0.4732, Time: 1.57s\n",
      "Epoch: 35, Batch: 111/842, Loss: 0.5841, Time: 0.72s\n",
      "Epoch: 35, Batch: 121/842, Loss: 0.5784, Time: 0.72s\n",
      "Epoch: 35, Batch: 131/842, Loss: 0.5555, Time: 0.73s\n",
      "Epoch: 35, Batch: 141/842, Loss: 0.5470, Time: 0.74s\n",
      "Epoch: 35, Batch: 151/842, Loss: 0.6123, Time: 0.72s\n",
      "Epoch: 35, Batch: 161/842, Loss: 0.4387, Time: 0.73s\n",
      "Epoch: 35, Batch: 171/842, Loss: 0.4864, Time: 0.73s\n",
      "Epoch: 35, Batch: 181/842, Loss: 0.4610, Time: 0.74s\n",
      "Epoch: 35, Batch: 191/842, Loss: 0.4799, Time: 0.72s\n",
      "Epoch: 35, Batch: 201/842, Loss: 0.6814, Time: 0.73s\n",
      "Epoch: 35, Batch: 211/842, Loss: 0.4143, Time: 0.73s\n",
      "Epoch: 35, Batch: 221/842, Loss: 0.3955, Time: 0.71s\n",
      "Epoch: 35, Batch: 231/842, Loss: 0.6363, Time: 0.73s\n",
      "Epoch: 35, Batch: 241/842, Loss: 0.5314, Time: 0.73s\n",
      "Epoch: 35, Batch: 251/842, Loss: 0.4138, Time: 0.73s\n",
      "Epoch: 35, Batch: 261/842, Loss: 0.4424, Time: 0.73s\n",
      "Epoch: 35, Batch: 271/842, Loss: 0.5284, Time: 0.73s\n",
      "Epoch: 35, Batch: 281/842, Loss: 0.4266, Time: 0.72s\n",
      "Epoch: 35, Batch: 291/842, Loss: 0.5106, Time: 0.73s\n",
      "Epoch: 35, Batch: 301/842, Loss: 0.6139, Time: 0.71s\n",
      "Epoch: 35, Batch: 311/842, Loss: 0.5387, Time: 0.75s\n",
      "Epoch: 35, Batch: 321/842, Loss: 0.5332, Time: 0.72s\n",
      "Epoch: 35, Batch: 331/842, Loss: 0.3731, Time: 0.73s\n",
      "Epoch: 35, Batch: 341/842, Loss: 0.3971, Time: 0.75s\n",
      "Epoch: 35, Batch: 351/842, Loss: 0.3420, Time: 0.72s\n",
      "Epoch: 35, Batch: 361/842, Loss: 0.6082, Time: 0.71s\n",
      "Epoch: 35, Batch: 371/842, Loss: 0.3495, Time: 0.75s\n",
      "Epoch: 35, Batch: 381/842, Loss: 0.5764, Time: 0.72s\n",
      "Epoch: 35, Batch: 391/842, Loss: 0.5193, Time: 0.73s\n",
      "Epoch: 35, Batch: 401/842, Loss: 0.4899, Time: 0.71s\n",
      "Epoch: 35, Batch: 411/842, Loss: 0.6146, Time: 0.74s\n",
      "Epoch: 35, Batch: 421/842, Loss: 0.4832, Time: 0.72s\n",
      "Epoch: 35, Batch: 431/842, Loss: 0.4003, Time: 0.72s\n",
      "Epoch: 35, Batch: 441/842, Loss: 0.4362, Time: 0.73s\n",
      "Epoch: 35, Batch: 451/842, Loss: 0.5271, Time: 0.73s\n",
      "Epoch: 35, Batch: 461/842, Loss: 0.4941, Time: 0.73s\n",
      "Epoch: 35, Batch: 471/842, Loss: 0.4235, Time: 0.72s\n",
      "Epoch: 35, Batch: 481/842, Loss: 0.4418, Time: 0.73s\n",
      "Epoch: 35, Batch: 491/842, Loss: 0.4201, Time: 0.73s\n",
      "Epoch: 35, Batch: 501/842, Loss: 0.6312, Time: 0.72s\n",
      "Epoch: 35, Batch: 511/842, Loss: 0.6014, Time: 0.72s\n",
      "Epoch: 35, Batch: 521/842, Loss: 0.4878, Time: 0.73s\n",
      "Epoch: 35, Batch: 531/842, Loss: 0.5485, Time: 0.75s\n",
      "Epoch: 35, Batch: 541/842, Loss: 0.6154, Time: 0.75s\n",
      "Epoch: 35, Batch: 551/842, Loss: 0.6038, Time: 0.74s\n",
      "Epoch: 35, Batch: 561/842, Loss: 0.5140, Time: 0.75s\n",
      "Epoch: 35, Batch: 571/842, Loss: 0.5701, Time: 0.75s\n",
      "Epoch: 35, Batch: 581/842, Loss: 0.6757, Time: 0.78s\n",
      "Epoch: 35, Batch: 591/842, Loss: 0.4648, Time: 0.87s\n",
      "Epoch: 35, Batch: 601/842, Loss: 0.5430, Time: 0.87s\n",
      "Epoch: 35, Batch: 611/842, Loss: 0.5397, Time: 0.89s\n",
      "Epoch: 35, Batch: 621/842, Loss: 0.4395, Time: 0.89s\n",
      "Epoch: 35, Batch: 631/842, Loss: 0.4853, Time: 0.87s\n",
      "Epoch: 35, Batch: 641/842, Loss: 0.4386, Time: 0.87s\n",
      "Epoch: 35, Batch: 651/842, Loss: 0.5134, Time: 0.86s\n",
      "Epoch: 35, Batch: 661/842, Loss: 0.5845, Time: 0.89s\n",
      "Epoch: 35, Batch: 671/842, Loss: 0.4489, Time: 0.79s\n",
      "Epoch: 35, Batch: 681/842, Loss: 0.6071, Time: 0.76s\n",
      "Epoch: 35, Batch: 691/842, Loss: 0.5008, Time: 0.74s\n",
      "Epoch: 35, Batch: 701/842, Loss: 0.5769, Time: 0.72s\n",
      "Epoch: 35, Batch: 711/842, Loss: 0.5395, Time: 0.73s\n",
      "Epoch: 35, Batch: 721/842, Loss: 0.5450, Time: 0.72s\n",
      "Epoch: 35, Batch: 731/842, Loss: 0.5119, Time: 0.72s\n",
      "Epoch: 35, Batch: 741/842, Loss: 0.3575, Time: 0.73s\n",
      "Epoch: 35, Batch: 751/842, Loss: 0.5837, Time: 0.73s\n",
      "Epoch: 35, Batch: 761/842, Loss: 0.4884, Time: 0.73s\n",
      "Epoch: 35, Batch: 771/842, Loss: 0.4825, Time: 0.74s\n",
      "Epoch: 35, Batch: 781/842, Loss: 0.4191, Time: 0.75s\n",
      "Epoch: 35, Batch: 791/842, Loss: 0.5535, Time: 0.73s\n",
      "Epoch: 35, Batch: 801/842, Loss: 0.5136, Time: 0.76s\n",
      "Epoch: 35, Batch: 811/842, Loss: 0.6472, Time: 0.72s\n",
      "Epoch: 35, Batch: 821/842, Loss: 0.4131, Time: 0.70s\n",
      "Epoch: 35, Batch: 831/842, Loss: 0.7366, Time: 0.70s\n",
      "Epoch: 35, Batch: 841/842, Loss: 0.5850, Time: 0.68s\n",
      "Epoch 35/100: Train Loss: 0.5027, Val Loss: 0.5025, mIoU: 0.6239, F1: 0.7594, OA: 0.9587\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7574, F1=0.8619, OA=0.8572, Precision=0.8074, Recall=0.9244\n",
      "    Class 1: IoU=0.4399, F1=0.6110, OA=0.9895, Precision=0.5027, Recall=0.7787\n",
      "    Class 2: IoU=0.8593, F1=0.9243, OA=0.9705, Precision=0.9612, Recall=0.8901\n",
      "    Class 3: IoU=0.4530, F1=0.6235, OA=0.9603, Precision=0.7367, Recall=0.5405\n",
      "    Class 4: IoU=0.7102, F1=0.8305, OA=0.9769, Precision=0.8288, Recall=0.8322\n",
      "    Class 5: IoU=0.6452, F1=0.7843, OA=0.9962, Precision=0.8026, Recall=0.7668\n",
      "    Class 6: IoU=0.5326, F1=0.6950, OA=0.9548, Precision=0.8365, Recall=0.5945\n",
      "    Class 7: IoU=0.5935, F1=0.7449, OA=0.9641, Precision=0.8638, Recall=0.6548\n",
      "Epoch: 36, Batch: 1/842, Loss: 0.3740, Time: 2.01s\n",
      "Epoch: 36, Batch: 11/842, Loss: 0.4207, Time: 0.73s\n",
      "Epoch: 36, Batch: 21/842, Loss: 0.5687, Time: 0.74s\n",
      "Epoch: 36, Batch: 31/842, Loss: 0.4907, Time: 0.73s\n",
      "Epoch: 36, Batch: 41/842, Loss: 0.4533, Time: 0.73s\n",
      "Epoch: 36, Batch: 51/842, Loss: 0.5376, Time: 0.76s\n",
      "Epoch: 36, Batch: 61/842, Loss: 0.5233, Time: 0.74s\n",
      "Epoch: 36, Batch: 71/842, Loss: 0.4488, Time: 0.72s\n",
      "Epoch: 36, Batch: 81/842, Loss: 0.5900, Time: 0.73s\n",
      "Epoch: 36, Batch: 91/842, Loss: 0.5432, Time: 0.74s\n",
      "Epoch: 36, Batch: 101/842, Loss: 0.6181, Time: 0.74s\n",
      "Epoch: 36, Batch: 111/842, Loss: 0.4681, Time: 0.75s\n",
      "Epoch: 36, Batch: 121/842, Loss: 0.4060, Time: 0.74s\n",
      "Epoch: 36, Batch: 131/842, Loss: 0.4788, Time: 0.73s\n",
      "Epoch: 36, Batch: 141/842, Loss: 0.3289, Time: 0.72s\n",
      "Epoch: 36, Batch: 151/842, Loss: 0.5017, Time: 0.72s\n",
      "Epoch: 36, Batch: 161/842, Loss: 0.3333, Time: 0.72s\n",
      "Epoch: 36, Batch: 171/842, Loss: 0.5385, Time: 0.74s\n",
      "Epoch: 36, Batch: 181/842, Loss: 0.6360, Time: 0.75s\n",
      "Epoch: 36, Batch: 191/842, Loss: 0.4512, Time: 0.75s\n",
      "Epoch: 36, Batch: 201/842, Loss: 0.5127, Time: 0.79s\n",
      "Epoch: 36, Batch: 211/842, Loss: 0.4865, Time: 0.75s\n",
      "Epoch: 36, Batch: 221/842, Loss: 0.4287, Time: 0.76s\n",
      "Epoch: 36, Batch: 231/842, Loss: 0.4811, Time: 0.76s\n",
      "Epoch: 36, Batch: 241/842, Loss: 0.5770, Time: 0.74s\n",
      "Epoch: 36, Batch: 251/842, Loss: 0.6530, Time: 0.75s\n",
      "Epoch: 36, Batch: 261/842, Loss: 0.4242, Time: 0.74s\n",
      "Epoch: 36, Batch: 271/842, Loss: 0.4825, Time: 0.73s\n",
      "Epoch: 36, Batch: 281/842, Loss: 0.3801, Time: 0.73s\n",
      "Epoch: 36, Batch: 291/842, Loss: 0.4497, Time: 0.73s\n",
      "Epoch: 36, Batch: 301/842, Loss: 0.6368, Time: 0.74s\n",
      "Epoch: 36, Batch: 311/842, Loss: 0.6584, Time: 0.76s\n",
      "Epoch: 36, Batch: 321/842, Loss: 0.4911, Time: 0.74s\n",
      "Epoch: 36, Batch: 331/842, Loss: 0.5114, Time: 0.74s\n",
      "Epoch: 36, Batch: 341/842, Loss: 0.5129, Time: 0.74s\n",
      "Epoch: 36, Batch: 351/842, Loss: 0.4923, Time: 0.76s\n",
      "Epoch: 36, Batch: 361/842, Loss: 0.4564, Time: 0.73s\n",
      "Epoch: 36, Batch: 371/842, Loss: 0.5814, Time: 0.73s\n",
      "Epoch: 36, Batch: 381/842, Loss: 0.6613, Time: 0.74s\n",
      "Epoch: 36, Batch: 391/842, Loss: 0.4396, Time: 0.74s\n",
      "Epoch: 36, Batch: 401/842, Loss: 0.5305, Time: 0.73s\n",
      "Epoch: 36, Batch: 411/842, Loss: 0.5281, Time: 0.73s\n",
      "Epoch: 36, Batch: 421/842, Loss: 0.6188, Time: 0.73s\n",
      "Epoch: 36, Batch: 431/842, Loss: 0.4603, Time: 0.72s\n",
      "Epoch: 36, Batch: 441/842, Loss: 0.4532, Time: 0.74s\n",
      "Epoch: 36, Batch: 451/842, Loss: 0.6275, Time: 0.74s\n",
      "Epoch: 36, Batch: 461/842, Loss: 0.4156, Time: 0.73s\n",
      "Epoch: 36, Batch: 471/842, Loss: 0.6218, Time: 0.75s\n",
      "Epoch: 36, Batch: 481/842, Loss: 0.5323, Time: 0.75s\n",
      "Epoch: 36, Batch: 491/842, Loss: 0.4663, Time: 0.73s\n",
      "Epoch: 36, Batch: 501/842, Loss: 0.5567, Time: 0.75s\n",
      "Epoch: 36, Batch: 511/842, Loss: 0.4351, Time: 0.73s\n",
      "Epoch: 36, Batch: 521/842, Loss: 0.3942, Time: 0.73s\n",
      "Epoch: 36, Batch: 531/842, Loss: 0.6892, Time: 0.75s\n",
      "Epoch: 36, Batch: 541/842, Loss: 0.4987, Time: 0.71s\n",
      "Epoch: 36, Batch: 551/842, Loss: 0.5045, Time: 0.71s\n",
      "Epoch: 36, Batch: 561/842, Loss: 0.4657, Time: 0.67s\n",
      "Epoch: 36, Batch: 571/842, Loss: 0.3940, Time: 0.79s\n",
      "Epoch: 36, Batch: 581/842, Loss: 0.4447, Time: 0.85s\n",
      "Epoch: 36, Batch: 591/842, Loss: 0.4543, Time: 0.76s\n",
      "Epoch: 36, Batch: 601/842, Loss: 0.3540, Time: 0.74s\n",
      "Epoch: 36, Batch: 611/842, Loss: 0.3403, Time: 0.74s\n",
      "Epoch: 36, Batch: 621/842, Loss: 0.4265, Time: 0.73s\n",
      "Epoch: 36, Batch: 631/842, Loss: 0.5199, Time: 0.74s\n",
      "Epoch: 36, Batch: 641/842, Loss: 0.5469, Time: 0.76s\n",
      "Epoch: 36, Batch: 651/842, Loss: 0.5924, Time: 0.75s\n",
      "Epoch: 36, Batch: 661/842, Loss: 0.6571, Time: 0.78s\n",
      "Epoch: 36, Batch: 671/842, Loss: 0.4215, Time: 0.81s\n",
      "Epoch: 36, Batch: 681/842, Loss: 0.6159, Time: 0.73s\n",
      "Epoch: 36, Batch: 691/842, Loss: 0.4273, Time: 0.86s\n",
      "Epoch: 36, Batch: 701/842, Loss: 0.6619, Time: 0.94s\n",
      "Epoch: 36, Batch: 711/842, Loss: 0.4725, Time: 0.97s\n",
      "Epoch: 36, Batch: 721/842, Loss: 0.3964, Time: 0.95s\n",
      "Epoch: 36, Batch: 731/842, Loss: 0.4556, Time: 0.91s\n",
      "Epoch: 36, Batch: 741/842, Loss: 0.5352, Time: 0.74s\n",
      "Epoch: 36, Batch: 751/842, Loss: 0.4959, Time: 0.77s\n",
      "Epoch: 36, Batch: 761/842, Loss: 0.5589, Time: 0.76s\n",
      "Epoch: 36, Batch: 771/842, Loss: 0.4087, Time: 0.77s\n",
      "Epoch: 36, Batch: 781/842, Loss: 0.4306, Time: 0.77s\n",
      "Epoch: 36, Batch: 791/842, Loss: 0.5846, Time: 0.78s\n",
      "Epoch: 36, Batch: 801/842, Loss: 0.5141, Time: 0.76s\n",
      "Epoch: 36, Batch: 811/842, Loss: 0.4126, Time: 0.77s\n",
      "Epoch: 36, Batch: 821/842, Loss: 0.5042, Time: 0.78s\n",
      "Epoch: 36, Batch: 831/842, Loss: 0.5778, Time: 0.79s\n",
      "Epoch: 36, Batch: 841/842, Loss: 0.4979, Time: 0.74s\n",
      "Epoch 36/100: Train Loss: 0.5063, Val Loss: 0.5246, mIoU: 0.6158, F1: 0.7535, OA: 0.9573\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7487, F1=0.8563, OA=0.8505, Precision=0.7979, Recall=0.9239\n",
      "    Class 1: IoU=0.6044, F1=0.7534, OA=0.9949, Precision=0.7707, Recall=0.7370\n",
      "    Class 2: IoU=0.8591, F1=0.9242, OA=0.9704, Precision=0.9618, Recall=0.8894\n",
      "    Class 3: IoU=0.4260, F1=0.5975, OA=0.9592, Precision=0.7474, Recall=0.4977\n",
      "    Class 4: IoU=0.7060, F1=0.8277, OA=0.9767, Precision=0.8340, Recall=0.8215\n",
      "    Class 5: IoU=0.4941, F1=0.6614, OA=0.9925, Precision=0.5555, Recall=0.8170\n",
      "    Class 6: IoU=0.5018, F1=0.6683, OA=0.9521, Precision=0.8360, Recall=0.5566\n",
      "    Class 7: IoU=0.5862, F1=0.7392, OA=0.9620, Precision=0.8196, Recall=0.6731\n",
      "Epoch: 37, Batch: 1/842, Loss: 0.4554, Time: 2.10s\n",
      "Epoch: 37, Batch: 11/842, Loss: 0.5694, Time: 0.81s\n",
      "Epoch: 37, Batch: 21/842, Loss: 0.4976, Time: 0.76s\n",
      "Epoch: 37, Batch: 31/842, Loss: 0.5543, Time: 0.77s\n",
      "Epoch: 37, Batch: 41/842, Loss: 0.4698, Time: 0.78s\n",
      "Epoch: 37, Batch: 51/842, Loss: 0.6624, Time: 0.76s\n",
      "Epoch: 37, Batch: 61/842, Loss: 0.3828, Time: 0.77s\n",
      "Epoch: 37, Batch: 71/842, Loss: 0.5215, Time: 0.78s\n",
      "Epoch: 37, Batch: 81/842, Loss: 0.4480, Time: 0.89s\n",
      "Epoch: 37, Batch: 91/842, Loss: 0.5647, Time: 0.84s\n",
      "Epoch: 37, Batch: 101/842, Loss: 0.4768, Time: 0.76s\n",
      "Epoch: 37, Batch: 111/842, Loss: 0.4886, Time: 0.75s\n",
      "Epoch: 37, Batch: 121/842, Loss: 0.4168, Time: 0.75s\n",
      "Epoch: 37, Batch: 131/842, Loss: 0.4852, Time: 0.76s\n",
      "Epoch: 37, Batch: 141/842, Loss: 0.3656, Time: 0.76s\n",
      "Epoch: 37, Batch: 151/842, Loss: 0.4994, Time: 0.73s\n",
      "Epoch: 37, Batch: 161/842, Loss: 0.6861, Time: 0.75s\n",
      "Epoch: 37, Batch: 171/842, Loss: 0.5442, Time: 0.74s\n",
      "Epoch: 37, Batch: 181/842, Loss: 0.5355, Time: 0.74s\n",
      "Epoch: 37, Batch: 191/842, Loss: 0.5257, Time: 0.75s\n",
      "Epoch: 37, Batch: 201/842, Loss: 0.6273, Time: 0.74s\n",
      "Epoch: 37, Batch: 211/842, Loss: 0.5284, Time: 0.86s\n",
      "Epoch: 37, Batch: 221/842, Loss: 0.5087, Time: 0.80s\n",
      "Epoch: 37, Batch: 231/842, Loss: 0.5065, Time: 0.80s\n",
      "Epoch: 37, Batch: 241/842, Loss: 0.5123, Time: 0.78s\n",
      "Epoch: 37, Batch: 251/842, Loss: 0.4610, Time: 0.80s\n",
      "Epoch: 37, Batch: 261/842, Loss: 0.5199, Time: 0.78s\n",
      "Epoch: 37, Batch: 271/842, Loss: 0.5166, Time: 0.78s\n",
      "Epoch: 37, Batch: 281/842, Loss: 0.5683, Time: 0.83s\n",
      "Epoch: 37, Batch: 291/842, Loss: 0.5557, Time: 0.78s\n",
      "Epoch: 37, Batch: 301/842, Loss: 0.4836, Time: 0.77s\n",
      "Epoch: 37, Batch: 311/842, Loss: 0.5349, Time: 0.79s\n",
      "Epoch: 37, Batch: 321/842, Loss: 0.4564, Time: 0.80s\n",
      "Epoch: 37, Batch: 331/842, Loss: 0.5016, Time: 0.78s\n",
      "Epoch: 37, Batch: 341/842, Loss: 0.4911, Time: 0.81s\n",
      "Epoch: 37, Batch: 351/842, Loss: 0.4183, Time: 0.80s\n",
      "Epoch: 37, Batch: 361/842, Loss: 0.4726, Time: 0.80s\n",
      "Epoch: 37, Batch: 371/842, Loss: 0.5165, Time: 0.80s\n",
      "Epoch: 37, Batch: 381/842, Loss: 0.4962, Time: 0.81s\n",
      "Epoch: 37, Batch: 391/842, Loss: 0.4830, Time: 0.82s\n",
      "Epoch: 37, Batch: 401/842, Loss: 0.4895, Time: 0.79s\n",
      "Epoch: 37, Batch: 411/842, Loss: 0.5117, Time: 0.78s\n",
      "Epoch: 37, Batch: 421/842, Loss: 0.2693, Time: 0.82s\n",
      "Epoch: 37, Batch: 431/842, Loss: 0.4962, Time: 0.80s\n",
      "Epoch: 37, Batch: 441/842, Loss: 0.5237, Time: 0.80s\n",
      "Epoch: 37, Batch: 451/842, Loss: 0.5823, Time: 0.79s\n",
      "Epoch: 37, Batch: 461/842, Loss: 0.4826, Time: 0.82s\n",
      "Epoch: 37, Batch: 471/842, Loss: 0.4883, Time: 0.81s\n",
      "Epoch: 37, Batch: 481/842, Loss: 0.3781, Time: 0.79s\n",
      "Epoch: 37, Batch: 491/842, Loss: 0.6002, Time: 0.78s\n",
      "Epoch: 37, Batch: 501/842, Loss: 0.7196, Time: 0.75s\n",
      "Epoch: 37, Batch: 511/842, Loss: 0.6217, Time: 0.76s\n",
      "Epoch: 37, Batch: 521/842, Loss: 0.3764, Time: 0.78s\n",
      "Epoch: 37, Batch: 531/842, Loss: 0.3989, Time: 0.79s\n",
      "Epoch: 37, Batch: 541/842, Loss: 0.4657, Time: 0.81s\n",
      "Epoch: 37, Batch: 551/842, Loss: 0.3971, Time: 0.81s\n",
      "Epoch: 37, Batch: 561/842, Loss: 0.3645, Time: 0.85s\n",
      "Epoch: 37, Batch: 571/842, Loss: 0.5632, Time: 0.83s\n",
      "Epoch: 37, Batch: 581/842, Loss: 0.4770, Time: 0.73s\n",
      "Epoch: 37, Batch: 591/842, Loss: 0.5927, Time: 0.84s\n",
      "Epoch: 37, Batch: 601/842, Loss: 0.5985, Time: 0.80s\n",
      "Epoch: 37, Batch: 611/842, Loss: 0.5228, Time: 0.79s\n",
      "Epoch: 37, Batch: 621/842, Loss: 0.3691, Time: 0.80s\n",
      "Epoch: 37, Batch: 631/842, Loss: 0.5790, Time: 0.76s\n",
      "Epoch: 37, Batch: 641/842, Loss: 0.5146, Time: 0.79s\n",
      "Epoch: 37, Batch: 651/842, Loss: 0.5197, Time: 0.77s\n",
      "Epoch: 37, Batch: 661/842, Loss: 0.6759, Time: 0.78s\n",
      "Epoch: 37, Batch: 671/842, Loss: 0.4061, Time: 0.79s\n",
      "Epoch: 37, Batch: 681/842, Loss: 0.5042, Time: 0.78s\n",
      "Epoch: 37, Batch: 691/842, Loss: 0.6412, Time: 0.83s\n",
      "Epoch: 37, Batch: 701/842, Loss: 0.5575, Time: 0.79s\n",
      "Epoch: 37, Batch: 711/842, Loss: 0.3470, Time: 0.82s\n",
      "Epoch: 37, Batch: 721/842, Loss: 0.5205, Time: 0.83s\n",
      "Epoch: 37, Batch: 731/842, Loss: 0.5043, Time: 0.76s\n",
      "Epoch: 37, Batch: 741/842, Loss: 0.4262, Time: 0.78s\n",
      "Epoch: 37, Batch: 751/842, Loss: 0.4699, Time: 0.77s\n",
      "Epoch: 37, Batch: 761/842, Loss: 0.6891, Time: 0.77s\n",
      "Epoch: 37, Batch: 771/842, Loss: 0.5741, Time: 0.80s\n",
      "Epoch: 37, Batch: 781/842, Loss: 0.4861, Time: 0.79s\n",
      "Epoch: 37, Batch: 791/842, Loss: 0.3856, Time: 0.79s\n",
      "Epoch: 37, Batch: 801/842, Loss: 0.4642, Time: 0.78s\n",
      "Epoch: 37, Batch: 811/842, Loss: 0.5786, Time: 0.78s\n",
      "Epoch: 37, Batch: 821/842, Loss: 0.4196, Time: 0.79s\n",
      "Epoch: 37, Batch: 831/842, Loss: 0.4724, Time: 0.78s\n",
      "Epoch: 37, Batch: 841/842, Loss: 0.5493, Time: 0.80s\n",
      "Epoch 37/100: Train Loss: 0.5029, Val Loss: 0.4862, mIoU: 0.6497, F1: 0.7789, OA: 0.9599\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7556, F1=0.8608, OA=0.8537, Precision=0.7954, Recall=0.9378\n",
      "    Class 1: IoU=0.6800, F1=0.8095, OA=0.9962, Precision=0.8542, Recall=0.7693\n",
      "    Class 2: IoU=0.9001, F1=0.9474, OA=0.9790, Precision=0.9619, Recall=0.9334\n",
      "    Class 3: IoU=0.4086, F1=0.5801, OA=0.9601, Precision=0.8056, Recall=0.4532\n",
      "    Class 4: IoU=0.7033, F1=0.8258, OA=0.9776, Precision=0.8793, Recall=0.7785\n",
      "    Class 5: IoU=0.6422, F1=0.7821, OA=0.9962, Precision=0.8074, Recall=0.7584\n",
      "    Class 6: IoU=0.5301, F1=0.6929, OA=0.9538, Precision=0.8179, Recall=0.6010\n",
      "    Class 7: IoU=0.5779, F1=0.7325, OA=0.9629, Precision=0.8655, Recall=0.6349\n",
      "Epoch: 38, Batch: 1/842, Loss: 0.5036, Time: 1.93s\n",
      "Epoch: 38, Batch: 11/842, Loss: 0.5392, Time: 0.82s\n",
      "Epoch: 38, Batch: 21/842, Loss: 0.4374, Time: 0.83s\n",
      "Epoch: 38, Batch: 31/842, Loss: 0.4637, Time: 0.87s\n",
      "Epoch: 38, Batch: 41/842, Loss: 0.6719, Time: 0.82s\n",
      "Epoch: 38, Batch: 51/842, Loss: 0.5107, Time: 0.78s\n",
      "Epoch: 38, Batch: 61/842, Loss: 0.4688, Time: 0.80s\n",
      "Epoch: 38, Batch: 71/842, Loss: 0.5404, Time: 0.80s\n",
      "Epoch: 38, Batch: 81/842, Loss: 0.4234, Time: 0.80s\n",
      "Epoch: 38, Batch: 91/842, Loss: 0.4249, Time: 0.80s\n",
      "Epoch: 38, Batch: 101/842, Loss: 0.5368, Time: 0.81s\n",
      "Epoch: 38, Batch: 111/842, Loss: 0.4843, Time: 0.79s\n",
      "Epoch: 38, Batch: 121/842, Loss: 0.4047, Time: 0.79s\n",
      "Epoch: 38, Batch: 131/842, Loss: 0.4365, Time: 0.80s\n",
      "Epoch: 38, Batch: 141/842, Loss: 0.5404, Time: 0.82s\n",
      "Epoch: 38, Batch: 151/842, Loss: 0.4864, Time: 0.80s\n",
      "Epoch: 38, Batch: 161/842, Loss: 0.6701, Time: 0.81s\n",
      "Epoch: 38, Batch: 171/842, Loss: 0.4944, Time: 0.80s\n",
      "Epoch: 38, Batch: 181/842, Loss: 0.5760, Time: 0.79s\n",
      "Epoch: 38, Batch: 191/842, Loss: 0.4167, Time: 0.81s\n",
      "Epoch: 38, Batch: 201/842, Loss: 0.5875, Time: 0.80s\n",
      "Epoch: 38, Batch: 211/842, Loss: 0.3850, Time: 0.82s\n",
      "Epoch: 38, Batch: 221/842, Loss: 0.5967, Time: 0.80s\n",
      "Epoch: 38, Batch: 231/842, Loss: 0.5367, Time: 0.80s\n",
      "Epoch: 38, Batch: 241/842, Loss: 0.5685, Time: 0.81s\n",
      "Epoch: 38, Batch: 251/842, Loss: 0.5507, Time: 0.83s\n",
      "Epoch: 38, Batch: 261/842, Loss: 0.4569, Time: 0.81s\n",
      "Epoch: 38, Batch: 271/842, Loss: 0.5298, Time: 0.83s\n",
      "Epoch: 38, Batch: 281/842, Loss: 0.4324, Time: 0.80s\n",
      "Epoch: 38, Batch: 291/842, Loss: 0.5909, Time: 0.81s\n",
      "Epoch: 38, Batch: 301/842, Loss: 0.5894, Time: 0.81s\n",
      "Epoch: 38, Batch: 311/842, Loss: 0.5466, Time: 0.81s\n",
      "Epoch: 38, Batch: 321/842, Loss: 0.4858, Time: 0.80s\n",
      "Epoch: 38, Batch: 331/842, Loss: 0.7073, Time: 0.81s\n",
      "Epoch: 38, Batch: 341/842, Loss: 0.4978, Time: 0.82s\n",
      "Epoch: 38, Batch: 351/842, Loss: 0.4637, Time: 0.83s\n",
      "Epoch: 38, Batch: 361/842, Loss: 0.4152, Time: 0.80s\n",
      "Epoch: 38, Batch: 371/842, Loss: 0.3597, Time: 0.79s\n",
      "Epoch: 38, Batch: 381/842, Loss: 0.5042, Time: 0.78s\n",
      "Epoch: 38, Batch: 391/842, Loss: 0.4896, Time: 0.79s\n",
      "Epoch: 38, Batch: 401/842, Loss: 0.4187, Time: 0.79s\n",
      "Epoch: 38, Batch: 411/842, Loss: 0.5240, Time: 0.78s\n",
      "Epoch: 38, Batch: 421/842, Loss: 0.6852, Time: 0.77s\n",
      "Epoch: 38, Batch: 431/842, Loss: 0.4247, Time: 0.80s\n",
      "Epoch: 38, Batch: 441/842, Loss: 0.5165, Time: 0.80s\n",
      "Epoch: 38, Batch: 451/842, Loss: 0.6122, Time: 0.80s\n",
      "Epoch: 38, Batch: 461/842, Loss: 0.4102, Time: 0.79s\n",
      "Epoch: 38, Batch: 471/842, Loss: 0.3557, Time: 0.78s\n",
      "Epoch: 38, Batch: 481/842, Loss: 0.5847, Time: 0.79s\n",
      "Epoch: 38, Batch: 491/842, Loss: 0.4479, Time: 1.60s\n",
      "Epoch: 38, Batch: 501/842, Loss: 0.5814, Time: 0.79s\n",
      "Epoch: 38, Batch: 511/842, Loss: 0.4517, Time: 0.79s\n",
      "Epoch: 38, Batch: 521/842, Loss: 0.5282, Time: 0.82s\n",
      "Epoch: 38, Batch: 531/842, Loss: 0.4332, Time: 0.79s\n",
      "Epoch: 38, Batch: 541/842, Loss: 0.4837, Time: 0.79s\n",
      "Epoch: 38, Batch: 551/842, Loss: 0.5247, Time: 0.80s\n",
      "Epoch: 38, Batch: 561/842, Loss: 0.3134, Time: 0.78s\n",
      "Epoch: 38, Batch: 571/842, Loss: 0.6160, Time: 0.79s\n",
      "Epoch: 38, Batch: 581/842, Loss: 0.4367, Time: 0.79s\n",
      "Epoch: 38, Batch: 591/842, Loss: 0.5567, Time: 0.79s\n",
      "Epoch: 38, Batch: 601/842, Loss: 0.4634, Time: 0.80s\n",
      "Epoch: 38, Batch: 611/842, Loss: 0.5913, Time: 0.83s\n",
      "Epoch: 38, Batch: 621/842, Loss: 0.5652, Time: 0.82s\n",
      "Epoch: 38, Batch: 631/842, Loss: 0.5377, Time: 0.82s\n",
      "Epoch: 38, Batch: 641/842, Loss: 0.4075, Time: 0.86s\n",
      "Epoch: 38, Batch: 651/842, Loss: 0.5128, Time: 0.78s\n",
      "Epoch: 38, Batch: 661/842, Loss: 0.6076, Time: 0.81s\n",
      "Epoch: 38, Batch: 671/842, Loss: 0.6390, Time: 0.80s\n",
      "Epoch: 38, Batch: 681/842, Loss: 0.5273, Time: 0.79s\n",
      "Epoch: 38, Batch: 691/842, Loss: 0.6297, Time: 0.79s\n",
      "Epoch: 38, Batch: 701/842, Loss: 0.5525, Time: 0.79s\n",
      "Epoch: 38, Batch: 711/842, Loss: 0.5611, Time: 0.79s\n",
      "Epoch: 38, Batch: 721/842, Loss: 0.4209, Time: 0.80s\n",
      "Epoch: 38, Batch: 731/842, Loss: 0.3866, Time: 0.77s\n",
      "Epoch: 38, Batch: 741/842, Loss: 0.5292, Time: 0.80s\n",
      "Epoch: 38, Batch: 751/842, Loss: 0.6708, Time: 0.79s\n",
      "Epoch: 38, Batch: 761/842, Loss: 0.5077, Time: 0.79s\n",
      "Epoch: 38, Batch: 771/842, Loss: 0.4197, Time: 0.78s\n",
      "Epoch: 38, Batch: 781/842, Loss: 0.6016, Time: 0.79s\n",
      "Epoch: 38, Batch: 791/842, Loss: 0.5257, Time: 0.79s\n",
      "Epoch: 38, Batch: 801/842, Loss: 0.5576, Time: 0.79s\n",
      "Epoch: 38, Batch: 811/842, Loss: 0.4998, Time: 0.84s\n",
      "Epoch: 38, Batch: 821/842, Loss: 0.3626, Time: 0.83s\n",
      "Epoch: 38, Batch: 831/842, Loss: 0.3897, Time: 0.79s\n",
      "Epoch: 38, Batch: 841/842, Loss: 0.5344, Time: 0.83s\n",
      "Epoch 38/100: Train Loss: 0.5025, Val Loss: 0.4809, mIoU: 0.6477, F1: 0.7787, OA: 0.9606\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7622, F1=0.8651, OA=0.8607, Precision=0.8116, Recall=0.9261\n",
      "    Class 1: IoU=0.5786, F1=0.7330, OA=0.9939, Precision=0.6820, Recall=0.7923\n",
      "    Class 2: IoU=0.8903, F1=0.9420, OA=0.9769, Precision=0.9604, Recall=0.9243\n",
      "    Class 3: IoU=0.4549, F1=0.6254, OA=0.9606, Precision=0.7423, Recall=0.5402\n",
      "    Class 4: IoU=0.7246, F1=0.8403, OA=0.9786, Precision=0.8566, Recall=0.8247\n",
      "    Class 5: IoU=0.6492, F1=0.7873, OA=0.9964, Precision=0.8370, Recall=0.7432\n",
      "    Class 6: IoU=0.5402, F1=0.7015, OA=0.9536, Precision=0.7921, Recall=0.6295\n",
      "    Class 7: IoU=0.5816, F1=0.7354, OA=0.9641, Precision=0.8956, Recall=0.6238\n",
      "Epoch: 39, Batch: 1/842, Loss: 0.5915, Time: 1.55s\n",
      "Epoch: 39, Batch: 11/842, Loss: 0.4845, Time: 0.81s\n",
      "Epoch: 39, Batch: 21/842, Loss: 0.5306, Time: 0.81s\n",
      "Epoch: 39, Batch: 31/842, Loss: 0.4198, Time: 0.83s\n",
      "Epoch: 39, Batch: 41/842, Loss: 0.4777, Time: 0.81s\n",
      "Epoch: 39, Batch: 51/842, Loss: 0.6880, Time: 0.81s\n",
      "Epoch: 39, Batch: 61/842, Loss: 0.2657, Time: 0.79s\n",
      "Epoch: 39, Batch: 71/842, Loss: 0.3833, Time: 0.77s\n",
      "Epoch: 39, Batch: 81/842, Loss: 0.4153, Time: 0.84s\n",
      "Epoch: 39, Batch: 91/842, Loss: 0.3302, Time: 0.81s\n",
      "Epoch: 39, Batch: 101/842, Loss: 0.3028, Time: 0.78s\n",
      "Epoch: 39, Batch: 111/842, Loss: 0.4929, Time: 0.80s\n",
      "Epoch: 39, Batch: 121/842, Loss: 0.4441, Time: 0.84s\n",
      "Epoch: 39, Batch: 131/842, Loss: 0.5570, Time: 0.97s\n",
      "Epoch: 39, Batch: 141/842, Loss: 0.4661, Time: 0.85s\n",
      "Epoch: 39, Batch: 151/842, Loss: 0.4242, Time: 0.79s\n",
      "Epoch: 39, Batch: 161/842, Loss: 0.4991, Time: 0.84s\n",
      "Epoch: 39, Batch: 171/842, Loss: 0.5902, Time: 0.81s\n",
      "Epoch: 39, Batch: 181/842, Loss: 0.5113, Time: 0.80s\n",
      "Epoch: 39, Batch: 191/842, Loss: 0.6242, Time: 0.83s\n",
      "Epoch: 39, Batch: 201/842, Loss: 0.6010, Time: 0.82s\n",
      "Epoch: 39, Batch: 211/842, Loss: 0.6260, Time: 0.82s\n",
      "Epoch: 39, Batch: 221/842, Loss: 0.4325, Time: 0.84s\n",
      "Epoch: 39, Batch: 231/842, Loss: 0.3915, Time: 0.80s\n",
      "Epoch: 39, Batch: 241/842, Loss: 0.6180, Time: 0.79s\n",
      "Epoch: 39, Batch: 251/842, Loss: 0.3616, Time: 0.77s\n",
      "Epoch: 39, Batch: 261/842, Loss: 0.4568, Time: 0.81s\n",
      "Epoch: 39, Batch: 271/842, Loss: 0.5693, Time: 0.79s\n",
      "Epoch: 39, Batch: 281/842, Loss: 0.3812, Time: 0.79s\n",
      "Epoch: 39, Batch: 291/842, Loss: 0.5047, Time: 0.80s\n",
      "Epoch: 39, Batch: 301/842, Loss: 0.4296, Time: 0.78s\n",
      "Epoch: 39, Batch: 311/842, Loss: 0.4588, Time: 0.78s\n",
      "Epoch: 39, Batch: 321/842, Loss: 0.5539, Time: 0.82s\n",
      "Epoch: 39, Batch: 331/842, Loss: 0.4999, Time: 0.82s\n",
      "Epoch: 39, Batch: 341/842, Loss: 0.5237, Time: 0.82s\n",
      "Epoch: 39, Batch: 351/842, Loss: 0.5260, Time: 0.79s\n",
      "Epoch: 39, Batch: 361/842, Loss: 0.4423, Time: 0.78s\n",
      "Epoch: 39, Batch: 371/842, Loss: 0.5633, Time: 0.79s\n",
      "Epoch: 39, Batch: 381/842, Loss: 0.4937, Time: 0.77s\n",
      "Epoch: 39, Batch: 391/842, Loss: 0.5778, Time: 0.91s\n",
      "Epoch: 39, Batch: 401/842, Loss: 0.4586, Time: 0.81s\n",
      "Epoch: 39, Batch: 411/842, Loss: 0.4719, Time: 0.81s\n",
      "Epoch: 39, Batch: 421/842, Loss: 0.5819, Time: 0.78s\n",
      "Epoch: 39, Batch: 431/842, Loss: 0.5369, Time: 0.78s\n",
      "Epoch: 39, Batch: 441/842, Loss: 0.4380, Time: 0.83s\n",
      "Epoch: 39, Batch: 451/842, Loss: 0.5300, Time: 0.79s\n",
      "Epoch: 39, Batch: 461/842, Loss: 0.6288, Time: 0.77s\n",
      "Epoch: 39, Batch: 471/842, Loss: 0.3540, Time: 0.78s\n",
      "Epoch: 39, Batch: 481/842, Loss: 0.6041, Time: 0.75s\n",
      "Epoch: 39, Batch: 491/842, Loss: 0.6955, Time: 0.82s\n",
      "Epoch: 39, Batch: 501/842, Loss: 0.5723, Time: 0.77s\n",
      "Epoch: 39, Batch: 511/842, Loss: 0.6097, Time: 0.77s\n",
      "Epoch: 39, Batch: 521/842, Loss: 0.6858, Time: 0.77s\n",
      "Epoch: 39, Batch: 531/842, Loss: 0.4582, Time: 0.75s\n",
      "Epoch: 39, Batch: 541/842, Loss: 0.5186, Time: 0.75s\n",
      "Epoch: 39, Batch: 551/842, Loss: 0.4830, Time: 0.77s\n",
      "Epoch: 39, Batch: 561/842, Loss: 0.4948, Time: 0.76s\n",
      "Epoch: 39, Batch: 571/842, Loss: 0.5008, Time: 0.76s\n",
      "Epoch: 39, Batch: 581/842, Loss: 0.4227, Time: 0.80s\n",
      "Epoch: 39, Batch: 591/842, Loss: 0.4464, Time: 0.78s\n",
      "Epoch: 39, Batch: 601/842, Loss: 0.4082, Time: 0.77s\n",
      "Epoch: 39, Batch: 611/842, Loss: 0.4549, Time: 0.78s\n",
      "Epoch: 39, Batch: 621/842, Loss: 0.4771, Time: 0.79s\n",
      "Epoch: 39, Batch: 631/842, Loss: 0.5289, Time: 0.79s\n",
      "Epoch: 39, Batch: 641/842, Loss: 0.3594, Time: 0.77s\n",
      "Epoch: 39, Batch: 651/842, Loss: 0.7203, Time: 0.77s\n",
      "Epoch: 39, Batch: 661/842, Loss: 0.5563, Time: 0.78s\n",
      "Epoch: 39, Batch: 671/842, Loss: 0.4316, Time: 0.80s\n",
      "Epoch: 39, Batch: 681/842, Loss: 0.6536, Time: 0.80s\n",
      "Epoch: 39, Batch: 691/842, Loss: 0.4269, Time: 0.76s\n",
      "Epoch: 39, Batch: 701/842, Loss: 0.5272, Time: 0.77s\n",
      "Epoch: 39, Batch: 711/842, Loss: 0.4326, Time: 0.82s\n",
      "Epoch: 39, Batch: 721/842, Loss: 0.4709, Time: 0.80s\n",
      "Epoch: 39, Batch: 731/842, Loss: 0.5404, Time: 0.81s\n",
      "Epoch: 39, Batch: 741/842, Loss: 0.5917, Time: 0.78s\n",
      "Epoch: 39, Batch: 751/842, Loss: 0.5463, Time: 0.80s\n",
      "Epoch: 39, Batch: 761/842, Loss: 0.5579, Time: 0.88s\n",
      "Epoch: 39, Batch: 771/842, Loss: 0.3553, Time: 0.98s\n",
      "Epoch: 39, Batch: 781/842, Loss: 0.6192, Time: 0.81s\n",
      "Epoch: 39, Batch: 791/842, Loss: 0.5284, Time: 0.81s\n",
      "Epoch: 39, Batch: 801/842, Loss: 0.6200, Time: 0.78s\n",
      "Epoch: 39, Batch: 811/842, Loss: 0.4254, Time: 0.78s\n",
      "Epoch: 39, Batch: 821/842, Loss: 0.5365, Time: 0.82s\n",
      "Epoch: 39, Batch: 831/842, Loss: 0.3950, Time: 0.81s\n",
      "Epoch: 39, Batch: 841/842, Loss: 0.5371, Time: 0.80s\n",
      "Epoch 39/100: Train Loss: 0.5013, Val Loss: 0.4783, mIoU: 0.6546, F1: 0.7832, OA: 0.9611\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7609, F1=0.8642, OA=0.8596, Precision=0.8097, Recall=0.9267\n",
      "    Class 1: IoU=0.6777, F1=0.8079, OA=0.9961, Precision=0.8487, Recall=0.7708\n",
      "    Class 2: IoU=0.9068, F1=0.9511, OA=0.9802, Precision=0.9494, Recall=0.9529\n",
      "    Class 3: IoU=0.4277, F1=0.5992, OA=0.9594, Precision=0.7498, Recall=0.4990\n",
      "    Class 4: IoU=0.6977, F1=0.8219, OA=0.9778, Precision=0.9067, Recall=0.7517\n",
      "    Class 5: IoU=0.6127, F1=0.7598, OA=0.9958, Precision=0.7847, Recall=0.7365\n",
      "    Class 6: IoU=0.5363, F1=0.6981, OA=0.9548, Precision=0.8301, Recall=0.6024\n",
      "    Class 7: IoU=0.6171, F1=0.7632, OA=0.9652, Precision=0.8380, Recall=0.7007\n",
      "Epoch: 40, Batch: 1/842, Loss: 0.5243, Time: 1.45s\n",
      "Epoch: 40, Batch: 11/842, Loss: 0.4262, Time: 0.80s\n",
      "Epoch: 40, Batch: 21/842, Loss: 0.4496, Time: 0.77s\n",
      "Epoch: 40, Batch: 31/842, Loss: 0.5311, Time: 0.79s\n",
      "Epoch: 40, Batch: 41/842, Loss: 0.5858, Time: 0.78s\n",
      "Epoch: 40, Batch: 51/842, Loss: 0.6815, Time: 0.78s\n",
      "Epoch: 40, Batch: 61/842, Loss: 0.5191, Time: 0.81s\n",
      "Epoch: 40, Batch: 71/842, Loss: 0.5133, Time: 0.83s\n",
      "Epoch: 40, Batch: 81/842, Loss: 0.5221, Time: 0.83s\n",
      "Epoch: 40, Batch: 91/842, Loss: 0.4516, Time: 0.87s\n",
      "Epoch: 40, Batch: 101/842, Loss: 0.4187, Time: 0.89s\n",
      "Epoch: 40, Batch: 111/842, Loss: 0.5298, Time: 0.91s\n",
      "Epoch: 40, Batch: 121/842, Loss: 0.6551, Time: 0.85s\n",
      "Epoch: 40, Batch: 131/842, Loss: 0.4206, Time: 0.85s\n",
      "Epoch: 40, Batch: 141/842, Loss: 0.4187, Time: 0.90s\n",
      "Epoch: 40, Batch: 151/842, Loss: 0.4546, Time: 0.92s\n",
      "Epoch: 40, Batch: 161/842, Loss: 0.4750, Time: 0.91s\n",
      "Epoch: 40, Batch: 171/842, Loss: 0.5158, Time: 0.90s\n",
      "Epoch: 40, Batch: 181/842, Loss: 0.4921, Time: 0.89s\n",
      "Epoch: 40, Batch: 191/842, Loss: 0.3737, Time: 0.81s\n",
      "Epoch: 40, Batch: 201/842, Loss: 0.4319, Time: 0.81s\n",
      "Epoch: 40, Batch: 211/842, Loss: 0.4925, Time: 0.79s\n",
      "Epoch: 40, Batch: 221/842, Loss: 0.5121, Time: 0.82s\n",
      "Epoch: 40, Batch: 231/842, Loss: 0.5725, Time: 0.81s\n",
      "Epoch: 40, Batch: 241/842, Loss: 0.5509, Time: 0.82s\n",
      "Epoch: 40, Batch: 251/842, Loss: 0.3650, Time: 0.81s\n",
      "Epoch: 40, Batch: 261/842, Loss: 0.5680, Time: 0.80s\n",
      "Epoch: 40, Batch: 271/842, Loss: 0.5554, Time: 0.82s\n",
      "Epoch: 40, Batch: 281/842, Loss: 0.6061, Time: 0.83s\n",
      "Epoch: 40, Batch: 291/842, Loss: 0.4559, Time: 0.79s\n",
      "Epoch: 40, Batch: 301/842, Loss: 0.5269, Time: 0.78s\n",
      "Epoch: 40, Batch: 311/842, Loss: 0.5275, Time: 0.75s\n",
      "Epoch: 40, Batch: 321/842, Loss: 0.4565, Time: 0.76s\n",
      "Epoch: 40, Batch: 331/842, Loss: 0.6203, Time: 0.77s\n",
      "Epoch: 40, Batch: 341/842, Loss: 0.4284, Time: 0.77s\n",
      "Epoch: 40, Batch: 351/842, Loss: 0.6769, Time: 0.74s\n",
      "Epoch: 40, Batch: 361/842, Loss: 0.3646, Time: 0.74s\n",
      "Epoch: 40, Batch: 371/842, Loss: 0.5335, Time: 0.77s\n",
      "Epoch: 40, Batch: 381/842, Loss: 0.5363, Time: 0.74s\n",
      "Epoch: 40, Batch: 391/842, Loss: 0.5556, Time: 0.74s\n",
      "Epoch: 40, Batch: 401/842, Loss: 0.5157, Time: 0.75s\n",
      "Epoch: 40, Batch: 411/842, Loss: 0.3438, Time: 0.74s\n",
      "Epoch: 40, Batch: 421/842, Loss: 0.5084, Time: 0.72s\n",
      "Epoch: 40, Batch: 431/842, Loss: 0.5457, Time: 0.72s\n",
      "Epoch: 40, Batch: 441/842, Loss: 0.5812, Time: 0.72s\n",
      "Epoch: 40, Batch: 451/842, Loss: 0.3992, Time: 0.73s\n",
      "Epoch: 40, Batch: 461/842, Loss: 0.4049, Time: 0.72s\n",
      "Epoch: 40, Batch: 471/842, Loss: 0.5353, Time: 0.72s\n",
      "Epoch: 40, Batch: 481/842, Loss: 0.6753, Time: 0.73s\n",
      "Epoch: 40, Batch: 491/842, Loss: 0.5569, Time: 0.72s\n",
      "Epoch: 40, Batch: 501/842, Loss: 0.5079, Time: 0.71s\n",
      "Epoch: 40, Batch: 511/842, Loss: 0.3896, Time: 0.74s\n",
      "Epoch: 40, Batch: 521/842, Loss: 0.4912, Time: 0.73s\n",
      "Epoch: 40, Batch: 531/842, Loss: 0.4708, Time: 0.75s\n",
      "Epoch: 40, Batch: 541/842, Loss: 0.6252, Time: 0.73s\n",
      "Epoch: 40, Batch: 551/842, Loss: 0.5064, Time: 0.72s\n",
      "Epoch: 40, Batch: 561/842, Loss: 0.4512, Time: 0.73s\n",
      "Epoch: 40, Batch: 571/842, Loss: 0.4518, Time: 0.74s\n",
      "Epoch: 40, Batch: 581/842, Loss: 0.4101, Time: 0.75s\n",
      "Epoch: 40, Batch: 591/842, Loss: 0.5887, Time: 0.73s\n",
      "Epoch: 40, Batch: 601/842, Loss: 0.4674, Time: 0.74s\n",
      "Epoch: 40, Batch: 611/842, Loss: 0.4327, Time: 0.78s\n",
      "Epoch: 40, Batch: 621/842, Loss: 0.4402, Time: 0.77s\n",
      "Epoch: 40, Batch: 631/842, Loss: 0.4444, Time: 0.74s\n",
      "Epoch: 40, Batch: 641/842, Loss: 0.4751, Time: 0.74s\n",
      "Epoch: 40, Batch: 651/842, Loss: 0.4487, Time: 0.75s\n",
      "Epoch: 40, Batch: 661/842, Loss: 0.6043, Time: 0.74s\n",
      "Epoch: 40, Batch: 671/842, Loss: 0.4358, Time: 0.74s\n",
      "Epoch: 40, Batch: 681/842, Loss: 0.6251, Time: 0.74s\n",
      "Epoch: 40, Batch: 691/842, Loss: 0.5948, Time: 0.73s\n",
      "Epoch: 40, Batch: 701/842, Loss: 0.4945, Time: 0.73s\n",
      "Epoch: 40, Batch: 711/842, Loss: 0.6281, Time: 0.73s\n",
      "Epoch: 40, Batch: 721/842, Loss: 0.4023, Time: 0.74s\n",
      "Epoch: 40, Batch: 731/842, Loss: 0.5950, Time: 0.73s\n",
      "Epoch: 40, Batch: 741/842, Loss: 0.4716, Time: 0.73s\n",
      "Epoch: 40, Batch: 751/842, Loss: 0.4990, Time: 0.73s\n",
      "Epoch: 40, Batch: 761/842, Loss: 0.4429, Time: 0.75s\n",
      "Epoch: 40, Batch: 771/842, Loss: 0.4318, Time: 0.74s\n",
      "Epoch: 40, Batch: 781/842, Loss: 0.5911, Time: 0.74s\n",
      "Epoch: 40, Batch: 791/842, Loss: 0.3926, Time: 0.73s\n",
      "Epoch: 40, Batch: 801/842, Loss: 0.5462, Time: 0.72s\n",
      "Epoch: 40, Batch: 811/842, Loss: 0.5706, Time: 0.72s\n",
      "Epoch: 40, Batch: 821/842, Loss: 0.6881, Time: 0.74s\n",
      "Epoch: 40, Batch: 831/842, Loss: 0.5220, Time: 0.72s\n",
      "Epoch: 40, Batch: 841/842, Loss: 0.5160, Time: 0.67s\n",
      "Epoch 40/100: Train Loss: 0.5022, Val Loss: 0.4698, mIoU: 0.6472, F1: 0.7776, OA: 0.9617\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7653, F1=0.8670, OA=0.8642, Precision=0.8209, Recall=0.9186\n",
      "    Class 1: IoU=0.6366, F1=0.7780, OA=0.9952, Precision=0.7583, Recall=0.7987\n",
      "    Class 2: IoU=0.9029, F1=0.9489, OA=0.9796, Precision=0.9623, Recall=0.9359\n",
      "    Class 3: IoU=0.4512, F1=0.6218, OA=0.9601, Precision=0.7340, Recall=0.5394\n",
      "    Class 4: IoU=0.7255, F1=0.8409, OA=0.9791, Precision=0.8736, Recall=0.8105\n",
      "    Class 5: IoU=0.5184, F1=0.6828, OA=0.9933, Precision=0.5892, Recall=0.8119\n",
      "    Class 6: IoU=0.5533, F1=0.7125, OA=0.9560, Precision=0.8229, Recall=0.6282\n",
      "    Class 7: IoU=0.6242, F1=0.7686, OA=0.9663, Precision=0.8515, Recall=0.7004\n",
      "Epoch: 41, Batch: 1/842, Loss: 0.4302, Time: 1.55s\n",
      "Epoch: 41, Batch: 11/842, Loss: 0.4738, Time: 0.73s\n",
      "Epoch: 41, Batch: 21/842, Loss: 0.4077, Time: 0.72s\n",
      "Epoch: 41, Batch: 31/842, Loss: 0.2202, Time: 0.72s\n",
      "Epoch: 41, Batch: 41/842, Loss: 0.6380, Time: 0.69s\n",
      "Epoch: 41, Batch: 51/842, Loss: 0.3214, Time: 0.73s\n",
      "Epoch: 41, Batch: 61/842, Loss: 0.5017, Time: 0.72s\n",
      "Epoch: 41, Batch: 71/842, Loss: 0.4296, Time: 0.71s\n",
      "Epoch: 41, Batch: 81/842, Loss: 0.4681, Time: 0.71s\n",
      "Epoch: 41, Batch: 91/842, Loss: 0.4002, Time: 0.71s\n",
      "Epoch: 41, Batch: 101/842, Loss: 0.6191, Time: 0.71s\n",
      "Epoch: 41, Batch: 111/842, Loss: 0.6010, Time: 0.71s\n",
      "Epoch: 41, Batch: 121/842, Loss: 0.5386, Time: 0.71s\n",
      "Epoch: 41, Batch: 131/842, Loss: 0.4690, Time: 0.72s\n",
      "Epoch: 41, Batch: 141/842, Loss: 0.5334, Time: 0.72s\n",
      "Epoch: 41, Batch: 151/842, Loss: 0.5585, Time: 0.72s\n",
      "Epoch: 41, Batch: 161/842, Loss: 0.4418, Time: 0.73s\n",
      "Epoch: 41, Batch: 171/842, Loss: 0.3592, Time: 0.73s\n",
      "Epoch: 41, Batch: 181/842, Loss: 0.5850, Time: 0.73s\n",
      "Epoch: 41, Batch: 191/842, Loss: 0.5157, Time: 0.72s\n",
      "Epoch: 41, Batch: 201/842, Loss: 0.4215, Time: 0.73s\n",
      "Epoch: 41, Batch: 211/842, Loss: 0.5252, Time: 0.74s\n",
      "Epoch: 41, Batch: 221/842, Loss: 0.3967, Time: 0.72s\n",
      "Epoch: 41, Batch: 231/842, Loss: 0.5548, Time: 0.73s\n",
      "Epoch: 41, Batch: 241/842, Loss: 0.3383, Time: 0.72s\n",
      "Epoch: 41, Batch: 251/842, Loss: 0.4754, Time: 0.72s\n",
      "Epoch: 41, Batch: 261/842, Loss: 0.3690, Time: 0.72s\n",
      "Epoch: 41, Batch: 271/842, Loss: 0.6968, Time: 0.73s\n",
      "Epoch: 41, Batch: 281/842, Loss: 0.6268, Time: 0.72s\n",
      "Epoch: 41, Batch: 291/842, Loss: 0.4890, Time: 0.73s\n",
      "Epoch: 41, Batch: 301/842, Loss: 0.3078, Time: 0.73s\n",
      "Epoch: 41, Batch: 311/842, Loss: 0.4071, Time: 0.78s\n",
      "Epoch: 41, Batch: 321/842, Loss: 0.3948, Time: 0.75s\n",
      "Epoch: 41, Batch: 331/842, Loss: 0.3904, Time: 0.74s\n",
      "Epoch: 41, Batch: 341/842, Loss: 0.5093, Time: 0.73s\n",
      "Epoch: 41, Batch: 351/842, Loss: 0.4567, Time: 0.73s\n",
      "Epoch: 41, Batch: 361/842, Loss: 0.4505, Time: 0.73s\n",
      "Epoch: 41, Batch: 371/842, Loss: 0.6193, Time: 0.73s\n",
      "Epoch: 41, Batch: 381/842, Loss: 0.3862, Time: 0.72s\n",
      "Epoch: 41, Batch: 391/842, Loss: 0.3838, Time: 0.73s\n",
      "Epoch: 41, Batch: 401/842, Loss: 0.4603, Time: 0.73s\n",
      "Epoch: 41, Batch: 411/842, Loss: 0.4196, Time: 0.72s\n",
      "Epoch: 41, Batch: 421/842, Loss: 0.6383, Time: 0.73s\n",
      "Epoch: 41, Batch: 431/842, Loss: 0.4688, Time: 0.72s\n",
      "Epoch: 41, Batch: 441/842, Loss: 0.4819, Time: 0.72s\n",
      "Epoch: 41, Batch: 451/842, Loss: 0.3864, Time: 0.72s\n",
      "Epoch: 41, Batch: 461/842, Loss: 0.3753, Time: 0.73s\n",
      "Epoch: 41, Batch: 471/842, Loss: 0.3806, Time: 0.75s\n",
      "Epoch: 41, Batch: 481/842, Loss: 0.4672, Time: 0.75s\n",
      "Epoch: 41, Batch: 491/842, Loss: 0.4838, Time: 0.76s\n",
      "Epoch: 41, Batch: 501/842, Loss: 0.4908, Time: 0.77s\n",
      "Epoch: 41, Batch: 511/842, Loss: 0.2909, Time: 0.75s\n",
      "Epoch: 41, Batch: 521/842, Loss: 0.2453, Time: 0.75s\n",
      "Epoch: 41, Batch: 531/842, Loss: 0.3997, Time: 0.75s\n",
      "Epoch: 41, Batch: 541/842, Loss: 0.4650, Time: 0.77s\n",
      "Epoch: 41, Batch: 551/842, Loss: 0.5735, Time: 0.75s\n",
      "Epoch: 41, Batch: 561/842, Loss: 0.3848, Time: 0.75s\n",
      "Epoch: 41, Batch: 571/842, Loss: 0.3644, Time: 0.74s\n",
      "Epoch: 41, Batch: 581/842, Loss: 0.4354, Time: 0.76s\n",
      "Epoch: 41, Batch: 591/842, Loss: 0.5137, Time: 0.76s\n",
      "Epoch: 41, Batch: 601/842, Loss: 0.4454, Time: 0.76s\n",
      "Epoch: 41, Batch: 611/842, Loss: 0.4441, Time: 0.75s\n",
      "Epoch: 41, Batch: 621/842, Loss: 0.5207, Time: 0.77s\n",
      "Epoch: 41, Batch: 631/842, Loss: 0.4949, Time: 0.74s\n",
      "Epoch: 41, Batch: 641/842, Loss: 0.4549, Time: 0.74s\n",
      "Epoch: 41, Batch: 651/842, Loss: 0.5385, Time: 0.74s\n",
      "Epoch: 41, Batch: 661/842, Loss: 0.4775, Time: 0.75s\n",
      "Epoch: 41, Batch: 671/842, Loss: 0.5334, Time: 0.74s\n",
      "Epoch: 41, Batch: 681/842, Loss: 0.4266, Time: 0.74s\n",
      "Epoch: 41, Batch: 691/842, Loss: 0.5576, Time: 0.75s\n",
      "Epoch: 41, Batch: 701/842, Loss: 0.5064, Time: 0.74s\n",
      "Epoch: 41, Batch: 711/842, Loss: 0.5003, Time: 0.73s\n",
      "Epoch: 41, Batch: 721/842, Loss: 0.4130, Time: 0.73s\n",
      "Epoch: 41, Batch: 731/842, Loss: 0.4837, Time: 0.72s\n",
      "Epoch: 41, Batch: 741/842, Loss: 0.4894, Time: 0.73s\n",
      "Epoch: 41, Batch: 751/842, Loss: 0.4178, Time: 0.73s\n",
      "Epoch: 41, Batch: 761/842, Loss: 0.7454, Time: 0.73s\n",
      "Epoch: 41, Batch: 771/842, Loss: 0.4797, Time: 0.72s\n",
      "Epoch: 41, Batch: 781/842, Loss: 0.4497, Time: 0.72s\n",
      "Epoch: 41, Batch: 791/842, Loss: 0.5067, Time: 0.73s\n",
      "Epoch: 41, Batch: 801/842, Loss: 0.4720, Time: 0.73s\n",
      "Epoch: 41, Batch: 811/842, Loss: 0.5177, Time: 0.74s\n",
      "Epoch: 41, Batch: 821/842, Loss: 0.5022, Time: 1.29s\n",
      "Epoch: 41, Batch: 831/842, Loss: 0.4033, Time: 0.81s\n",
      "Epoch: 41, Batch: 841/842, Loss: 0.4295, Time: 0.69s\n",
      "Epoch 41/100: Train Loss: 0.4815, Val Loss: 0.4457, mIoU: 0.6809, F1: 0.8033, OA: 0.9638\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7735, F1=0.8723, OA=0.8688, Precision=0.8216, Recall=0.9297\n",
      "    Class 1: IoU=0.7012, F1=0.8244, OA=0.9965, Precision=0.8771, Recall=0.7776\n",
      "    Class 2: IoU=0.9084, F1=0.9520, OA=0.9805, Precision=0.9483, Recall=0.9558\n",
      "    Class 3: IoU=0.4608, F1=0.6309, OA=0.9630, Precision=0.8003, Recall=0.5207\n",
      "    Class 4: IoU=0.7370, F1=0.8486, OA=0.9803, Precision=0.8888, Recall=0.8119\n",
      "    Class 5: IoU=0.6670, F1=0.8002, OA=0.9966, Precision=0.8409, Recall=0.7633\n",
      "    Class 6: IoU=0.5612, F1=0.7189, OA=0.9576, Precision=0.8453, Recall=0.6254\n",
      "    Class 7: IoU=0.6379, F1=0.7789, OA=0.9676, Precision=0.8571, Recall=0.7138\n",
      "Saving best model with mIoU: 0.6809\n",
      "Epoch: 42, Batch: 1/842, Loss: 0.5411, Time: 1.64s\n",
      "Epoch: 42, Batch: 11/842, Loss: 0.5001, Time: 0.72s\n",
      "Epoch: 42, Batch: 21/842, Loss: 0.5965, Time: 0.72s\n",
      "Epoch: 42, Batch: 31/842, Loss: 0.5421, Time: 0.73s\n",
      "Epoch: 42, Batch: 41/842, Loss: 0.4480, Time: 0.73s\n",
      "Epoch: 42, Batch: 51/842, Loss: 0.4426, Time: 0.72s\n",
      "Epoch: 42, Batch: 61/842, Loss: 0.4586, Time: 0.72s\n",
      "Epoch: 42, Batch: 71/842, Loss: 0.5049, Time: 0.72s\n",
      "Epoch: 42, Batch: 81/842, Loss: 0.4885, Time: 0.73s\n",
      "Epoch: 42, Batch: 91/842, Loss: 0.5512, Time: 0.72s\n",
      "Epoch: 42, Batch: 101/842, Loss: 0.4880, Time: 0.72s\n",
      "Epoch: 42, Batch: 111/842, Loss: 0.5649, Time: 0.72s\n",
      "Epoch: 42, Batch: 121/842, Loss: 0.5452, Time: 0.73s\n",
      "Epoch: 42, Batch: 131/842, Loss: 0.4969, Time: 0.72s\n",
      "Epoch: 42, Batch: 141/842, Loss: 0.5741, Time: 0.74s\n",
      "Epoch: 42, Batch: 151/842, Loss: 0.3220, Time: 0.72s\n",
      "Epoch: 42, Batch: 161/842, Loss: 0.4047, Time: 0.72s\n",
      "Epoch: 42, Batch: 171/842, Loss: 0.4420, Time: 0.74s\n",
      "Epoch: 42, Batch: 181/842, Loss: 0.4252, Time: 0.76s\n",
      "Epoch: 42, Batch: 191/842, Loss: 0.4911, Time: 0.77s\n",
      "Epoch: 42, Batch: 201/842, Loss: 0.5532, Time: 0.77s\n",
      "Epoch: 42, Batch: 211/842, Loss: 0.5271, Time: 0.79s\n",
      "Epoch: 42, Batch: 221/842, Loss: 0.4701, Time: 0.76s\n",
      "Epoch: 42, Batch: 231/842, Loss: 0.5151, Time: 0.75s\n",
      "Epoch: 42, Batch: 241/842, Loss: 0.5135, Time: 0.77s\n",
      "Epoch: 42, Batch: 251/842, Loss: 0.5466, Time: 0.76s\n",
      "Epoch: 42, Batch: 261/842, Loss: 0.4885, Time: 0.77s\n",
      "Epoch: 42, Batch: 271/842, Loss: 0.4430, Time: 0.74s\n",
      "Epoch: 42, Batch: 281/842, Loss: 0.5973, Time: 0.73s\n",
      "Epoch: 42, Batch: 291/842, Loss: 0.5528, Time: 0.74s\n",
      "Epoch: 42, Batch: 301/842, Loss: 0.4094, Time: 0.73s\n",
      "Epoch: 42, Batch: 311/842, Loss: 0.5060, Time: 0.73s\n",
      "Epoch: 42, Batch: 321/842, Loss: 0.5393, Time: 0.73s\n",
      "Epoch: 42, Batch: 331/842, Loss: 0.5663, Time: 0.72s\n",
      "Epoch: 42, Batch: 341/842, Loss: 0.5495, Time: 0.73s\n",
      "Epoch: 42, Batch: 351/842, Loss: 0.4466, Time: 0.73s\n",
      "Epoch: 42, Batch: 361/842, Loss: 0.5586, Time: 0.73s\n",
      "Epoch: 42, Batch: 371/842, Loss: 0.3069, Time: 0.74s\n",
      "Epoch: 42, Batch: 381/842, Loss: 0.4209, Time: 0.72s\n",
      "Epoch: 42, Batch: 391/842, Loss: 0.5134, Time: 0.76s\n",
      "Epoch: 42, Batch: 401/842, Loss: 0.3081, Time: 0.79s\n",
      "Epoch: 42, Batch: 411/842, Loss: 0.4804, Time: 0.81s\n",
      "Epoch: 42, Batch: 421/842, Loss: 0.5176, Time: 0.80s\n",
      "Epoch: 42, Batch: 431/842, Loss: 0.4895, Time: 0.77s\n",
      "Epoch: 42, Batch: 441/842, Loss: 0.4040, Time: 0.75s\n",
      "Epoch: 42, Batch: 451/842, Loss: 0.5137, Time: 0.76s\n",
      "Epoch: 42, Batch: 461/842, Loss: 0.5502, Time: 0.75s\n",
      "Epoch: 42, Batch: 471/842, Loss: 0.3472, Time: 0.75s\n",
      "Epoch: 42, Batch: 481/842, Loss: 0.2309, Time: 0.74s\n",
      "Epoch: 42, Batch: 491/842, Loss: 0.4863, Time: 0.75s\n",
      "Epoch: 42, Batch: 501/842, Loss: 0.4871, Time: 0.75s\n",
      "Epoch: 42, Batch: 511/842, Loss: 0.3962, Time: 0.74s\n",
      "Epoch: 42, Batch: 521/842, Loss: 0.4855, Time: 0.73s\n",
      "Epoch: 42, Batch: 531/842, Loss: 0.4729, Time: 0.74s\n",
      "Epoch: 42, Batch: 541/842, Loss: 0.3955, Time: 0.72s\n",
      "Epoch: 42, Batch: 551/842, Loss: 0.3762, Time: 0.73s\n",
      "Epoch: 42, Batch: 561/842, Loss: 0.5148, Time: 0.73s\n",
      "Epoch: 42, Batch: 571/842, Loss: 0.4852, Time: 0.72s\n",
      "Epoch: 42, Batch: 581/842, Loss: 0.5642, Time: 0.72s\n",
      "Epoch: 42, Batch: 591/842, Loss: 0.4958, Time: 0.73s\n",
      "Epoch: 42, Batch: 601/842, Loss: 0.2677, Time: 0.71s\n",
      "Epoch: 42, Batch: 611/842, Loss: 0.5147, Time: 0.72s\n",
      "Epoch: 42, Batch: 621/842, Loss: 0.5335, Time: 0.73s\n",
      "Epoch: 42, Batch: 631/842, Loss: 0.3903, Time: 0.71s\n",
      "Epoch: 42, Batch: 641/842, Loss: 0.5148, Time: 0.74s\n",
      "Epoch: 42, Batch: 651/842, Loss: 0.5215, Time: 0.72s\n",
      "Epoch: 42, Batch: 661/842, Loss: 0.5809, Time: 0.72s\n",
      "Epoch: 42, Batch: 671/842, Loss: 0.3215, Time: 0.72s\n",
      "Epoch: 42, Batch: 681/842, Loss: 0.4725, Time: 0.71s\n",
      "Epoch: 42, Batch: 691/842, Loss: 0.4647, Time: 0.72s\n",
      "Epoch: 42, Batch: 701/842, Loss: 0.4444, Time: 0.71s\n",
      "Epoch: 42, Batch: 711/842, Loss: 0.5059, Time: 0.72s\n",
      "Epoch: 42, Batch: 721/842, Loss: 0.4980, Time: 0.72s\n",
      "Epoch: 42, Batch: 731/842, Loss: 0.5190, Time: 0.71s\n",
      "Epoch: 42, Batch: 741/842, Loss: 0.5215, Time: 0.71s\n",
      "Epoch: 42, Batch: 751/842, Loss: 0.4172, Time: 0.71s\n",
      "Epoch: 42, Batch: 761/842, Loss: 0.4544, Time: 0.73s\n",
      "Epoch: 42, Batch: 771/842, Loss: 0.4660, Time: 0.73s\n",
      "Epoch: 42, Batch: 781/842, Loss: 0.4029, Time: 0.72s\n",
      "Epoch: 42, Batch: 791/842, Loss: 0.5601, Time: 0.72s\n",
      "Epoch: 42, Batch: 801/842, Loss: 0.4179, Time: 0.72s\n",
      "Epoch: 42, Batch: 811/842, Loss: 0.5601, Time: 0.73s\n",
      "Epoch: 42, Batch: 821/842, Loss: 0.4394, Time: 0.72s\n",
      "Epoch: 42, Batch: 831/842, Loss: 0.4973, Time: 0.72s\n",
      "Epoch: 42, Batch: 841/842, Loss: 0.5210, Time: 0.72s\n",
      "Epoch 42/100: Train Loss: 0.4785, Val Loss: 0.4498, mIoU: 0.6761, F1: 0.7998, OA: 0.9634\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7724, F1=0.8716, OA=0.8679, Precision=0.8202, Recall=0.9299\n",
      "    Class 1: IoU=0.6901, F1=0.8166, OA=0.9963, Precision=0.8686, Recall=0.7705\n",
      "    Class 2: IoU=0.9091, F1=0.9524, OA=0.9808, Precision=0.9585, Recall=0.9464\n",
      "    Class 3: IoU=0.4652, F1=0.6350, OA=0.9628, Precision=0.7871, Recall=0.5321\n",
      "    Class 4: IoU=0.7398, F1=0.8505, OA=0.9803, Precision=0.8824, Recall=0.8208\n",
      "    Class 5: IoU=0.6536, F1=0.7905, OA=0.9963, Precision=0.7931, Recall=0.7880\n",
      "    Class 6: IoU=0.5604, F1=0.7183, OA=0.9561, Precision=0.8089, Recall=0.6459\n",
      "    Class 7: IoU=0.6180, F1=0.7639, OA=0.9666, Precision=0.8806, Recall=0.6745\n",
      "Epoch: 43, Batch: 1/842, Loss: 0.3795, Time: 1.72s\n",
      "Epoch: 43, Batch: 11/842, Loss: 0.5815, Time: 0.79s\n",
      "Epoch: 43, Batch: 21/842, Loss: 0.5748, Time: 0.76s\n",
      "Epoch: 43, Batch: 31/842, Loss: 0.4176, Time: 0.82s\n",
      "Epoch: 43, Batch: 41/842, Loss: 0.5121, Time: 0.79s\n",
      "Epoch: 43, Batch: 51/842, Loss: 0.4810, Time: 0.80s\n",
      "Epoch: 43, Batch: 61/842, Loss: 0.6257, Time: 0.77s\n",
      "Epoch: 43, Batch: 71/842, Loss: 0.6587, Time: 0.78s\n",
      "Epoch: 43, Batch: 81/842, Loss: 0.4476, Time: 0.81s\n",
      "Epoch: 43, Batch: 91/842, Loss: 0.6614, Time: 0.78s\n",
      "Epoch: 43, Batch: 101/842, Loss: 0.5586, Time: 0.79s\n",
      "Epoch: 43, Batch: 111/842, Loss: 0.4522, Time: 0.78s\n",
      "Epoch: 43, Batch: 121/842, Loss: 0.4509, Time: 0.80s\n",
      "Epoch: 43, Batch: 131/842, Loss: 0.3745, Time: 0.81s\n",
      "Epoch: 43, Batch: 141/842, Loss: 0.4592, Time: 0.78s\n",
      "Epoch: 43, Batch: 151/842, Loss: 0.4134, Time: 0.79s\n",
      "Epoch: 43, Batch: 161/842, Loss: 0.4392, Time: 0.79s\n",
      "Epoch: 43, Batch: 171/842, Loss: 0.4647, Time: 0.79s\n",
      "Epoch: 43, Batch: 181/842, Loss: 0.4148, Time: 0.77s\n",
      "Epoch: 43, Batch: 191/842, Loss: 0.5134, Time: 0.77s\n",
      "Epoch: 43, Batch: 201/842, Loss: 0.3201, Time: 0.78s\n",
      "Epoch: 43, Batch: 211/842, Loss: 0.4705, Time: 0.78s\n",
      "Epoch: 43, Batch: 221/842, Loss: 0.4180, Time: 0.78s\n",
      "Epoch: 43, Batch: 231/842, Loss: 0.5617, Time: 0.79s\n",
      "Epoch: 43, Batch: 241/842, Loss: 0.5301, Time: 0.79s\n",
      "Epoch: 43, Batch: 251/842, Loss: 0.4008, Time: 0.80s\n",
      "Epoch: 43, Batch: 261/842, Loss: 0.4525, Time: 0.81s\n",
      "Epoch: 43, Batch: 271/842, Loss: 0.4338, Time: 0.79s\n",
      "Epoch: 43, Batch: 281/842, Loss: 0.4380, Time: 0.81s\n",
      "Epoch: 43, Batch: 291/842, Loss: 0.3501, Time: 0.81s\n",
      "Epoch: 43, Batch: 301/842, Loss: 0.3931, Time: 0.79s\n",
      "Epoch: 43, Batch: 311/842, Loss: 0.5377, Time: 0.80s\n",
      "Epoch: 43, Batch: 321/842, Loss: 0.4771, Time: 0.81s\n",
      "Epoch: 43, Batch: 331/842, Loss: 0.5220, Time: 0.80s\n",
      "Epoch: 43, Batch: 341/842, Loss: 0.6351, Time: 0.79s\n",
      "Epoch: 43, Batch: 351/842, Loss: 0.6126, Time: 0.80s\n",
      "Epoch: 43, Batch: 361/842, Loss: 0.5023, Time: 0.78s\n",
      "Epoch: 43, Batch: 371/842, Loss: 0.3784, Time: 0.81s\n",
      "Epoch: 43, Batch: 381/842, Loss: 0.3745, Time: 0.79s\n",
      "Epoch: 43, Batch: 391/842, Loss: 0.5387, Time: 0.81s\n",
      "Epoch: 43, Batch: 401/842, Loss: 0.4879, Time: 0.79s\n",
      "Epoch: 43, Batch: 411/842, Loss: 0.3893, Time: 0.79s\n",
      "Epoch: 43, Batch: 421/842, Loss: 0.4294, Time: 0.79s\n",
      "Epoch: 43, Batch: 431/842, Loss: 0.4477, Time: 0.80s\n",
      "Epoch: 43, Batch: 441/842, Loss: 0.6016, Time: 0.81s\n",
      "Epoch: 43, Batch: 451/842, Loss: 0.5030, Time: 0.79s\n",
      "Epoch: 43, Batch: 461/842, Loss: 0.3916, Time: 0.79s\n",
      "Epoch: 43, Batch: 471/842, Loss: 0.5177, Time: 0.81s\n",
      "Epoch: 43, Batch: 481/842, Loss: 0.5339, Time: 0.81s\n",
      "Epoch: 43, Batch: 491/842, Loss: 0.4422, Time: 0.79s\n",
      "Epoch: 43, Batch: 501/842, Loss: 0.2712, Time: 0.79s\n",
      "Epoch: 43, Batch: 511/842, Loss: 0.6043, Time: 0.80s\n",
      "Epoch: 43, Batch: 521/842, Loss: 0.4932, Time: 0.81s\n",
      "Epoch: 43, Batch: 531/842, Loss: 0.4736, Time: 0.79s\n",
      "Epoch: 43, Batch: 541/842, Loss: 0.4268, Time: 0.79s\n",
      "Epoch: 43, Batch: 551/842, Loss: 0.4909, Time: 0.79s\n",
      "Epoch: 43, Batch: 561/842, Loss: 0.5723, Time: 0.80s\n",
      "Epoch: 43, Batch: 571/842, Loss: 0.5803, Time: 0.79s\n",
      "Epoch: 43, Batch: 581/842, Loss: 0.5313, Time: 0.79s\n",
      "Epoch: 43, Batch: 591/842, Loss: 0.4665, Time: 0.80s\n",
      "Epoch: 43, Batch: 601/842, Loss: 0.4796, Time: 0.79s\n",
      "Epoch: 43, Batch: 611/842, Loss: 0.3674, Time: 0.81s\n",
      "Epoch: 43, Batch: 621/842, Loss: 0.4570, Time: 0.81s\n",
      "Epoch: 43, Batch: 631/842, Loss: 0.4645, Time: 0.80s\n",
      "Epoch: 43, Batch: 641/842, Loss: 0.5134, Time: 0.80s\n",
      "Epoch: 43, Batch: 651/842, Loss: 0.4732, Time: 0.78s\n",
      "Epoch: 43, Batch: 661/842, Loss: 0.3043, Time: 0.79s\n",
      "Epoch: 43, Batch: 671/842, Loss: 0.3382, Time: 0.82s\n",
      "Epoch: 43, Batch: 681/842, Loss: 0.4663, Time: 0.81s\n",
      "Epoch: 43, Batch: 691/842, Loss: 0.4716, Time: 0.81s\n",
      "Epoch: 43, Batch: 701/842, Loss: 0.3561, Time: 0.82s\n",
      "Epoch: 43, Batch: 711/842, Loss: 0.3683, Time: 0.77s\n",
      "Epoch: 43, Batch: 721/842, Loss: 0.3945, Time: 0.78s\n",
      "Epoch: 43, Batch: 731/842, Loss: 0.5568, Time: 0.78s\n",
      "Epoch: 43, Batch: 741/842, Loss: 0.3851, Time: 0.78s\n",
      "Epoch: 43, Batch: 751/842, Loss: 0.4010, Time: 0.78s\n",
      "Epoch: 43, Batch: 761/842, Loss: 0.4678, Time: 0.78s\n",
      "Epoch: 43, Batch: 771/842, Loss: 0.5504, Time: 0.77s\n",
      "Epoch: 43, Batch: 781/842, Loss: 0.3898, Time: 0.78s\n",
      "Epoch: 43, Batch: 791/842, Loss: 0.5945, Time: 0.78s\n",
      "Epoch: 43, Batch: 801/842, Loss: 0.4682, Time: 0.78s\n",
      "Epoch: 43, Batch: 811/842, Loss: 0.4986, Time: 0.80s\n",
      "Epoch: 43, Batch: 821/842, Loss: 0.5406, Time: 0.81s\n",
      "Epoch: 43, Batch: 831/842, Loss: 0.5641, Time: 0.84s\n",
      "Epoch: 43, Batch: 841/842, Loss: 0.4660, Time: 0.82s\n",
      "Epoch 43/100: Train Loss: 0.4785, Val Loss: 0.4500, mIoU: 0.6781, F1: 0.8012, OA: 0.9635\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7719, F1=0.8712, OA=0.8687, Precision=0.8265, Recall=0.9211\n",
      "    Class 1: IoU=0.6896, F1=0.8163, OA=0.9963, Precision=0.8541, Recall=0.7816\n",
      "    Class 2: IoU=0.9076, F1=0.9515, OA=0.9803, Precision=0.9502, Recall=0.9529\n",
      "    Class 3: IoU=0.4533, F1=0.6238, OA=0.9636, Precision=0.8394, Recall=0.4963\n",
      "    Class 4: IoU=0.7354, F1=0.8475, OA=0.9802, Precision=0.8929, Recall=0.8065\n",
      "    Class 5: IoU=0.6677, F1=0.8007, OA=0.9965, Precision=0.8127, Recall=0.7891\n",
      "    Class 6: IoU=0.5608, F1=0.7186, OA=0.9566, Precision=0.8200, Recall=0.6395\n",
      "    Class 7: IoU=0.6387, F1=0.7795, OA=0.9656, Precision=0.8002, Recall=0.7598\n",
      "Epoch: 44, Batch: 1/842, Loss: 0.4122, Time: 1.89s\n",
      "Epoch: 44, Batch: 11/842, Loss: 0.3973, Time: 0.79s\n",
      "Epoch: 44, Batch: 21/842, Loss: 0.4435, Time: 0.77s\n",
      "Epoch: 44, Batch: 31/842, Loss: 0.3595, Time: 0.79s\n",
      "Epoch: 44, Batch: 41/842, Loss: 0.3511, Time: 0.84s\n",
      "Epoch: 44, Batch: 51/842, Loss: 0.4721, Time: 0.85s\n",
      "Epoch: 44, Batch: 61/842, Loss: 0.4859, Time: 0.82s\n",
      "Epoch: 44, Batch: 71/842, Loss: 0.5666, Time: 0.85s\n",
      "Epoch: 44, Batch: 81/842, Loss: 0.5235, Time: 0.88s\n",
      "Epoch: 44, Batch: 91/842, Loss: 0.4926, Time: 0.85s\n",
      "Epoch: 44, Batch: 101/842, Loss: 0.3138, Time: 0.83s\n",
      "Epoch: 44, Batch: 111/842, Loss: 0.3364, Time: 0.80s\n",
      "Epoch: 44, Batch: 121/842, Loss: 0.4138, Time: 0.85s\n",
      "Epoch: 44, Batch: 131/842, Loss: 0.4697, Time: 0.84s\n",
      "Epoch: 44, Batch: 141/842, Loss: 0.5002, Time: 0.82s\n",
      "Epoch: 44, Batch: 151/842, Loss: 0.3666, Time: 0.80s\n",
      "Epoch: 44, Batch: 161/842, Loss: 0.5422, Time: 0.80s\n",
      "Epoch: 44, Batch: 171/842, Loss: 0.5437, Time: 0.78s\n",
      "Epoch: 44, Batch: 181/842, Loss: 0.4564, Time: 0.79s\n",
      "Epoch: 44, Batch: 191/842, Loss: 0.5375, Time: 0.79s\n",
      "Epoch: 44, Batch: 201/842, Loss: 0.4331, Time: 0.79s\n",
      "Epoch: 44, Batch: 211/842, Loss: 0.4199, Time: 0.75s\n",
      "Epoch: 44, Batch: 221/842, Loss: 0.4181, Time: 0.79s\n",
      "Epoch: 44, Batch: 231/842, Loss: 0.4842, Time: 0.79s\n",
      "Epoch: 44, Batch: 241/842, Loss: 0.5355, Time: 0.80s\n",
      "Epoch: 44, Batch: 251/842, Loss: 0.6381, Time: 0.79s\n",
      "Epoch: 44, Batch: 261/842, Loss: 0.5753, Time: 0.79s\n",
      "Epoch: 44, Batch: 271/842, Loss: 0.5119, Time: 0.79s\n",
      "Epoch: 44, Batch: 281/842, Loss: 0.4036, Time: 0.79s\n",
      "Epoch: 44, Batch: 291/842, Loss: 0.4442, Time: 0.79s\n",
      "Epoch: 44, Batch: 301/842, Loss: 0.4166, Time: 0.79s\n",
      "Epoch: 44, Batch: 311/842, Loss: 0.4147, Time: 0.79s\n",
      "Epoch: 44, Batch: 321/842, Loss: 0.4751, Time: 0.80s\n",
      "Epoch: 44, Batch: 331/842, Loss: 0.6246, Time: 0.80s\n",
      "Epoch: 44, Batch: 341/842, Loss: 0.5900, Time: 0.79s\n",
      "Epoch: 44, Batch: 351/842, Loss: 0.4090, Time: 0.80s\n",
      "Epoch: 44, Batch: 361/842, Loss: 0.3593, Time: 0.79s\n",
      "Epoch: 44, Batch: 371/842, Loss: 0.4889, Time: 0.77s\n",
      "Epoch: 44, Batch: 381/842, Loss: 0.4697, Time: 0.79s\n",
      "Epoch: 44, Batch: 391/842, Loss: 0.4787, Time: 0.80s\n",
      "Epoch: 44, Batch: 401/842, Loss: 0.4802, Time: 0.80s\n",
      "Epoch: 44, Batch: 411/842, Loss: 0.4112, Time: 0.80s\n",
      "Epoch: 44, Batch: 421/842, Loss: 0.4904, Time: 0.79s\n",
      "Epoch: 44, Batch: 431/842, Loss: 0.4840, Time: 0.79s\n",
      "Epoch: 44, Batch: 441/842, Loss: 0.5157, Time: 0.79s\n",
      "Epoch: 44, Batch: 451/842, Loss: 0.4065, Time: 0.79s\n",
      "Epoch: 44, Batch: 461/842, Loss: 0.4621, Time: 0.79s\n",
      "Epoch: 44, Batch: 471/842, Loss: 0.4261, Time: 0.79s\n",
      "Epoch: 44, Batch: 481/842, Loss: 0.4267, Time: 0.79s\n",
      "Epoch: 44, Batch: 491/842, Loss: 0.3652, Time: 0.79s\n",
      "Epoch: 44, Batch: 501/842, Loss: 0.4543, Time: 0.78s\n",
      "Epoch: 44, Batch: 511/842, Loss: 0.6028, Time: 0.82s\n",
      "Epoch: 44, Batch: 521/842, Loss: 0.4634, Time: 0.79s\n",
      "Epoch: 44, Batch: 531/842, Loss: 0.4930, Time: 0.80s\n",
      "Epoch: 44, Batch: 541/842, Loss: 0.4794, Time: 0.81s\n",
      "Epoch: 44, Batch: 551/842, Loss: 0.3647, Time: 0.79s\n",
      "Epoch: 44, Batch: 561/842, Loss: 0.4238, Time: 0.80s\n",
      "Epoch: 44, Batch: 571/842, Loss: 0.4238, Time: 0.78s\n",
      "Epoch: 44, Batch: 581/842, Loss: 0.5819, Time: 0.79s\n",
      "Epoch: 44, Batch: 591/842, Loss: 0.5962, Time: 0.80s\n",
      "Epoch: 44, Batch: 601/842, Loss: 0.3498, Time: 0.79s\n",
      "Epoch: 44, Batch: 611/842, Loss: 0.4922, Time: 0.79s\n",
      "Epoch: 44, Batch: 621/842, Loss: 0.5677, Time: 0.80s\n",
      "Epoch: 44, Batch: 631/842, Loss: 0.5285, Time: 0.79s\n",
      "Epoch: 44, Batch: 641/842, Loss: 0.4091, Time: 0.79s\n",
      "Epoch: 44, Batch: 651/842, Loss: 0.6545, Time: 0.78s\n",
      "Epoch: 44, Batch: 661/842, Loss: 0.4451, Time: 0.79s\n",
      "Epoch: 44, Batch: 671/842, Loss: 0.4890, Time: 0.79s\n",
      "Epoch: 44, Batch: 681/842, Loss: 0.5024, Time: 0.79s\n",
      "Epoch: 44, Batch: 691/842, Loss: 0.4995, Time: 0.77s\n",
      "Epoch: 44, Batch: 701/842, Loss: 0.4395, Time: 0.79s\n",
      "Epoch: 44, Batch: 711/842, Loss: 0.5910, Time: 0.78s\n",
      "Epoch: 44, Batch: 721/842, Loss: 0.4948, Time: 0.80s\n",
      "Epoch: 44, Batch: 731/842, Loss: 0.4340, Time: 0.81s\n",
      "Epoch: 44, Batch: 741/842, Loss: 0.5271, Time: 0.79s\n",
      "Epoch: 44, Batch: 751/842, Loss: 0.3199, Time: 0.78s\n",
      "Epoch: 44, Batch: 761/842, Loss: 0.4260, Time: 0.79s\n",
      "Epoch: 44, Batch: 771/842, Loss: 0.4703, Time: 0.81s\n",
      "Epoch: 44, Batch: 781/842, Loss: 0.4144, Time: 0.82s\n",
      "Epoch: 44, Batch: 791/842, Loss: 0.5480, Time: 0.82s\n",
      "Epoch: 44, Batch: 801/842, Loss: 0.3499, Time: 0.84s\n",
      "Epoch: 44, Batch: 811/842, Loss: 0.4599, Time: 0.79s\n",
      "Epoch: 44, Batch: 821/842, Loss: 0.5404, Time: 0.80s\n",
      "Epoch: 44, Batch: 831/842, Loss: 0.5082, Time: 0.78s\n",
      "Epoch: 44, Batch: 841/842, Loss: 0.6074, Time: 0.78s\n",
      "Epoch 44/100: Train Loss: 0.4796, Val Loss: 0.4583, mIoU: 0.6736, F1: 0.7979, OA: 0.9626\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7689, F1=0.8693, OA=0.8657, Precision=0.8187, Recall=0.9266\n",
      "    Class 1: IoU=0.6951, F1=0.8201, OA=0.9964, Precision=0.8633, Recall=0.7810\n",
      "    Class 2: IoU=0.9082, F1=0.9519, OA=0.9807, Precision=0.9600, Recall=0.9439\n",
      "    Class 3: IoU=0.4565, F1=0.6269, OA=0.9587, Precision=0.6949, Recall=0.5710\n",
      "    Class 4: IoU=0.7252, F1=0.8407, OA=0.9796, Precision=0.8993, Recall=0.7893\n",
      "    Class 5: IoU=0.6584, F1=0.7940, OA=0.9965, Precision=0.8281, Recall=0.7627\n",
      "    Class 6: IoU=0.5576, F1=0.7160, OA=0.9564, Precision=0.8233, Recall=0.6335\n",
      "    Class 7: IoU=0.6188, F1=0.7645, OA=0.9668, Precision=0.8854, Recall=0.6727\n",
      "Epoch: 45, Batch: 1/842, Loss: 0.5039, Time: 2.21s\n",
      "Epoch: 45, Batch: 11/842, Loss: 0.3928, Time: 0.86s\n",
      "Epoch: 45, Batch: 21/842, Loss: 0.4126, Time: 0.83s\n",
      "Epoch: 45, Batch: 31/842, Loss: 0.4773, Time: 0.82s\n",
      "Epoch: 45, Batch: 41/842, Loss: 0.4071, Time: 0.83s\n",
      "Epoch: 45, Batch: 51/842, Loss: 0.4470, Time: 0.80s\n",
      "Epoch: 45, Batch: 61/842, Loss: 0.3958, Time: 0.83s\n",
      "Epoch: 45, Batch: 71/842, Loss: 0.5832, Time: 0.81s\n",
      "Epoch: 45, Batch: 81/842, Loss: 0.4572, Time: 0.84s\n",
      "Epoch: 45, Batch: 91/842, Loss: 0.5970, Time: 0.82s\n",
      "Epoch: 45, Batch: 101/842, Loss: 0.3925, Time: 0.83s\n",
      "Epoch: 45, Batch: 111/842, Loss: 0.3620, Time: 0.83s\n",
      "Epoch: 45, Batch: 121/842, Loss: 0.5468, Time: 0.83s\n",
      "Epoch: 45, Batch: 131/842, Loss: 0.6707, Time: 0.82s\n",
      "Epoch: 45, Batch: 141/842, Loss: 0.4945, Time: 0.82s\n",
      "Epoch: 45, Batch: 151/842, Loss: 0.4700, Time: 0.80s\n",
      "Epoch: 45, Batch: 161/842, Loss: 0.4992, Time: 0.82s\n",
      "Epoch: 45, Batch: 171/842, Loss: 0.4353, Time: 0.81s\n",
      "Epoch: 45, Batch: 181/842, Loss: 0.6163, Time: 0.82s\n",
      "Epoch: 45, Batch: 191/842, Loss: 0.5504, Time: 0.80s\n",
      "Epoch: 45, Batch: 201/842, Loss: 0.4874, Time: 0.81s\n",
      "Epoch: 45, Batch: 211/842, Loss: 0.5010, Time: 0.83s\n",
      "Epoch: 45, Batch: 221/842, Loss: 0.4176, Time: 0.80s\n",
      "Epoch: 45, Batch: 231/842, Loss: 0.4990, Time: 0.82s\n",
      "Epoch: 45, Batch: 241/842, Loss: 0.4707, Time: 0.82s\n",
      "Epoch: 45, Batch: 251/842, Loss: 0.4166, Time: 0.79s\n",
      "Epoch: 45, Batch: 261/842, Loss: 0.4449, Time: 0.79s\n",
      "Epoch: 45, Batch: 271/842, Loss: 0.4374, Time: 0.84s\n",
      "Epoch: 45, Batch: 281/842, Loss: 0.3625, Time: 0.80s\n",
      "Epoch: 45, Batch: 291/842, Loss: 0.6862, Time: 0.96s\n",
      "Epoch: 45, Batch: 301/842, Loss: 0.4328, Time: 0.97s\n",
      "Epoch: 45, Batch: 311/842, Loss: 0.3949, Time: 0.97s\n",
      "Epoch: 45, Batch: 321/842, Loss: 0.5453, Time: 0.96s\n",
      "Epoch: 45, Batch: 331/842, Loss: 0.3500, Time: 0.96s\n",
      "Epoch: 45, Batch: 341/842, Loss: 0.3992, Time: 0.86s\n",
      "Epoch: 45, Batch: 351/842, Loss: 0.5767, Time: 0.82s\n",
      "Epoch: 45, Batch: 361/842, Loss: 0.3691, Time: 0.79s\n",
      "Epoch: 45, Batch: 371/842, Loss: 0.5217, Time: 1.50s\n",
      "Epoch: 45, Batch: 381/842, Loss: 0.4365, Time: 0.80s\n",
      "Epoch: 45, Batch: 391/842, Loss: 0.3885, Time: 0.87s\n",
      "Epoch: 45, Batch: 401/842, Loss: 0.5647, Time: 0.97s\n",
      "Epoch: 45, Batch: 411/842, Loss: 0.4110, Time: 0.92s\n",
      "Epoch: 45, Batch: 421/842, Loss: 0.6031, Time: 0.81s\n",
      "Epoch: 45, Batch: 431/842, Loss: 0.4151, Time: 0.81s\n",
      "Epoch: 45, Batch: 441/842, Loss: 0.3677, Time: 0.82s\n",
      "Epoch: 45, Batch: 451/842, Loss: 0.5009, Time: 0.84s\n",
      "Epoch: 45, Batch: 461/842, Loss: 0.4041, Time: 0.82s\n",
      "Epoch: 45, Batch: 471/842, Loss: 0.3969, Time: 0.81s\n",
      "Epoch: 45, Batch: 481/842, Loss: 0.5415, Time: 0.81s\n",
      "Epoch: 45, Batch: 491/842, Loss: 0.3771, Time: 0.81s\n",
      "Epoch: 45, Batch: 501/842, Loss: 0.5649, Time: 0.81s\n",
      "Epoch: 45, Batch: 511/842, Loss: 0.3291, Time: 0.83s\n",
      "Epoch: 45, Batch: 521/842, Loss: 0.5111, Time: 0.82s\n",
      "Epoch: 45, Batch: 531/842, Loss: 0.3943, Time: 0.82s\n",
      "Epoch: 45, Batch: 541/842, Loss: 0.4263, Time: 0.81s\n",
      "Epoch: 45, Batch: 551/842, Loss: 0.4132, Time: 0.81s\n",
      "Epoch: 45, Batch: 561/842, Loss: 0.4493, Time: 0.82s\n",
      "Epoch: 45, Batch: 571/842, Loss: 0.4099, Time: 0.83s\n",
      "Epoch: 45, Batch: 581/842, Loss: 0.5668, Time: 0.83s\n",
      "Epoch: 45, Batch: 591/842, Loss: 0.5347, Time: 0.81s\n",
      "Epoch: 45, Batch: 601/842, Loss: 0.5289, Time: 0.80s\n",
      "Epoch: 45, Batch: 611/842, Loss: 0.3699, Time: 0.81s\n",
      "Epoch: 45, Batch: 621/842, Loss: 0.3199, Time: 0.82s\n",
      "Epoch: 45, Batch: 631/842, Loss: 0.5682, Time: 0.81s\n",
      "Epoch: 45, Batch: 641/842, Loss: 0.4604, Time: 0.80s\n",
      "Epoch: 45, Batch: 651/842, Loss: 0.5926, Time: 0.81s\n",
      "Epoch: 45, Batch: 661/842, Loss: 0.4318, Time: 0.80s\n",
      "Epoch: 45, Batch: 671/842, Loss: 0.4607, Time: 0.81s\n",
      "Epoch: 45, Batch: 681/842, Loss: 0.3807, Time: 0.80s\n",
      "Epoch: 45, Batch: 691/842, Loss: 0.3902, Time: 0.80s\n",
      "Epoch: 45, Batch: 701/842, Loss: 0.4582, Time: 0.80s\n",
      "Epoch: 45, Batch: 711/842, Loss: 0.4820, Time: 0.81s\n",
      "Epoch: 45, Batch: 721/842, Loss: 0.4537, Time: 0.79s\n",
      "Epoch: 45, Batch: 731/842, Loss: 0.5784, Time: 0.81s\n",
      "Epoch: 45, Batch: 741/842, Loss: 0.4055, Time: 0.79s\n",
      "Epoch: 45, Batch: 751/842, Loss: 0.4618, Time: 0.80s\n",
      "Epoch: 45, Batch: 761/842, Loss: 0.3826, Time: 0.80s\n",
      "Epoch: 45, Batch: 771/842, Loss: 0.4871, Time: 0.80s\n",
      "Epoch: 45, Batch: 781/842, Loss: 0.5268, Time: 0.79s\n",
      "Epoch: 45, Batch: 791/842, Loss: 0.4726, Time: 0.79s\n",
      "Epoch: 45, Batch: 801/842, Loss: 0.5543, Time: 0.80s\n",
      "Epoch: 45, Batch: 811/842, Loss: 0.4159, Time: 0.80s\n",
      "Epoch: 45, Batch: 821/842, Loss: 0.4877, Time: 0.78s\n",
      "Epoch: 45, Batch: 831/842, Loss: 0.5548, Time: 0.78s\n",
      "Epoch: 45, Batch: 841/842, Loss: 0.5706, Time: 0.78s\n",
      "Epoch 45/100: Train Loss: 0.4766, Val Loss: 0.4617, mIoU: 0.6634, F1: 0.7902, OA: 0.9623\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7708, F1=0.8705, OA=0.8665, Precision=0.8173, Recall=0.9312\n",
      "    Class 1: IoU=0.6388, F1=0.7796, OA=0.9953, Precision=0.7733, Recall=0.7860\n",
      "    Class 2: IoU=0.8997, F1=0.9472, OA=0.9789, Precision=0.9598, Recall=0.9350\n",
      "    Class 3: IoU=0.4554, F1=0.6258, OA=0.9634, Precision=0.8284, Recall=0.5029\n",
      "    Class 4: IoU=0.7431, F1=0.8526, OA=0.9804, Precision=0.8732, Recall=0.8330\n",
      "    Class 5: IoU=0.6603, F1=0.7954, OA=0.9965, Precision=0.8323, Recall=0.7616\n",
      "    Class 6: IoU=0.5477, F1=0.7077, OA=0.9531, Precision=0.7690, Recall=0.6555\n",
      "    Class 7: IoU=0.5912, F1=0.7431, OA=0.9644, Precision=0.8796, Recall=0.6432\n",
      "Epoch: 46, Batch: 1/842, Loss: 0.4307, Time: 1.94s\n",
      "Epoch: 46, Batch: 11/842, Loss: 0.4638, Time: 0.81s\n",
      "Epoch: 46, Batch: 21/842, Loss: 0.4484, Time: 0.79s\n",
      "Epoch: 46, Batch: 31/842, Loss: 0.4171, Time: 0.80s\n",
      "Epoch: 46, Batch: 41/842, Loss: 0.5412, Time: 0.80s\n",
      "Epoch: 46, Batch: 51/842, Loss: 0.3697, Time: 0.79s\n",
      "Epoch: 46, Batch: 61/842, Loss: 0.6095, Time: 0.80s\n",
      "Epoch: 46, Batch: 71/842, Loss: 0.3143, Time: 0.81s\n",
      "Epoch: 46, Batch: 81/842, Loss: 0.4133, Time: 0.80s\n",
      "Epoch: 46, Batch: 91/842, Loss: 0.4994, Time: 0.85s\n",
      "Epoch: 46, Batch: 101/842, Loss: 0.6279, Time: 0.83s\n",
      "Epoch: 46, Batch: 111/842, Loss: 0.4684, Time: 0.80s\n",
      "Epoch: 46, Batch: 121/842, Loss: 0.3731, Time: 0.77s\n",
      "Epoch: 46, Batch: 131/842, Loss: 0.4632, Time: 0.82s\n",
      "Epoch: 46, Batch: 141/842, Loss: 0.3934, Time: 0.81s\n",
      "Epoch: 46, Batch: 151/842, Loss: 0.4429, Time: 0.82s\n",
      "Epoch: 46, Batch: 161/842, Loss: 0.4556, Time: 0.82s\n",
      "Epoch: 46, Batch: 171/842, Loss: 0.5243, Time: 0.81s\n",
      "Epoch: 46, Batch: 181/842, Loss: 0.3904, Time: 0.83s\n",
      "Epoch: 46, Batch: 191/842, Loss: 0.3747, Time: 0.83s\n",
      "Epoch: 46, Batch: 201/842, Loss: 0.4570, Time: 0.81s\n",
      "Epoch: 46, Batch: 211/842, Loss: 0.5248, Time: 0.84s\n",
      "Epoch: 46, Batch: 221/842, Loss: 0.4703, Time: 0.84s\n",
      "Epoch: 46, Batch: 231/842, Loss: 0.4841, Time: 0.80s\n",
      "Epoch: 46, Batch: 241/842, Loss: 0.4648, Time: 0.81s\n",
      "Epoch: 46, Batch: 251/842, Loss: 0.5100, Time: 0.83s\n",
      "Epoch: 46, Batch: 261/842, Loss: 0.4164, Time: 0.82s\n",
      "Epoch: 46, Batch: 271/842, Loss: 0.5346, Time: 0.81s\n",
      "Epoch: 46, Batch: 281/842, Loss: 0.5193, Time: 0.84s\n",
      "Epoch: 46, Batch: 291/842, Loss: 0.3718, Time: 0.85s\n",
      "Epoch: 46, Batch: 301/842, Loss: 0.5190, Time: 0.85s\n",
      "Epoch: 46, Batch: 311/842, Loss: 0.3948, Time: 0.82s\n",
      "Epoch: 46, Batch: 321/842, Loss: 0.4549, Time: 0.81s\n",
      "Epoch: 46, Batch: 331/842, Loss: 0.4563, Time: 0.82s\n",
      "Epoch: 46, Batch: 341/842, Loss: 0.5011, Time: 0.86s\n",
      "Epoch: 46, Batch: 351/842, Loss: 0.4508, Time: 0.83s\n",
      "Epoch: 46, Batch: 361/842, Loss: 0.4572, Time: 0.88s\n",
      "Epoch: 46, Batch: 371/842, Loss: 0.4370, Time: 0.78s\n",
      "Epoch: 46, Batch: 381/842, Loss: 0.4374, Time: 0.77s\n",
      "Epoch: 46, Batch: 391/842, Loss: 0.5511, Time: 0.81s\n",
      "Epoch: 46, Batch: 401/842, Loss: 0.5440, Time: 0.78s\n",
      "Epoch: 46, Batch: 411/842, Loss: 0.4201, Time: 0.80s\n",
      "Epoch: 46, Batch: 421/842, Loss: 0.3211, Time: 0.78s\n",
      "Epoch: 46, Batch: 431/842, Loss: 0.5374, Time: 0.79s\n",
      "Epoch: 46, Batch: 441/842, Loss: 0.4114, Time: 0.80s\n",
      "Epoch: 46, Batch: 451/842, Loss: 0.5118, Time: 0.81s\n",
      "Epoch: 46, Batch: 461/842, Loss: 0.4821, Time: 0.76s\n",
      "Epoch: 46, Batch: 471/842, Loss: 0.3473, Time: 0.74s\n",
      "Epoch: 46, Batch: 481/842, Loss: 0.4186, Time: 0.74s\n",
      "Epoch: 46, Batch: 491/842, Loss: 0.4675, Time: 0.75s\n",
      "Epoch: 46, Batch: 501/842, Loss: 0.5119, Time: 0.76s\n",
      "Epoch: 46, Batch: 511/842, Loss: 0.4092, Time: 0.72s\n",
      "Epoch: 46, Batch: 521/842, Loss: 0.3933, Time: 0.73s\n",
      "Epoch: 46, Batch: 531/842, Loss: 0.6724, Time: 0.72s\n",
      "Epoch: 46, Batch: 541/842, Loss: 0.4481, Time: 0.73s\n",
      "Epoch: 46, Batch: 551/842, Loss: 0.4864, Time: 0.73s\n",
      "Epoch: 46, Batch: 561/842, Loss: 0.4865, Time: 0.72s\n",
      "Epoch: 46, Batch: 571/842, Loss: 0.4497, Time: 0.72s\n",
      "Epoch: 46, Batch: 581/842, Loss: 0.4242, Time: 0.72s\n",
      "Epoch: 46, Batch: 591/842, Loss: 0.3278, Time: 0.73s\n",
      "Epoch: 46, Batch: 601/842, Loss: 0.4092, Time: 0.72s\n",
      "Epoch: 46, Batch: 611/842, Loss: 0.4088, Time: 0.72s\n",
      "Epoch: 46, Batch: 621/842, Loss: 0.3572, Time: 0.72s\n",
      "Epoch: 46, Batch: 631/842, Loss: 0.3815, Time: 0.79s\n",
      "Epoch: 46, Batch: 641/842, Loss: 0.4323, Time: 0.79s\n",
      "Epoch: 46, Batch: 651/842, Loss: 0.4317, Time: 0.79s\n",
      "Epoch: 46, Batch: 661/842, Loss: 0.4333, Time: 0.78s\n",
      "Epoch: 46, Batch: 671/842, Loss: 0.4578, Time: 0.69s\n",
      "Epoch: 46, Batch: 681/842, Loss: 0.4849, Time: 0.73s\n",
      "Epoch: 46, Batch: 691/842, Loss: 0.5672, Time: 0.75s\n",
      "Epoch: 46, Batch: 701/842, Loss: 0.5374, Time: 0.75s\n",
      "Epoch: 46, Batch: 711/842, Loss: 0.4301, Time: 0.74s\n",
      "Epoch: 46, Batch: 721/842, Loss: 0.5103, Time: 0.74s\n",
      "Epoch: 46, Batch: 731/842, Loss: 0.4672, Time: 0.75s\n",
      "Epoch: 46, Batch: 741/842, Loss: 0.4045, Time: 0.75s\n",
      "Epoch: 46, Batch: 751/842, Loss: 0.5265, Time: 0.75s\n",
      "Epoch: 46, Batch: 761/842, Loss: 0.3223, Time: 0.76s\n",
      "Epoch: 46, Batch: 771/842, Loss: 0.4988, Time: 0.77s\n",
      "Epoch: 46, Batch: 781/842, Loss: 0.6112, Time: 0.75s\n",
      "Epoch: 46, Batch: 791/842, Loss: 0.5197, Time: 0.74s\n",
      "Epoch: 46, Batch: 801/842, Loss: 0.5930, Time: 0.80s\n",
      "Epoch: 46, Batch: 811/842, Loss: 0.5692, Time: 0.74s\n",
      "Epoch: 46, Batch: 821/842, Loss: 0.4091, Time: 0.72s\n",
      "Epoch: 46, Batch: 831/842, Loss: 0.4614, Time: 0.72s\n",
      "Epoch: 46, Batch: 841/842, Loss: 0.5436, Time: 0.68s\n",
      "Epoch 46/100: Train Loss: 0.4723, Val Loss: 0.4487, mIoU: 0.6696, F1: 0.7949, OA: 0.9636\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7749, F1=0.8732, OA=0.8693, Precision=0.8203, Recall=0.9334\n",
      "    Class 1: IoU=0.6265, F1=0.7704, OA=0.9950, Precision=0.7551, Recall=0.7863\n",
      "    Class 2: IoU=0.9031, F1=0.9491, OA=0.9796, Precision=0.9589, Recall=0.9395\n",
      "    Class 3: IoU=0.4474, F1=0.6182, OA=0.9634, Precision=0.8437, Recall=0.4879\n",
      "    Class 4: IoU=0.7413, F1=0.8514, OA=0.9803, Precision=0.8760, Recall=0.8282\n",
      "    Class 5: IoU=0.6583, F1=0.7940, OA=0.9966, Precision=0.8564, Recall=0.7400\n",
      "    Class 6: IoU=0.5640, F1=0.7212, OA=0.9571, Precision=0.8263, Recall=0.6399\n",
      "    Class 7: IoU=0.6412, F1=0.7814, OA=0.9678, Precision=0.8547, Recall=0.7196\n",
      "Epoch: 47, Batch: 1/842, Loss: 0.5682, Time: 1.70s\n",
      "Epoch: 47, Batch: 11/842, Loss: 0.5037, Time: 0.74s\n",
      "Epoch: 47, Batch: 21/842, Loss: 0.3537, Time: 0.79s\n",
      "Epoch: 47, Batch: 31/842, Loss: 0.5209, Time: 0.81s\n",
      "Epoch: 47, Batch: 41/842, Loss: 0.4901, Time: 0.77s\n",
      "Epoch: 47, Batch: 51/842, Loss: 0.5977, Time: 0.77s\n",
      "Epoch: 47, Batch: 61/842, Loss: 0.5119, Time: 0.77s\n",
      "Epoch: 47, Batch: 71/842, Loss: 0.4733, Time: 0.81s\n",
      "Epoch: 47, Batch: 81/842, Loss: 0.4288, Time: 0.84s\n",
      "Epoch: 47, Batch: 91/842, Loss: 0.6014, Time: 0.81s\n",
      "Epoch: 47, Batch: 101/842, Loss: 0.5990, Time: 0.82s\n",
      "Epoch: 47, Batch: 111/842, Loss: 0.4929, Time: 0.78s\n",
      "Epoch: 47, Batch: 121/842, Loss: 0.3950, Time: 0.81s\n",
      "Epoch: 47, Batch: 131/842, Loss: 0.4904, Time: 0.80s\n",
      "Epoch: 47, Batch: 141/842, Loss: 0.4544, Time: 0.83s\n",
      "Epoch: 47, Batch: 151/842, Loss: 0.5020, Time: 0.76s\n",
      "Epoch: 47, Batch: 161/842, Loss: 0.6402, Time: 0.76s\n",
      "Epoch: 47, Batch: 171/842, Loss: 0.3401, Time: 0.74s\n",
      "Epoch: 47, Batch: 181/842, Loss: 0.5106, Time: 0.75s\n",
      "Epoch: 47, Batch: 191/842, Loss: 0.4472, Time: 0.75s\n",
      "Epoch: 47, Batch: 201/842, Loss: 0.4028, Time: 0.76s\n",
      "Epoch: 47, Batch: 211/842, Loss: 0.4375, Time: 0.76s\n",
      "Epoch: 47, Batch: 221/842, Loss: 0.3419, Time: 0.74s\n",
      "Epoch: 47, Batch: 231/842, Loss: 0.5461, Time: 0.72s\n",
      "Epoch: 47, Batch: 241/842, Loss: 0.4694, Time: 0.76s\n",
      "Epoch: 47, Batch: 251/842, Loss: 0.3949, Time: 0.73s\n",
      "Epoch: 47, Batch: 261/842, Loss: 0.5454, Time: 0.73s\n",
      "Epoch: 47, Batch: 271/842, Loss: 0.6424, Time: 0.73s\n",
      "Epoch: 47, Batch: 281/842, Loss: 0.5904, Time: 0.75s\n",
      "Epoch: 47, Batch: 291/842, Loss: 0.5610, Time: 0.74s\n",
      "Epoch: 47, Batch: 301/842, Loss: 0.7405, Time: 0.73s\n",
      "Epoch: 47, Batch: 311/842, Loss: 0.4081, Time: 0.72s\n",
      "Epoch: 47, Batch: 321/842, Loss: 0.7040, Time: 0.76s\n",
      "Epoch: 47, Batch: 331/842, Loss: 0.4114, Time: 0.78s\n",
      "Epoch: 47, Batch: 341/842, Loss: 0.6009, Time: 0.77s\n",
      "Epoch: 47, Batch: 351/842, Loss: 0.4797, Time: 0.76s\n",
      "Epoch: 47, Batch: 361/842, Loss: 0.3002, Time: 0.78s\n",
      "Epoch: 47, Batch: 371/842, Loss: 0.3997, Time: 0.78s\n",
      "Epoch: 47, Batch: 381/842, Loss: 0.4842, Time: 0.76s\n",
      "Epoch: 47, Batch: 391/842, Loss: 0.4123, Time: 0.76s\n",
      "Epoch: 47, Batch: 401/842, Loss: 0.4692, Time: 0.78s\n",
      "Epoch: 47, Batch: 411/842, Loss: 0.3757, Time: 0.76s\n",
      "Epoch: 47, Batch: 421/842, Loss: 0.4820, Time: 0.78s\n",
      "Epoch: 47, Batch: 431/842, Loss: 0.4380, Time: 0.73s\n",
      "Epoch: 47, Batch: 441/842, Loss: 0.4745, Time: 0.79s\n",
      "Epoch: 47, Batch: 451/842, Loss: 0.4078, Time: 0.77s\n",
      "Epoch: 47, Batch: 461/842, Loss: 0.3258, Time: 0.73s\n",
      "Epoch: 47, Batch: 471/842, Loss: 0.3083, Time: 0.73s\n",
      "Epoch: 47, Batch: 481/842, Loss: 0.5042, Time: 0.75s\n",
      "Epoch: 47, Batch: 491/842, Loss: 0.6853, Time: 0.74s\n",
      "Epoch: 47, Batch: 501/842, Loss: 0.4875, Time: 0.75s\n",
      "Epoch: 47, Batch: 511/842, Loss: 0.5093, Time: 0.72s\n",
      "Epoch: 47, Batch: 521/842, Loss: 0.4519, Time: 0.72s\n",
      "Epoch: 47, Batch: 531/842, Loss: 0.4415, Time: 0.73s\n",
      "Epoch: 47, Batch: 541/842, Loss: 0.4520, Time: 0.71s\n",
      "Epoch: 47, Batch: 551/842, Loss: 0.4441, Time: 0.75s\n",
      "Epoch: 47, Batch: 561/842, Loss: 0.5489, Time: 0.72s\n",
      "Epoch: 47, Batch: 571/842, Loss: 0.5638, Time: 0.72s\n",
      "Epoch: 47, Batch: 581/842, Loss: 0.4359, Time: 0.73s\n",
      "Epoch: 47, Batch: 591/842, Loss: 0.4780, Time: 0.73s\n",
      "Epoch: 47, Batch: 601/842, Loss: 0.4209, Time: 0.72s\n",
      "Epoch: 47, Batch: 611/842, Loss: 0.2923, Time: 0.72s\n",
      "Epoch: 47, Batch: 621/842, Loss: 0.4717, Time: 0.72s\n",
      "Epoch: 47, Batch: 631/842, Loss: 0.5421, Time: 0.71s\n",
      "Epoch: 47, Batch: 641/842, Loss: 0.5312, Time: 0.74s\n",
      "Epoch: 47, Batch: 651/842, Loss: 0.5260, Time: 0.74s\n",
      "Epoch: 47, Batch: 661/842, Loss: 0.4425, Time: 0.72s\n",
      "Epoch: 47, Batch: 671/842, Loss: 0.5017, Time: 0.73s\n",
      "Epoch: 47, Batch: 681/842, Loss: 0.4042, Time: 0.72s\n",
      "Epoch: 47, Batch: 691/842, Loss: 0.4982, Time: 0.72s\n",
      "Epoch: 47, Batch: 701/842, Loss: 0.5882, Time: 0.75s\n",
      "Epoch: 47, Batch: 711/842, Loss: 0.4793, Time: 0.72s\n",
      "Epoch: 47, Batch: 721/842, Loss: 0.3848, Time: 0.72s\n",
      "Epoch: 47, Batch: 731/842, Loss: 0.5727, Time: 0.72s\n",
      "Epoch: 47, Batch: 741/842, Loss: 0.6386, Time: 0.71s\n",
      "Epoch: 47, Batch: 751/842, Loss: 0.3268, Time: 0.72s\n",
      "Epoch: 47, Batch: 761/842, Loss: 0.5923, Time: 0.71s\n",
      "Epoch: 47, Batch: 771/842, Loss: 0.4263, Time: 0.72s\n",
      "Epoch: 47, Batch: 781/842, Loss: 0.5980, Time: 0.76s\n",
      "Epoch: 47, Batch: 791/842, Loss: 0.4651, Time: 0.75s\n",
      "Epoch: 47, Batch: 801/842, Loss: 0.4446, Time: 0.77s\n",
      "Epoch: 47, Batch: 811/842, Loss: 0.4645, Time: 0.79s\n",
      "Epoch: 47, Batch: 821/842, Loss: 0.5938, Time: 0.74s\n",
      "Epoch: 47, Batch: 831/842, Loss: 0.3918, Time: 0.72s\n",
      "Epoch: 47, Batch: 841/842, Loss: 0.4799, Time: 0.71s\n",
      "Epoch 47/100: Train Loss: 0.4786, Val Loss: 0.4706, mIoU: 0.6616, F1: 0.7895, OA: 0.9615\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7666, F1=0.8679, OA=0.8645, Precision=0.8188, Recall=0.9233\n",
      "    Class 1: IoU=0.6543, F1=0.7910, OA=0.9957, Precision=0.8239, Recall=0.7606\n",
      "    Class 2: IoU=0.8943, F1=0.9442, OA=0.9779, Precision=0.9651, Recall=0.9242\n",
      "    Class 3: IoU=0.4761, F1=0.6451, OA=0.9623, Precision=0.7535, Recall=0.5640\n",
      "    Class 4: IoU=0.7388, F1=0.8498, OA=0.9800, Precision=0.8730, Recall=0.8277\n",
      "    Class 5: IoU=0.6333, F1=0.7755, OA=0.9958, Precision=0.7367, Recall=0.8186\n",
      "    Class 6: IoU=0.5410, F1=0.7021, OA=0.9521, Precision=0.7611, Recall=0.6516\n",
      "    Class 7: IoU=0.5882, F1=0.7407, OA=0.9642, Precision=0.8795, Recall=0.6397\n",
      "Epoch: 48, Batch: 1/842, Loss: 0.4799, Time: 1.66s\n",
      "Epoch: 48, Batch: 11/842, Loss: 0.5376, Time: 0.75s\n",
      "Epoch: 48, Batch: 21/842, Loss: 0.4175, Time: 0.74s\n",
      "Epoch: 48, Batch: 31/842, Loss: 0.5199, Time: 0.74s\n",
      "Epoch: 48, Batch: 41/842, Loss: 0.5738, Time: 0.75s\n",
      "Epoch: 48, Batch: 51/842, Loss: 0.4171, Time: 0.79s\n",
      "Epoch: 48, Batch: 61/842, Loss: 0.5538, Time: 0.81s\n",
      "Epoch: 48, Batch: 71/842, Loss: 0.5080, Time: 0.85s\n",
      "Epoch: 48, Batch: 81/842, Loss: 0.5338, Time: 0.81s\n",
      "Epoch: 48, Batch: 91/842, Loss: 0.3691, Time: 0.76s\n",
      "Epoch: 48, Batch: 101/842, Loss: 0.4152, Time: 0.73s\n",
      "Epoch: 48, Batch: 111/842, Loss: 0.4450, Time: 0.73s\n",
      "Epoch: 48, Batch: 121/842, Loss: 0.4408, Time: 0.74s\n",
      "Epoch: 48, Batch: 131/842, Loss: 0.4880, Time: 0.73s\n",
      "Epoch: 48, Batch: 141/842, Loss: 0.5146, Time: 0.73s\n",
      "Epoch: 48, Batch: 151/842, Loss: 0.3814, Time: 0.73s\n",
      "Epoch: 48, Batch: 161/842, Loss: 0.4855, Time: 0.74s\n",
      "Epoch: 48, Batch: 171/842, Loss: 0.5044, Time: 0.76s\n",
      "Epoch: 48, Batch: 181/842, Loss: 0.4674, Time: 0.71s\n",
      "Epoch: 48, Batch: 191/842, Loss: 0.4855, Time: 0.70s\n",
      "Epoch: 48, Batch: 201/842, Loss: 0.4167, Time: 0.73s\n",
      "Epoch: 48, Batch: 211/842, Loss: 0.5131, Time: 0.72s\n",
      "Epoch: 48, Batch: 221/842, Loss: 0.3857, Time: 0.72s\n",
      "Epoch: 48, Batch: 231/842, Loss: 0.4980, Time: 0.73s\n",
      "Epoch: 48, Batch: 241/842, Loss: 0.3331, Time: 0.73s\n",
      "Epoch: 48, Batch: 251/842, Loss: 0.4593, Time: 0.73s\n",
      "Epoch: 48, Batch: 261/842, Loss: 0.6119, Time: 0.74s\n",
      "Epoch: 48, Batch: 271/842, Loss: 0.4956, Time: 0.73s\n",
      "Epoch: 48, Batch: 281/842, Loss: 0.4138, Time: 0.72s\n",
      "Epoch: 48, Batch: 291/842, Loss: 0.3813, Time: 0.73s\n",
      "Epoch: 48, Batch: 301/842, Loss: 0.5262, Time: 0.73s\n",
      "Epoch: 48, Batch: 311/842, Loss: 0.4609, Time: 0.75s\n",
      "Epoch: 48, Batch: 321/842, Loss: 0.4105, Time: 0.74s\n",
      "Epoch: 48, Batch: 331/842, Loss: 0.4076, Time: 0.74s\n",
      "Epoch: 48, Batch: 341/842, Loss: 0.5241, Time: 0.74s\n",
      "Epoch: 48, Batch: 351/842, Loss: 0.3982, Time: 0.73s\n",
      "Epoch: 48, Batch: 361/842, Loss: 0.3853, Time: 0.74s\n",
      "Epoch: 48, Batch: 371/842, Loss: 0.4210, Time: 0.74s\n",
      "Epoch: 48, Batch: 381/842, Loss: 0.3438, Time: 0.73s\n",
      "Epoch: 48, Batch: 391/842, Loss: 0.4941, Time: 0.73s\n",
      "Epoch: 48, Batch: 401/842, Loss: 0.5242, Time: 0.80s\n",
      "Epoch: 48, Batch: 411/842, Loss: 0.5518, Time: 0.79s\n",
      "Epoch: 48, Batch: 421/842, Loss: 0.4610, Time: 0.81s\n",
      "Epoch: 48, Batch: 431/842, Loss: 0.6262, Time: 0.82s\n",
      "Epoch: 48, Batch: 441/842, Loss: 0.4025, Time: 0.80s\n",
      "Epoch: 48, Batch: 451/842, Loss: 0.3431, Time: 0.76s\n",
      "Epoch: 48, Batch: 461/842, Loss: 0.5150, Time: 0.77s\n",
      "Epoch: 48, Batch: 471/842, Loss: 0.4520, Time: 0.76s\n",
      "Epoch: 48, Batch: 481/842, Loss: 0.5722, Time: 0.76s\n",
      "Epoch: 48, Batch: 491/842, Loss: 0.5380, Time: 0.76s\n",
      "Epoch: 48, Batch: 501/842, Loss: 0.3764, Time: 0.77s\n",
      "Epoch: 48, Batch: 511/842, Loss: 0.3873, Time: 0.76s\n",
      "Epoch: 48, Batch: 521/842, Loss: 0.3978, Time: 0.75s\n",
      "Epoch: 48, Batch: 531/842, Loss: 0.5254, Time: 0.73s\n",
      "Epoch: 48, Batch: 541/842, Loss: 0.5359, Time: 0.73s\n",
      "Epoch: 48, Batch: 551/842, Loss: 0.3897, Time: 0.73s\n",
      "Epoch: 48, Batch: 561/842, Loss: 0.4958, Time: 0.73s\n",
      "Epoch: 48, Batch: 571/842, Loss: 0.4506, Time: 0.73s\n",
      "Epoch: 48, Batch: 581/842, Loss: 0.5352, Time: 0.75s\n",
      "Epoch: 48, Batch: 591/842, Loss: 0.4235, Time: 0.78s\n",
      "Epoch: 48, Batch: 601/842, Loss: 0.6213, Time: 0.80s\n",
      "Epoch: 48, Batch: 611/842, Loss: 0.5479, Time: 0.77s\n",
      "Epoch: 48, Batch: 621/842, Loss: 0.4339, Time: 0.79s\n",
      "Epoch: 48, Batch: 631/842, Loss: 0.4936, Time: 0.79s\n",
      "Epoch: 48, Batch: 641/842, Loss: 0.4710, Time: 0.76s\n",
      "Epoch: 48, Batch: 651/842, Loss: 0.4095, Time: 0.78s\n",
      "Epoch: 48, Batch: 661/842, Loss: 0.4165, Time: 0.79s\n",
      "Epoch: 48, Batch: 671/842, Loss: 0.5408, Time: 0.76s\n",
      "Epoch: 48, Batch: 681/842, Loss: 0.4066, Time: 0.75s\n",
      "Epoch: 48, Batch: 691/842, Loss: 0.3490, Time: 0.74s\n",
      "Epoch: 48, Batch: 701/842, Loss: 0.4614, Time: 0.80s\n",
      "Epoch: 48, Batch: 711/842, Loss: 0.5063, Time: 1.42s\n",
      "Epoch: 48, Batch: 721/842, Loss: 0.4883, Time: 0.76s\n",
      "Epoch: 48, Batch: 731/842, Loss: 0.4647, Time: 0.81s\n",
      "Epoch: 48, Batch: 741/842, Loss: 0.4481, Time: 0.77s\n",
      "Epoch: 48, Batch: 751/842, Loss: 0.4631, Time: 0.80s\n",
      "Epoch: 48, Batch: 761/842, Loss: 0.5576, Time: 0.78s\n",
      "Epoch: 48, Batch: 771/842, Loss: 0.4520, Time: 0.73s\n",
      "Epoch: 48, Batch: 781/842, Loss: 0.5689, Time: 0.74s\n",
      "Epoch: 48, Batch: 791/842, Loss: 0.4526, Time: 0.78s\n",
      "Epoch: 48, Batch: 801/842, Loss: 0.4234, Time: 0.76s\n",
      "Epoch: 48, Batch: 811/842, Loss: 0.4017, Time: 0.75s\n",
      "Epoch: 48, Batch: 821/842, Loss: 0.4328, Time: 0.79s\n",
      "Epoch: 48, Batch: 831/842, Loss: 0.4524, Time: 0.75s\n",
      "Epoch: 48, Batch: 841/842, Loss: 0.4141, Time: 0.75s\n",
      "Epoch 48/100: Train Loss: 0.4713, Val Loss: 0.4356, mIoU: 0.6882, F1: 0.8087, OA: 0.9646\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7784, F1=0.8754, OA=0.8722, Precision=0.8259, Recall=0.9312\n",
      "    Class 1: IoU=0.7077, F1=0.8288, OA=0.9966, Precision=0.8804, Recall=0.7829\n",
      "    Class 2: IoU=0.9123, F1=0.9541, OA=0.9815, Precision=0.9584, Recall=0.9499\n",
      "    Class 3: IoU=0.4710, F1=0.6404, OA=0.9642, Precision=0.8225, Recall=0.5243\n",
      "    Class 4: IoU=0.7452, F1=0.8540, OA=0.9811, Precision=0.9028, Recall=0.8102\n",
      "    Class 5: IoU=0.6800, F1=0.8095, OA=0.9968, Precision=0.8504, Recall=0.7724\n",
      "    Class 6: IoU=0.5695, F1=0.7257, OA=0.9571, Precision=0.8131, Recall=0.6553\n",
      "    Class 7: IoU=0.6418, F1=0.7818, OA=0.9677, Precision=0.8495, Recall=0.7241\n",
      "Saving best model with mIoU: 0.6882\n",
      "Epoch: 49, Batch: 1/842, Loss: 0.4955, Time: 1.53s\n",
      "Epoch: 49, Batch: 11/842, Loss: 0.4798, Time: 0.78s\n",
      "Epoch: 49, Batch: 21/842, Loss: 0.5622, Time: 0.77s\n",
      "Epoch: 49, Batch: 31/842, Loss: 0.4812, Time: 0.77s\n",
      "Epoch: 49, Batch: 41/842, Loss: 0.4027, Time: 0.78s\n",
      "Epoch: 49, Batch: 51/842, Loss: 0.4806, Time: 0.78s\n",
      "Epoch: 49, Batch: 61/842, Loss: 0.5839, Time: 0.77s\n",
      "Epoch: 49, Batch: 71/842, Loss: 0.4676, Time: 0.76s\n",
      "Epoch: 49, Batch: 81/842, Loss: 0.4635, Time: 0.81s\n",
      "Epoch: 49, Batch: 91/842, Loss: 0.5802, Time: 0.83s\n",
      "Epoch: 49, Batch: 101/842, Loss: 0.5180, Time: 0.80s\n",
      "Epoch: 49, Batch: 111/842, Loss: 0.3334, Time: 0.83s\n",
      "Epoch: 49, Batch: 121/842, Loss: 0.5264, Time: 0.83s\n",
      "Epoch: 49, Batch: 131/842, Loss: 0.4382, Time: 0.83s\n",
      "Epoch: 49, Batch: 141/842, Loss: 0.4432, Time: 0.81s\n",
      "Epoch: 49, Batch: 151/842, Loss: 0.4504, Time: 0.81s\n",
      "Epoch: 49, Batch: 161/842, Loss: 0.4928, Time: 0.80s\n",
      "Epoch: 49, Batch: 171/842, Loss: 0.4881, Time: 0.79s\n",
      "Epoch: 49, Batch: 181/842, Loss: 0.6420, Time: 0.73s\n",
      "Epoch: 49, Batch: 191/842, Loss: 0.4333, Time: 0.75s\n",
      "Epoch: 49, Batch: 201/842, Loss: 0.4417, Time: 0.72s\n",
      "Epoch: 49, Batch: 211/842, Loss: 0.4855, Time: 0.73s\n",
      "Epoch: 49, Batch: 221/842, Loss: 0.3471, Time: 0.73s\n",
      "Epoch: 49, Batch: 231/842, Loss: 0.3988, Time: 0.74s\n",
      "Epoch: 49, Batch: 241/842, Loss: 0.5822, Time: 0.73s\n",
      "Epoch: 49, Batch: 251/842, Loss: 0.4286, Time: 0.72s\n",
      "Epoch: 49, Batch: 261/842, Loss: 0.4263, Time: 0.71s\n",
      "Epoch: 49, Batch: 271/842, Loss: 0.4466, Time: 0.73s\n",
      "Epoch: 49, Batch: 281/842, Loss: 0.3280, Time: 0.71s\n",
      "Epoch: 49, Batch: 291/842, Loss: 0.6544, Time: 0.74s\n",
      "Epoch: 49, Batch: 301/842, Loss: 0.5396, Time: 0.72s\n",
      "Epoch: 49, Batch: 311/842, Loss: 0.4632, Time: 0.72s\n",
      "Epoch: 49, Batch: 321/842, Loss: 0.4238, Time: 0.72s\n",
      "Epoch: 49, Batch: 331/842, Loss: 0.3736, Time: 0.72s\n",
      "Epoch: 49, Batch: 341/842, Loss: 0.5289, Time: 0.73s\n",
      "Epoch: 49, Batch: 351/842, Loss: 0.6026, Time: 0.73s\n",
      "Epoch: 49, Batch: 361/842, Loss: 0.3444, Time: 0.72s\n",
      "Epoch: 49, Batch: 371/842, Loss: 0.5435, Time: 0.72s\n",
      "Epoch: 49, Batch: 381/842, Loss: 0.5308, Time: 0.73s\n",
      "Epoch: 49, Batch: 391/842, Loss: 0.4305, Time: 0.72s\n",
      "Epoch: 49, Batch: 401/842, Loss: 0.5209, Time: 0.72s\n",
      "Epoch: 49, Batch: 411/842, Loss: 0.4244, Time: 0.72s\n",
      "Epoch: 49, Batch: 421/842, Loss: 0.5304, Time: 0.72s\n",
      "Epoch: 49, Batch: 431/842, Loss: 0.5383, Time: 0.72s\n",
      "Epoch: 49, Batch: 441/842, Loss: 0.3988, Time: 0.73s\n",
      "Epoch: 49, Batch: 451/842, Loss: 0.5977, Time: 0.70s\n",
      "Epoch: 49, Batch: 461/842, Loss: 0.4346, Time: 0.78s\n",
      "Epoch: 49, Batch: 471/842, Loss: 0.5098, Time: 0.77s\n",
      "Epoch: 49, Batch: 481/842, Loss: 0.4829, Time: 0.76s\n",
      "Epoch: 49, Batch: 491/842, Loss: 0.4420, Time: 0.77s\n",
      "Epoch: 49, Batch: 501/842, Loss: 0.4299, Time: 0.76s\n",
      "Epoch: 49, Batch: 511/842, Loss: 0.4038, Time: 0.80s\n",
      "Epoch: 49, Batch: 521/842, Loss: 0.5621, Time: 0.77s\n",
      "Epoch: 49, Batch: 531/842, Loss: 0.5155, Time: 0.73s\n",
      "Epoch: 49, Batch: 541/842, Loss: 0.4450, Time: 0.72s\n",
      "Epoch: 49, Batch: 551/842, Loss: 0.5291, Time: 0.73s\n",
      "Epoch: 49, Batch: 561/842, Loss: 0.4128, Time: 0.73s\n",
      "Epoch: 49, Batch: 571/842, Loss: 0.5095, Time: 0.73s\n",
      "Epoch: 49, Batch: 581/842, Loss: 0.3703, Time: 0.72s\n",
      "Epoch: 49, Batch: 591/842, Loss: 0.5064, Time: 0.72s\n",
      "Epoch: 49, Batch: 601/842, Loss: 0.5205, Time: 0.72s\n",
      "Epoch: 49, Batch: 611/842, Loss: 0.5328, Time: 0.72s\n",
      "Epoch: 49, Batch: 621/842, Loss: 0.4532, Time: 0.73s\n",
      "Epoch: 49, Batch: 631/842, Loss: 0.5442, Time: 0.73s\n",
      "Epoch: 49, Batch: 641/842, Loss: 0.4472, Time: 0.75s\n",
      "Epoch: 49, Batch: 651/842, Loss: 0.6042, Time: 0.74s\n",
      "Epoch: 49, Batch: 661/842, Loss: 0.4984, Time: 0.74s\n",
      "Epoch: 49, Batch: 671/842, Loss: 0.4233, Time: 0.74s\n",
      "Epoch: 49, Batch: 681/842, Loss: 0.4351, Time: 0.72s\n",
      "Epoch: 49, Batch: 691/842, Loss: 0.4305, Time: 0.69s\n",
      "Epoch: 49, Batch: 701/842, Loss: 0.4029, Time: 0.69s\n",
      "Epoch: 49, Batch: 711/842, Loss: 0.5421, Time: 0.69s\n",
      "Epoch: 49, Batch: 721/842, Loss: 0.5506, Time: 0.70s\n",
      "Epoch: 49, Batch: 731/842, Loss: 0.4064, Time: 0.72s\n",
      "Epoch: 49, Batch: 741/842, Loss: 0.4720, Time: 0.73s\n",
      "Epoch: 49, Batch: 751/842, Loss: 0.5278, Time: 0.73s\n",
      "Epoch: 49, Batch: 761/842, Loss: 0.4952, Time: 0.73s\n",
      "Epoch: 49, Batch: 771/842, Loss: 0.4807, Time: 0.73s\n",
      "Epoch: 49, Batch: 781/842, Loss: 0.4534, Time: 0.74s\n",
      "Epoch: 49, Batch: 791/842, Loss: 0.5238, Time: 0.73s\n",
      "Epoch: 49, Batch: 801/842, Loss: 0.3709, Time: 0.73s\n",
      "Epoch: 49, Batch: 811/842, Loss: 0.4178, Time: 0.73s\n",
      "Epoch: 49, Batch: 821/842, Loss: 0.5494, Time: 0.74s\n",
      "Epoch: 49, Batch: 831/842, Loss: 0.4150, Time: 0.71s\n",
      "Epoch: 49, Batch: 841/842, Loss: 0.5225, Time: 0.68s\n",
      "Epoch 49/100: Train Loss: 0.4662, Val Loss: 0.4502, mIoU: 0.6792, F1: 0.8025, OA: 0.9633\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7737, F1=0.8724, OA=0.8696, Precision=0.8257, Recall=0.9248\n",
      "    Class 1: IoU=0.6920, F1=0.8180, OA=0.9963, Precision=0.8447, Recall=0.7929\n",
      "    Class 2: IoU=0.9050, F1=0.9501, OA=0.9801, Precision=0.9643, Recall=0.9363\n",
      "    Class 3: IoU=0.4831, F1=0.6515, OA=0.9636, Precision=0.7801, Recall=0.5593\n",
      "    Class 4: IoU=0.7469, F1=0.8551, OA=0.9806, Precision=0.8718, Recall=0.8390\n",
      "    Class 5: IoU=0.6669, F1=0.8002, OA=0.9964, Precision=0.7901, Recall=0.8105\n",
      "    Class 6: IoU=0.5599, F1=0.7179, OA=0.9542, Precision=0.7704, Recall=0.6721\n",
      "    Class 7: IoU=0.6061, F1=0.7548, OA=0.9658, Precision=0.8866, Recall=0.6571\n",
      "Epoch: 50, Batch: 1/842, Loss: 0.3849, Time: 1.95s\n",
      "Epoch: 50, Batch: 11/842, Loss: 0.4006, Time: 0.76s\n",
      "Epoch: 50, Batch: 21/842, Loss: 0.3827, Time: 0.74s\n",
      "Epoch: 50, Batch: 31/842, Loss: 0.4564, Time: 0.75s\n",
      "Epoch: 50, Batch: 41/842, Loss: 0.4194, Time: 0.74s\n",
      "Epoch: 50, Batch: 51/842, Loss: 0.3595, Time: 0.74s\n",
      "Epoch: 50, Batch: 61/842, Loss: 0.5792, Time: 0.72s\n",
      "Epoch: 50, Batch: 71/842, Loss: 0.6563, Time: 0.75s\n",
      "Epoch: 50, Batch: 81/842, Loss: 0.4772, Time: 0.75s\n",
      "Epoch: 50, Batch: 91/842, Loss: 0.4849, Time: 0.74s\n",
      "Epoch: 50, Batch: 101/842, Loss: 0.3679, Time: 0.75s\n",
      "Epoch: 50, Batch: 111/842, Loss: 0.4940, Time: 0.74s\n",
      "Epoch: 50, Batch: 121/842, Loss: 0.5319, Time: 0.74s\n",
      "Epoch: 50, Batch: 131/842, Loss: 0.6295, Time: 0.74s\n",
      "Epoch: 50, Batch: 141/842, Loss: 0.3585, Time: 0.74s\n",
      "Epoch: 50, Batch: 151/842, Loss: 0.4186, Time: 0.73s\n",
      "Epoch: 50, Batch: 161/842, Loss: 0.4193, Time: 0.82s\n",
      "Epoch: 50, Batch: 171/842, Loss: 0.4259, Time: 0.82s\n",
      "Epoch: 50, Batch: 181/842, Loss: 0.3768, Time: 0.76s\n",
      "Epoch: 50, Batch: 191/842, Loss: 0.3431, Time: 0.76s\n",
      "Epoch: 50, Batch: 201/842, Loss: 0.3860, Time: 0.76s\n",
      "Epoch: 50, Batch: 211/842, Loss: 0.5356, Time: 0.76s\n",
      "Epoch: 50, Batch: 221/842, Loss: 0.3363, Time: 0.75s\n",
      "Epoch: 50, Batch: 231/842, Loss: 0.4149, Time: 0.74s\n",
      "Epoch: 50, Batch: 241/842, Loss: 0.3731, Time: 0.74s\n",
      "Epoch: 50, Batch: 251/842, Loss: 0.5658, Time: 0.75s\n",
      "Epoch: 50, Batch: 261/842, Loss: 0.4549, Time: 0.71s\n",
      "Epoch: 50, Batch: 271/842, Loss: 0.5326, Time: 0.69s\n",
      "Epoch: 50, Batch: 281/842, Loss: 0.4169, Time: 0.69s\n",
      "Epoch: 50, Batch: 291/842, Loss: 0.4204, Time: 0.69s\n",
      "Epoch: 50, Batch: 301/842, Loss: 0.5046, Time: 0.69s\n",
      "Epoch: 50, Batch: 311/842, Loss: 0.4250, Time: 0.73s\n",
      "Epoch: 50, Batch: 321/842, Loss: 0.4279, Time: 0.73s\n",
      "Epoch: 50, Batch: 331/842, Loss: 0.4208, Time: 0.73s\n",
      "Epoch: 50, Batch: 341/842, Loss: 0.5227, Time: 0.74s\n",
      "Epoch: 50, Batch: 351/842, Loss: 0.6064, Time: 0.73s\n",
      "Epoch: 50, Batch: 361/842, Loss: 0.5383, Time: 0.73s\n",
      "Epoch: 50, Batch: 371/842, Loss: 0.6215, Time: 0.74s\n",
      "Epoch: 50, Batch: 381/842, Loss: 0.4982, Time: 0.75s\n",
      "Epoch: 50, Batch: 391/842, Loss: 0.6096, Time: 0.73s\n",
      "Epoch: 50, Batch: 401/842, Loss: 0.4637, Time: 0.73s\n",
      "Epoch: 50, Batch: 411/842, Loss: 0.3792, Time: 0.73s\n",
      "Epoch: 50, Batch: 421/842, Loss: 0.5383, Time: 0.72s\n",
      "Epoch: 50, Batch: 431/842, Loss: 0.3586, Time: 0.74s\n",
      "Epoch: 50, Batch: 441/842, Loss: 0.3714, Time: 0.74s\n",
      "Epoch: 50, Batch: 451/842, Loss: 0.4341, Time: 0.72s\n",
      "Epoch: 50, Batch: 461/842, Loss: 0.5683, Time: 0.73s\n",
      "Epoch: 50, Batch: 471/842, Loss: 0.4374, Time: 0.72s\n",
      "Epoch: 50, Batch: 481/842, Loss: 0.4599, Time: 0.73s\n",
      "Epoch: 50, Batch: 491/842, Loss: 0.4184, Time: 0.72s\n",
      "Epoch: 50, Batch: 501/842, Loss: 0.4853, Time: 0.73s\n",
      "Epoch: 50, Batch: 511/842, Loss: 0.4527, Time: 0.74s\n",
      "Epoch: 50, Batch: 521/842, Loss: 0.4511, Time: 0.73s\n",
      "Epoch: 50, Batch: 531/842, Loss: 0.4137, Time: 0.73s\n",
      "Epoch: 50, Batch: 541/842, Loss: 0.4486, Time: 0.73s\n",
      "Epoch: 50, Batch: 551/842, Loss: 0.3662, Time: 0.73s\n",
      "Epoch: 50, Batch: 561/842, Loss: 0.4478, Time: 0.73s\n",
      "Epoch: 50, Batch: 571/842, Loss: 0.5009, Time: 0.72s\n",
      "Epoch: 50, Batch: 581/842, Loss: 0.4758, Time: 0.72s\n",
      "Epoch: 50, Batch: 591/842, Loss: 0.4268, Time: 0.73s\n",
      "Epoch: 50, Batch: 601/842, Loss: 0.5834, Time: 0.72s\n",
      "Epoch: 50, Batch: 611/842, Loss: 0.4242, Time: 0.72s\n",
      "Epoch: 50, Batch: 621/842, Loss: 0.5766, Time: 0.72s\n",
      "Epoch: 50, Batch: 631/842, Loss: 0.3944, Time: 0.72s\n",
      "Epoch: 50, Batch: 641/842, Loss: 0.6157, Time: 0.72s\n",
      "Epoch: 50, Batch: 651/842, Loss: 0.4479, Time: 0.72s\n",
      "Epoch: 50, Batch: 661/842, Loss: 0.4389, Time: 0.71s\n",
      "Epoch: 50, Batch: 671/842, Loss: 0.4316, Time: 0.72s\n",
      "Epoch: 50, Batch: 681/842, Loss: 0.4995, Time: 0.73s\n",
      "Epoch: 50, Batch: 691/842, Loss: 0.5227, Time: 0.72s\n",
      "Epoch: 50, Batch: 701/842, Loss: 0.4952, Time: 0.71s\n",
      "Epoch: 50, Batch: 711/842, Loss: 0.4906, Time: 0.71s\n",
      "Epoch: 50, Batch: 721/842, Loss: 0.5179, Time: 0.71s\n",
      "Epoch: 50, Batch: 731/842, Loss: 0.5168, Time: 0.71s\n",
      "Epoch: 50, Batch: 741/842, Loss: 0.4834, Time: 0.71s\n",
      "Epoch: 50, Batch: 751/842, Loss: 0.4892, Time: 0.72s\n",
      "Epoch: 50, Batch: 761/842, Loss: 0.4898, Time: 0.72s\n",
      "Epoch: 50, Batch: 771/842, Loss: 0.3981, Time: 0.74s\n",
      "Epoch: 50, Batch: 781/842, Loss: 0.5344, Time: 0.73s\n",
      "Epoch: 50, Batch: 791/842, Loss: 0.4486, Time: 0.75s\n",
      "Epoch: 50, Batch: 801/842, Loss: 0.5762, Time: 0.74s\n",
      "Epoch: 50, Batch: 811/842, Loss: 0.4747, Time: 0.74s\n",
      "Epoch: 50, Batch: 821/842, Loss: 0.5824, Time: 0.74s\n",
      "Epoch: 50, Batch: 831/842, Loss: 0.5922, Time: 0.74s\n",
      "Epoch: 50, Batch: 841/842, Loss: 0.4238, Time: 0.71s\n",
      "Epoch 50/100: Train Loss: 0.4666, Val Loss: 0.4360, mIoU: 0.6856, F1: 0.8069, OA: 0.9646\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7782, F1=0.8752, OA=0.8723, Precision=0.8274, Recall=0.9290\n",
      "    Class 1: IoU=0.7040, F1=0.8263, OA=0.9964, Precision=0.8566, Recall=0.7980\n",
      "    Class 2: IoU=0.9109, F1=0.9534, OA=0.9812, Precision=0.9590, Recall=0.9479\n",
      "    Class 3: IoU=0.4725, F1=0.6418, OA=0.9641, Precision=0.8157, Recall=0.5290\n",
      "    Class 4: IoU=0.7453, F1=0.8541, OA=0.9804, Precision=0.8661, Recall=0.8424\n",
      "    Class 5: IoU=0.6585, F1=0.7941, OA=0.9963, Precision=0.7789, Recall=0.8099\n",
      "    Class 6: IoU=0.5675, F1=0.7241, OA=0.9579, Precision=0.8380, Recall=0.6375\n",
      "    Class 7: IoU=0.6478, F1=0.7862, OA=0.9685, Precision=0.8594, Recall=0.7245\n",
      "Epoch: 51, Batch: 1/842, Loss: 0.4420, Time: 1.58s\n",
      "Epoch: 51, Batch: 11/842, Loss: 0.4797, Time: 0.78s\n",
      "Epoch: 51, Batch: 21/842, Loss: 0.4796, Time: 0.81s\n",
      "Epoch: 51, Batch: 31/842, Loss: 0.6829, Time: 0.81s\n",
      "Epoch: 51, Batch: 41/842, Loss: 0.4796, Time: 0.77s\n",
      "Epoch: 51, Batch: 51/842, Loss: 0.5059, Time: 0.79s\n",
      "Epoch: 51, Batch: 61/842, Loss: 0.3363, Time: 0.81s\n",
      "Epoch: 51, Batch: 71/842, Loss: 0.2995, Time: 0.78s\n",
      "Epoch: 51, Batch: 81/842, Loss: 0.3786, Time: 0.77s\n",
      "Epoch: 51, Batch: 91/842, Loss: 0.5832, Time: 0.78s\n",
      "Epoch: 51, Batch: 101/842, Loss: 0.4145, Time: 0.85s\n",
      "Epoch: 51, Batch: 111/842, Loss: 0.4478, Time: 0.74s\n",
      "Epoch: 51, Batch: 121/842, Loss: 0.5800, Time: 0.72s\n",
      "Epoch: 51, Batch: 131/842, Loss: 0.5965, Time: 0.72s\n",
      "Epoch: 51, Batch: 141/842, Loss: 0.4379, Time: 0.73s\n",
      "Epoch: 51, Batch: 151/842, Loss: 0.3840, Time: 0.72s\n",
      "Epoch: 51, Batch: 161/842, Loss: 0.6160, Time: 0.73s\n",
      "Epoch: 51, Batch: 171/842, Loss: 0.5861, Time: 0.72s\n",
      "Epoch: 51, Batch: 181/842, Loss: 0.4222, Time: 0.72s\n",
      "Epoch: 51, Batch: 191/842, Loss: 0.4657, Time: 0.72s\n",
      "Epoch: 51, Batch: 201/842, Loss: 0.5014, Time: 0.72s\n",
      "Epoch: 51, Batch: 211/842, Loss: 0.3364, Time: 0.72s\n",
      "Epoch: 51, Batch: 221/842, Loss: 0.3804, Time: 0.71s\n",
      "Epoch: 51, Batch: 231/842, Loss: 0.5122, Time: 0.72s\n",
      "Epoch: 51, Batch: 241/842, Loss: 0.3593, Time: 0.72s\n",
      "Epoch: 51, Batch: 251/842, Loss: 0.4723, Time: 0.71s\n",
      "Epoch: 51, Batch: 261/842, Loss: 0.4346, Time: 0.73s\n",
      "Epoch: 51, Batch: 271/842, Loss: 0.5611, Time: 0.74s\n",
      "Epoch: 51, Batch: 281/842, Loss: 0.5067, Time: 0.75s\n",
      "Epoch: 51, Batch: 291/842, Loss: 0.4897, Time: 0.74s\n",
      "Epoch: 51, Batch: 301/842, Loss: 0.4270, Time: 0.75s\n",
      "Epoch: 51, Batch: 311/842, Loss: 0.4382, Time: 0.74s\n",
      "Epoch: 51, Batch: 321/842, Loss: 0.3798, Time: 0.74s\n",
      "Epoch: 51, Batch: 331/842, Loss: 0.5153, Time: 0.74s\n",
      "Epoch: 51, Batch: 341/842, Loss: 0.3861, Time: 0.75s\n",
      "Epoch: 51, Batch: 351/842, Loss: 0.4390, Time: 0.73s\n",
      "Epoch: 51, Batch: 361/842, Loss: 0.4457, Time: 0.75s\n",
      "Epoch: 51, Batch: 371/842, Loss: 0.5948, Time: 0.75s\n",
      "Epoch: 51, Batch: 381/842, Loss: 0.4519, Time: 0.74s\n",
      "Epoch: 51, Batch: 391/842, Loss: 0.4407, Time: 0.74s\n",
      "Epoch: 51, Batch: 401/842, Loss: 0.5098, Time: 0.74s\n",
      "Epoch: 51, Batch: 411/842, Loss: 0.4438, Time: 0.74s\n",
      "Epoch: 51, Batch: 421/842, Loss: 0.3909, Time: 0.74s\n",
      "Epoch: 51, Batch: 431/842, Loss: 0.3675, Time: 0.74s\n",
      "Epoch: 51, Batch: 441/842, Loss: 0.4602, Time: 0.74s\n",
      "Epoch: 51, Batch: 451/842, Loss: 0.2906, Time: 0.74s\n",
      "Epoch: 51, Batch: 461/842, Loss: 0.3528, Time: 0.74s\n",
      "Epoch: 51, Batch: 471/842, Loss: 0.5256, Time: 0.76s\n",
      "Epoch: 51, Batch: 481/842, Loss: 0.5837, Time: 0.74s\n",
      "Epoch: 51, Batch: 491/842, Loss: 0.4113, Time: 0.74s\n",
      "Epoch: 51, Batch: 501/842, Loss: 0.5337, Time: 0.74s\n",
      "Epoch: 51, Batch: 511/842, Loss: 0.4213, Time: 0.72s\n",
      "Epoch: 51, Batch: 521/842, Loss: 0.4874, Time: 0.73s\n",
      "Epoch: 51, Batch: 531/842, Loss: 0.4841, Time: 0.73s\n",
      "Epoch: 51, Batch: 541/842, Loss: 0.4008, Time: 0.73s\n",
      "Epoch: 51, Batch: 551/842, Loss: 0.5162, Time: 0.72s\n",
      "Epoch: 51, Batch: 561/842, Loss: 0.4251, Time: 0.74s\n",
      "Epoch: 51, Batch: 571/842, Loss: 0.4296, Time: 0.73s\n",
      "Epoch: 51, Batch: 581/842, Loss: 0.4216, Time: 0.73s\n",
      "Epoch: 51, Batch: 591/842, Loss: 0.4331, Time: 0.73s\n",
      "Epoch: 51, Batch: 601/842, Loss: 0.5456, Time: 0.74s\n",
      "Epoch: 51, Batch: 611/842, Loss: 0.3544, Time: 0.73s\n",
      "Epoch: 51, Batch: 621/842, Loss: 0.5664, Time: 0.72s\n",
      "Epoch: 51, Batch: 631/842, Loss: 0.5179, Time: 0.73s\n",
      "Epoch: 51, Batch: 641/842, Loss: 0.4815, Time: 0.73s\n",
      "Epoch: 51, Batch: 651/842, Loss: 0.4973, Time: 0.73s\n",
      "Epoch: 51, Batch: 661/842, Loss: 0.5582, Time: 0.75s\n",
      "Epoch: 51, Batch: 671/842, Loss: 0.5278, Time: 0.75s\n",
      "Epoch: 51, Batch: 681/842, Loss: 0.4846, Time: 0.74s\n",
      "Epoch: 51, Batch: 691/842, Loss: 0.4088, Time: 0.74s\n",
      "Epoch: 51, Batch: 701/842, Loss: 0.4158, Time: 0.76s\n",
      "Epoch: 51, Batch: 711/842, Loss: 0.4469, Time: 0.77s\n",
      "Epoch: 51, Batch: 721/842, Loss: 0.4760, Time: 0.78s\n",
      "Epoch: 51, Batch: 731/842, Loss: 0.3875, Time: 0.77s\n",
      "Epoch: 51, Batch: 741/842, Loss: 0.5321, Time: 0.78s\n",
      "Epoch: 51, Batch: 751/842, Loss: 0.4227, Time: 0.80s\n",
      "Epoch: 51, Batch: 761/842, Loss: 0.5424, Time: 0.75s\n",
      "Epoch: 51, Batch: 771/842, Loss: 0.4760, Time: 0.76s\n",
      "Epoch: 51, Batch: 781/842, Loss: 0.4740, Time: 0.74s\n",
      "Epoch: 51, Batch: 791/842, Loss: 0.4201, Time: 0.74s\n",
      "Epoch: 51, Batch: 801/842, Loss: 0.3220, Time: 0.77s\n",
      "Epoch: 51, Batch: 811/842, Loss: 0.5589, Time: 0.74s\n",
      "Epoch: 51, Batch: 821/842, Loss: 0.4477, Time: 0.73s\n",
      "Epoch: 51, Batch: 831/842, Loss: 0.5125, Time: 0.73s\n",
      "Epoch: 51, Batch: 841/842, Loss: 0.4758, Time: 0.71s\n",
      "Epoch 51/100: Train Loss: 0.4657, Val Loss: 0.4452, mIoU: 0.6820, F1: 0.8042, OA: 0.9637\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7738, F1=0.8725, OA=0.8688, Precision=0.8208, Recall=0.9311\n",
      "    Class 1: IoU=0.7020, F1=0.8249, OA=0.9964, Precision=0.8549, Recall=0.7969\n",
      "    Class 2: IoU=0.9105, F1=0.9532, OA=0.9812, Precision=0.9605, Recall=0.9459\n",
      "    Class 3: IoU=0.4704, F1=0.6399, OA=0.9619, Precision=0.7516, Recall=0.5571\n",
      "    Class 4: IoU=0.7423, F1=0.8521, OA=0.9805, Precision=0.8839, Recall=0.8225\n",
      "    Class 5: IoU=0.6730, F1=0.8045, OA=0.9966, Precision=0.8358, Recall=0.7756\n",
      "    Class 6: IoU=0.5653, F1=0.7223, OA=0.9572, Precision=0.8246, Recall=0.6426\n",
      "    Class 7: IoU=0.6183, F1=0.7641, OA=0.9671, Precision=0.8979, Recall=0.6650\n",
      "Epoch: 52, Batch: 1/842, Loss: 0.3100, Time: 1.69s\n",
      "Epoch: 52, Batch: 11/842, Loss: 0.6541, Time: 0.73s\n",
      "Epoch: 52, Batch: 21/842, Loss: 0.3640, Time: 0.73s\n",
      "Epoch: 52, Batch: 31/842, Loss: 0.4298, Time: 0.72s\n",
      "Epoch: 52, Batch: 41/842, Loss: 0.4080, Time: 0.73s\n",
      "Epoch: 52, Batch: 51/842, Loss: 0.5587, Time: 0.73s\n",
      "Epoch: 52, Batch: 61/842, Loss: 0.5955, Time: 0.74s\n",
      "Epoch: 52, Batch: 71/842, Loss: 0.5027, Time: 0.73s\n",
      "Epoch: 52, Batch: 81/842, Loss: 0.4348, Time: 0.73s\n",
      "Epoch: 52, Batch: 91/842, Loss: 0.5053, Time: 0.73s\n",
      "Epoch: 52, Batch: 101/842, Loss: 0.4142, Time: 0.72s\n",
      "Epoch: 52, Batch: 111/842, Loss: 0.3468, Time: 0.73s\n",
      "Epoch: 52, Batch: 121/842, Loss: 0.3706, Time: 0.73s\n",
      "Epoch: 52, Batch: 131/842, Loss: 0.3892, Time: 0.73s\n",
      "Epoch: 52, Batch: 141/842, Loss: 0.4006, Time: 0.72s\n",
      "Epoch: 52, Batch: 151/842, Loss: 0.5884, Time: 0.73s\n",
      "Epoch: 52, Batch: 161/842, Loss: 0.5165, Time: 0.74s\n",
      "Epoch: 52, Batch: 171/842, Loss: 0.4328, Time: 0.74s\n",
      "Epoch: 52, Batch: 181/842, Loss: 0.4580, Time: 0.74s\n",
      "Epoch: 52, Batch: 191/842, Loss: 0.4241, Time: 0.73s\n",
      "Epoch: 52, Batch: 201/842, Loss: 0.3283, Time: 0.76s\n",
      "Epoch: 52, Batch: 211/842, Loss: 0.4223, Time: 0.76s\n",
      "Epoch: 52, Batch: 221/842, Loss: 0.5263, Time: 0.73s\n",
      "Epoch: 52, Batch: 231/842, Loss: 0.5392, Time: 1.36s\n",
      "Epoch: 52, Batch: 241/842, Loss: 0.5575, Time: 0.84s\n",
      "Epoch: 52, Batch: 251/842, Loss: 0.6350, Time: 0.84s\n",
      "Epoch: 52, Batch: 261/842, Loss: 0.4860, Time: 0.87s\n",
      "Epoch: 52, Batch: 271/842, Loss: 0.6065, Time: 0.87s\n",
      "Epoch: 52, Batch: 281/842, Loss: 0.5551, Time: 0.89s\n",
      "Epoch: 52, Batch: 291/842, Loss: 0.5304, Time: 0.84s\n",
      "Epoch: 52, Batch: 301/842, Loss: 0.4196, Time: 0.85s\n",
      "Epoch: 52, Batch: 311/842, Loss: 0.5166, Time: 0.73s\n",
      "Epoch: 52, Batch: 321/842, Loss: 0.3818, Time: 0.73s\n",
      "Epoch: 52, Batch: 331/842, Loss: 0.4108, Time: 0.74s\n",
      "Epoch: 52, Batch: 341/842, Loss: 0.3522, Time: 0.77s\n",
      "Epoch: 52, Batch: 351/842, Loss: 0.4343, Time: 0.74s\n",
      "Epoch: 52, Batch: 361/842, Loss: 0.5421, Time: 0.74s\n",
      "Epoch: 52, Batch: 371/842, Loss: 0.5096, Time: 0.74s\n",
      "Epoch: 52, Batch: 381/842, Loss: 0.5521, Time: 0.74s\n",
      "Epoch: 52, Batch: 391/842, Loss: 0.5309, Time: 0.74s\n",
      "Epoch: 52, Batch: 401/842, Loss: 0.2884, Time: 0.72s\n",
      "Epoch: 52, Batch: 411/842, Loss: 0.4941, Time: 0.73s\n",
      "Epoch: 52, Batch: 421/842, Loss: 0.4165, Time: 0.74s\n",
      "Epoch: 52, Batch: 431/842, Loss: 0.4166, Time: 0.75s\n",
      "Epoch: 52, Batch: 441/842, Loss: 0.4932, Time: 0.75s\n",
      "Epoch: 52, Batch: 451/842, Loss: 0.5226, Time: 0.76s\n",
      "Epoch: 52, Batch: 461/842, Loss: 0.4714, Time: 0.77s\n",
      "Epoch: 52, Batch: 471/842, Loss: 0.4987, Time: 0.78s\n",
      "Epoch: 52, Batch: 481/842, Loss: 0.4745, Time: 0.76s\n",
      "Epoch: 52, Batch: 491/842, Loss: 0.4760, Time: 0.76s\n",
      "Epoch: 52, Batch: 501/842, Loss: 0.4817, Time: 0.76s\n",
      "Epoch: 52, Batch: 511/842, Loss: 0.4988, Time: 0.75s\n",
      "Epoch: 52, Batch: 521/842, Loss: 0.5097, Time: 0.75s\n",
      "Epoch: 52, Batch: 531/842, Loss: 0.5009, Time: 0.73s\n",
      "Epoch: 52, Batch: 541/842, Loss: 0.3885, Time: 0.69s\n",
      "Epoch: 52, Batch: 551/842, Loss: 0.4460, Time: 0.71s\n",
      "Epoch: 52, Batch: 561/842, Loss: 0.4261, Time: 0.71s\n",
      "Epoch: 52, Batch: 571/842, Loss: 0.6759, Time: 0.71s\n",
      "Epoch: 52, Batch: 581/842, Loss: 0.4284, Time: 0.68s\n",
      "Epoch: 52, Batch: 591/842, Loss: 0.4303, Time: 0.73s\n",
      "Epoch: 52, Batch: 601/842, Loss: 0.4600, Time: 0.75s\n",
      "Epoch: 52, Batch: 611/842, Loss: 0.5366, Time: 0.73s\n",
      "Epoch: 52, Batch: 621/842, Loss: 0.5548, Time: 0.73s\n",
      "Epoch: 52, Batch: 631/842, Loss: 0.4230, Time: 0.74s\n",
      "Epoch: 52, Batch: 641/842, Loss: 0.4659, Time: 0.75s\n",
      "Epoch: 52, Batch: 651/842, Loss: 0.5579, Time: 0.74s\n",
      "Epoch: 52, Batch: 661/842, Loss: 0.4708, Time: 0.73s\n",
      "Epoch: 52, Batch: 671/842, Loss: 0.3924, Time: 0.79s\n",
      "Epoch: 52, Batch: 681/842, Loss: 0.4175, Time: 0.78s\n",
      "Epoch: 52, Batch: 691/842, Loss: 0.4508, Time: 0.76s\n",
      "Epoch: 52, Batch: 701/842, Loss: 0.5081, Time: 0.75s\n",
      "Epoch: 52, Batch: 711/842, Loss: 0.4397, Time: 0.75s\n",
      "Epoch: 52, Batch: 721/842, Loss: 0.4921, Time: 0.76s\n",
      "Epoch: 52, Batch: 731/842, Loss: 0.3328, Time: 0.74s\n",
      "Epoch: 52, Batch: 741/842, Loss: 0.4340, Time: 0.74s\n",
      "Epoch: 52, Batch: 751/842, Loss: 0.3953, Time: 0.74s\n",
      "Epoch: 52, Batch: 761/842, Loss: 0.5200, Time: 0.76s\n",
      "Epoch: 52, Batch: 771/842, Loss: 0.4797, Time: 0.75s\n",
      "Epoch: 52, Batch: 781/842, Loss: 0.4507, Time: 0.74s\n",
      "Epoch: 52, Batch: 791/842, Loss: 0.2961, Time: 0.74s\n",
      "Epoch: 52, Batch: 801/842, Loss: 0.4738, Time: 0.75s\n",
      "Epoch: 52, Batch: 811/842, Loss: 0.4883, Time: 0.74s\n",
      "Epoch: 52, Batch: 821/842, Loss: 0.4631, Time: 0.74s\n",
      "Epoch: 52, Batch: 831/842, Loss: 0.3185, Time: 0.74s\n",
      "Epoch: 52, Batch: 841/842, Loss: 0.6530, Time: 0.73s\n",
      "Epoch 52/100: Train Loss: 0.4664, Val Loss: 0.4324, mIoU: 0.6891, F1: 0.8090, OA: 0.9650\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7795, F1=0.8761, OA=0.8731, Precision=0.8277, Recall=0.9305\n",
      "    Class 1: IoU=0.7119, F1=0.8317, OA=0.9966, Precision=0.8716, Recall=0.7953\n",
      "    Class 2: IoU=0.9121, F1=0.9540, OA=0.9813, Precision=0.9532, Recall=0.9548\n",
      "    Class 3: IoU=0.4591, F1=0.6293, OA=0.9643, Precision=0.8517, Recall=0.4990\n",
      "    Class 4: IoU=0.7507, F1=0.8576, OA=0.9812, Precision=0.8855, Recall=0.8314\n",
      "    Class 5: IoU=0.6745, F1=0.8056, OA=0.9965, Precision=0.8004, Recall=0.8109\n",
      "    Class 6: IoU=0.5735, F1=0.7290, OA=0.9580, Precision=0.8269, Recall=0.6518\n",
      "    Class 7: IoU=0.6512, F1=0.7887, OA=0.9687, Precision=0.8561, Recall=0.7312\n",
      "Saving best model with mIoU: 0.6891\n",
      "Epoch: 53, Batch: 1/842, Loss: 0.4867, Time: 1.55s\n",
      "Epoch: 53, Batch: 11/842, Loss: 0.3969, Time: 0.74s\n",
      "Epoch: 53, Batch: 21/842, Loss: 0.3567, Time: 0.73s\n",
      "Epoch: 53, Batch: 31/842, Loss: 0.5393, Time: 0.71s\n",
      "Epoch: 53, Batch: 41/842, Loss: 0.4675, Time: 0.72s\n",
      "Epoch: 53, Batch: 51/842, Loss: 0.4697, Time: 0.71s\n",
      "Epoch: 53, Batch: 61/842, Loss: 0.4366, Time: 0.73s\n",
      "Epoch: 53, Batch: 71/842, Loss: 0.5535, Time: 0.73s\n",
      "Epoch: 53, Batch: 81/842, Loss: 0.4981, Time: 0.72s\n",
      "Epoch: 53, Batch: 91/842, Loss: 0.4667, Time: 0.73s\n",
      "Epoch: 53, Batch: 101/842, Loss: 0.3786, Time: 0.73s\n",
      "Epoch: 53, Batch: 111/842, Loss: 0.3759, Time: 0.71s\n",
      "Epoch: 53, Batch: 121/842, Loss: 0.4322, Time: 0.73s\n",
      "Epoch: 53, Batch: 131/842, Loss: 0.3070, Time: 0.71s\n",
      "Epoch: 53, Batch: 141/842, Loss: 0.4605, Time: 0.72s\n",
      "Epoch: 53, Batch: 151/842, Loss: 0.5241, Time: 0.72s\n",
      "Epoch: 53, Batch: 161/842, Loss: 0.4377, Time: 0.69s\n",
      "Epoch: 53, Batch: 171/842, Loss: 0.3928, Time: 0.71s\n",
      "Epoch: 53, Batch: 181/842, Loss: 0.3781, Time: 0.72s\n",
      "Epoch: 53, Batch: 191/842, Loss: 0.5638, Time: 0.72s\n",
      "Epoch: 53, Batch: 201/842, Loss: 0.5628, Time: 0.72s\n",
      "Epoch: 53, Batch: 211/842, Loss: 0.3890, Time: 0.73s\n",
      "Epoch: 53, Batch: 221/842, Loss: 0.4897, Time: 0.74s\n",
      "Epoch: 53, Batch: 231/842, Loss: 0.4849, Time: 0.73s\n",
      "Epoch: 53, Batch: 241/842, Loss: 0.5095, Time: 0.73s\n",
      "Epoch: 53, Batch: 251/842, Loss: 0.5507, Time: 0.73s\n",
      "Epoch: 53, Batch: 261/842, Loss: 0.4293, Time: 0.73s\n",
      "Epoch: 53, Batch: 271/842, Loss: 0.4469, Time: 0.70s\n",
      "Epoch: 53, Batch: 281/842, Loss: 0.5672, Time: 0.73s\n",
      "Epoch: 53, Batch: 291/842, Loss: 0.3822, Time: 0.73s\n",
      "Epoch: 53, Batch: 301/842, Loss: 0.6197, Time: 0.72s\n",
      "Epoch: 53, Batch: 311/842, Loss: 0.4485, Time: 0.73s\n",
      "Epoch: 53, Batch: 321/842, Loss: 0.4724, Time: 0.73s\n",
      "Epoch: 53, Batch: 331/842, Loss: 0.4416, Time: 0.73s\n",
      "Epoch: 53, Batch: 341/842, Loss: 0.3081, Time: 0.72s\n",
      "Epoch: 53, Batch: 351/842, Loss: 0.6592, Time: 0.74s\n",
      "Epoch: 53, Batch: 361/842, Loss: 0.4010, Time: 0.74s\n",
      "Epoch: 53, Batch: 371/842, Loss: 0.5451, Time: 0.74s\n",
      "Epoch: 53, Batch: 381/842, Loss: 0.5216, Time: 0.73s\n",
      "Epoch: 53, Batch: 391/842, Loss: 0.4334, Time: 0.75s\n",
      "Epoch: 53, Batch: 401/842, Loss: 0.5706, Time: 0.77s\n",
      "Epoch: 53, Batch: 411/842, Loss: 0.4879, Time: 0.79s\n",
      "Epoch: 53, Batch: 421/842, Loss: 0.3872, Time: 0.75s\n",
      "Epoch: 53, Batch: 431/842, Loss: 0.5468, Time: 0.70s\n",
      "Epoch: 53, Batch: 441/842, Loss: 0.5316, Time: 0.70s\n",
      "Epoch: 53, Batch: 451/842, Loss: 0.3878, Time: 0.69s\n",
      "Epoch: 53, Batch: 461/842, Loss: 0.5294, Time: 0.71s\n",
      "Epoch: 53, Batch: 471/842, Loss: 0.4148, Time: 0.71s\n",
      "Epoch: 53, Batch: 481/842, Loss: 0.4748, Time: 0.76s\n",
      "Epoch: 53, Batch: 491/842, Loss: 0.4219, Time: 0.74s\n",
      "Epoch: 53, Batch: 501/842, Loss: 0.4959, Time: 0.72s\n",
      "Epoch: 53, Batch: 511/842, Loss: 0.4162, Time: 0.74s\n",
      "Epoch: 53, Batch: 521/842, Loss: 0.3975, Time: 0.73s\n",
      "Epoch: 53, Batch: 531/842, Loss: 0.5120, Time: 0.73s\n",
      "Epoch: 53, Batch: 541/842, Loss: 0.5401, Time: 0.73s\n",
      "Epoch: 53, Batch: 551/842, Loss: 0.4068, Time: 0.72s\n",
      "Epoch: 53, Batch: 561/842, Loss: 0.5083, Time: 0.72s\n",
      "Epoch: 53, Batch: 571/842, Loss: 0.3448, Time: 0.72s\n",
      "Epoch: 53, Batch: 581/842, Loss: 0.4935, Time: 0.72s\n",
      "Epoch: 53, Batch: 591/842, Loss: 0.4356, Time: 0.72s\n",
      "Epoch: 53, Batch: 601/842, Loss: 0.4654, Time: 0.75s\n",
      "Epoch: 53, Batch: 611/842, Loss: 0.5321, Time: 0.74s\n",
      "Epoch: 53, Batch: 621/842, Loss: 0.5068, Time: 0.77s\n",
      "Epoch: 53, Batch: 631/842, Loss: 0.5921, Time: 0.73s\n",
      "Epoch: 53, Batch: 641/842, Loss: 0.3913, Time: 0.75s\n",
      "Epoch: 53, Batch: 651/842, Loss: 0.5111, Time: 0.72s\n",
      "Epoch: 53, Batch: 661/842, Loss: 0.5908, Time: 0.75s\n",
      "Epoch: 53, Batch: 671/842, Loss: 0.6600, Time: 0.73s\n",
      "Epoch: 53, Batch: 681/842, Loss: 0.3938, Time: 0.74s\n",
      "Epoch: 53, Batch: 691/842, Loss: 0.6256, Time: 0.72s\n",
      "Epoch: 53, Batch: 701/842, Loss: 0.4443, Time: 0.73s\n",
      "Epoch: 53, Batch: 711/842, Loss: 0.6265, Time: 0.73s\n",
      "Epoch: 53, Batch: 721/842, Loss: 0.4986, Time: 0.74s\n",
      "Epoch: 53, Batch: 731/842, Loss: 0.4758, Time: 0.73s\n",
      "Epoch: 53, Batch: 741/842, Loss: 0.3907, Time: 0.74s\n",
      "Epoch: 53, Batch: 751/842, Loss: 0.4663, Time: 0.73s\n",
      "Epoch: 53, Batch: 761/842, Loss: 0.3484, Time: 0.74s\n",
      "Epoch: 53, Batch: 771/842, Loss: 0.5541, Time: 0.68s\n",
      "Epoch: 53, Batch: 781/842, Loss: 0.4113, Time: 0.72s\n",
      "Epoch: 53, Batch: 791/842, Loss: 0.3720, Time: 0.72s\n",
      "Epoch: 53, Batch: 801/842, Loss: 0.4324, Time: 0.72s\n",
      "Epoch: 53, Batch: 811/842, Loss: 0.5605, Time: 0.74s\n",
      "Epoch: 53, Batch: 821/842, Loss: 0.4832, Time: 0.72s\n",
      "Epoch: 53, Batch: 831/842, Loss: 0.4155, Time: 0.71s\n",
      "Epoch: 53, Batch: 841/842, Loss: 0.4675, Time: 0.72s\n",
      "Epoch 53/100: Train Loss: 0.4638, Val Loss: 0.4384, mIoU: 0.6840, F1: 0.8059, OA: 0.9644\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7782, F1=0.8753, OA=0.8723, Precision=0.8273, Recall=0.9292\n",
      "    Class 1: IoU=0.6897, F1=0.8164, OA=0.9963, Precision=0.8514, Recall=0.7841\n",
      "    Class 2: IoU=0.9079, F1=0.9517, OA=0.9806, Precision=0.9617, Recall=0.9419\n",
      "    Class 3: IoU=0.4767, F1=0.6456, OA=0.9637, Precision=0.7942, Recall=0.5439\n",
      "    Class 4: IoU=0.7501, F1=0.8572, OA=0.9810, Precision=0.8808, Recall=0.8348\n",
      "    Class 5: IoU=0.6661, F1=0.7996, OA=0.9964, Precision=0.7995, Recall=0.7996\n",
      "    Class 6: IoU=0.5622, F1=0.7197, OA=0.9567, Precision=0.8209, Recall=0.6407\n",
      "    Class 7: IoU=0.6416, F1=0.7817, OA=0.9678, Precision=0.8551, Recall=0.7198\n",
      "Epoch: 54, Batch: 1/842, Loss: 0.5171, Time: 1.78s\n",
      "Epoch: 54, Batch: 11/842, Loss: 0.5901, Time: 0.74s\n",
      "Epoch: 54, Batch: 21/842, Loss: 0.5488, Time: 0.75s\n",
      "Epoch: 54, Batch: 31/842, Loss: 0.6065, Time: 0.75s\n",
      "Epoch: 54, Batch: 41/842, Loss: 0.4871, Time: 0.75s\n",
      "Epoch: 54, Batch: 51/842, Loss: 0.3887, Time: 0.73s\n",
      "Epoch: 54, Batch: 61/842, Loss: 0.4334, Time: 0.76s\n",
      "Epoch: 54, Batch: 71/842, Loss: 0.5523, Time: 0.75s\n",
      "Epoch: 54, Batch: 81/842, Loss: 0.5061, Time: 0.73s\n",
      "Epoch: 54, Batch: 91/842, Loss: 0.3790, Time: 0.73s\n",
      "Epoch: 54, Batch: 101/842, Loss: 0.4168, Time: 0.73s\n",
      "Epoch: 54, Batch: 111/842, Loss: 0.4709, Time: 0.73s\n",
      "Epoch: 54, Batch: 121/842, Loss: 0.3920, Time: 0.72s\n",
      "Epoch: 54, Batch: 131/842, Loss: 0.5657, Time: 0.72s\n",
      "Epoch: 54, Batch: 141/842, Loss: 0.4772, Time: 0.72s\n",
      "Epoch: 54, Batch: 151/842, Loss: 0.3886, Time: 0.72s\n",
      "Epoch: 54, Batch: 161/842, Loss: 0.5743, Time: 0.72s\n",
      "Epoch: 54, Batch: 171/842, Loss: 0.4328, Time: 0.73s\n",
      "Epoch: 54, Batch: 181/842, Loss: 0.4507, Time: 0.72s\n",
      "Epoch: 54, Batch: 191/842, Loss: 0.5158, Time: 0.74s\n",
      "Epoch: 54, Batch: 201/842, Loss: 0.4513, Time: 0.75s\n",
      "Epoch: 54, Batch: 211/842, Loss: 0.4980, Time: 0.74s\n",
      "Epoch: 54, Batch: 221/842, Loss: 0.4044, Time: 0.74s\n",
      "Epoch: 54, Batch: 231/842, Loss: 0.3692, Time: 0.73s\n",
      "Epoch: 54, Batch: 241/842, Loss: 0.4204, Time: 0.74s\n",
      "Epoch: 54, Batch: 251/842, Loss: 0.3911, Time: 0.77s\n",
      "Epoch: 54, Batch: 261/842, Loss: 0.6470, Time: 0.77s\n",
      "Epoch: 54, Batch: 271/842, Loss: 0.3648, Time: 0.77s\n",
      "Epoch: 54, Batch: 281/842, Loss: 0.3508, Time: 0.79s\n",
      "Epoch: 54, Batch: 291/842, Loss: 0.4394, Time: 0.76s\n",
      "Epoch: 54, Batch: 301/842, Loss: 0.5601, Time: 0.78s\n",
      "Epoch: 54, Batch: 311/842, Loss: 0.3875, Time: 0.79s\n",
      "Epoch: 54, Batch: 321/842, Loss: 0.5042, Time: 0.75s\n",
      "Epoch: 54, Batch: 331/842, Loss: 0.3570, Time: 0.75s\n",
      "Epoch: 54, Batch: 341/842, Loss: 0.5753, Time: 0.74s\n",
      "Epoch: 54, Batch: 351/842, Loss: 0.4293, Time: 0.74s\n",
      "Epoch: 54, Batch: 361/842, Loss: 0.5366, Time: 0.76s\n",
      "Epoch: 54, Batch: 371/842, Loss: 0.4329, Time: 0.68s\n",
      "Epoch: 54, Batch: 381/842, Loss: 0.4343, Time: 0.67s\n",
      "Epoch: 54, Batch: 391/842, Loss: 0.5194, Time: 0.74s\n",
      "Epoch: 54, Batch: 401/842, Loss: 0.4241, Time: 0.80s\n",
      "Epoch: 54, Batch: 411/842, Loss: 0.4775, Time: 0.78s\n",
      "Epoch: 54, Batch: 421/842, Loss: 0.3736, Time: 0.81s\n",
      "Epoch: 54, Batch: 431/842, Loss: 0.4416, Time: 0.78s\n",
      "Epoch: 54, Batch: 441/842, Loss: 0.3041, Time: 0.79s\n",
      "Epoch: 54, Batch: 451/842, Loss: 0.5206, Time: 0.77s\n",
      "Epoch: 54, Batch: 461/842, Loss: 0.4579, Time: 0.73s\n",
      "Epoch: 54, Batch: 471/842, Loss: 0.5329, Time: 0.72s\n",
      "Epoch: 54, Batch: 481/842, Loss: 0.3890, Time: 0.72s\n",
      "Epoch: 54, Batch: 491/842, Loss: 0.3715, Time: 0.77s\n",
      "Epoch: 54, Batch: 501/842, Loss: 0.4614, Time: 0.76s\n",
      "Epoch: 54, Batch: 511/842, Loss: 0.4273, Time: 0.74s\n",
      "Epoch: 54, Batch: 521/842, Loss: 0.5152, Time: 0.73s\n",
      "Epoch: 54, Batch: 531/842, Loss: 0.4264, Time: 0.74s\n",
      "Epoch: 54, Batch: 541/842, Loss: 0.4322, Time: 0.73s\n",
      "Epoch: 54, Batch: 551/842, Loss: 0.3716, Time: 0.72s\n",
      "Epoch: 54, Batch: 561/842, Loss: 0.3955, Time: 0.72s\n",
      "Epoch: 54, Batch: 571/842, Loss: 0.4713, Time: 0.72s\n",
      "Epoch: 54, Batch: 581/842, Loss: 0.5555, Time: 0.72s\n",
      "Epoch: 54, Batch: 591/842, Loss: 0.5009, Time: 0.72s\n",
      "Epoch: 54, Batch: 601/842, Loss: 0.4595, Time: 0.72s\n",
      "Epoch: 54, Batch: 611/842, Loss: 0.4099, Time: 0.71s\n",
      "Epoch: 54, Batch: 621/842, Loss: 0.4957, Time: 0.72s\n",
      "Epoch: 54, Batch: 631/842, Loss: 0.4612, Time: 0.72s\n",
      "Epoch: 54, Batch: 641/842, Loss: 0.4598, Time: 0.78s\n",
      "Epoch: 54, Batch: 651/842, Loss: 0.3930, Time: 0.76s\n",
      "Epoch: 54, Batch: 661/842, Loss: 0.3933, Time: 0.78s\n",
      "Epoch: 54, Batch: 671/842, Loss: 0.5220, Time: 0.78s\n",
      "Epoch: 54, Batch: 681/842, Loss: 0.4979, Time: 0.76s\n",
      "Epoch: 54, Batch: 691/842, Loss: 0.5589, Time: 0.74s\n",
      "Epoch: 54, Batch: 701/842, Loss: 0.4909, Time: 0.77s\n",
      "Epoch: 54, Batch: 711/842, Loss: 0.4862, Time: 0.75s\n",
      "Epoch: 54, Batch: 721/842, Loss: 0.5504, Time: 0.75s\n",
      "Epoch: 54, Batch: 731/842, Loss: 0.3454, Time: 0.76s\n",
      "Epoch: 54, Batch: 741/842, Loss: 0.4936, Time: 0.74s\n",
      "Epoch: 54, Batch: 751/842, Loss: 0.5079, Time: 0.72s\n",
      "Epoch: 54, Batch: 761/842, Loss: 0.5424, Time: 0.72s\n",
      "Epoch: 54, Batch: 771/842, Loss: 0.4860, Time: 0.71s\n",
      "Epoch: 54, Batch: 781/842, Loss: 0.4520, Time: 0.72s\n",
      "Epoch: 54, Batch: 791/842, Loss: 0.2937, Time: 0.71s\n",
      "Epoch: 54, Batch: 801/842, Loss: 0.5880, Time: 0.71s\n",
      "Epoch: 54, Batch: 811/842, Loss: 0.4675, Time: 0.75s\n",
      "Epoch: 54, Batch: 821/842, Loss: 0.3722, Time: 0.78s\n",
      "Epoch: 54, Batch: 831/842, Loss: 0.3813, Time: 0.72s\n",
      "Epoch: 54, Batch: 841/842, Loss: 0.4949, Time: 0.67s\n",
      "Epoch 54/100: Train Loss: 0.4630, Val Loss: 0.4372, mIoU: 0.6830, F1: 0.8051, OA: 0.9645\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7778, F1=0.8750, OA=0.8715, Precision=0.8236, Recall=0.9332\n",
      "    Class 1: IoU=0.7014, F1=0.8245, OA=0.9964, Precision=0.8614, Recall=0.7906\n",
      "    Class 2: IoU=0.9088, F1=0.9522, OA=0.9807, Precision=0.9567, Recall=0.9478\n",
      "    Class 3: IoU=0.4758, F1=0.6448, OA=0.9645, Precision=0.8237, Recall=0.5297\n",
      "    Class 4: IoU=0.7416, F1=0.8517, OA=0.9808, Precision=0.8989, Recall=0.8091\n",
      "    Class 5: IoU=0.6439, F1=0.7834, OA=0.9960, Precision=0.7538, Recall=0.8154\n",
      "    Class 6: IoU=0.5641, F1=0.7213, OA=0.9581, Precision=0.8515, Recall=0.6257\n",
      "    Class 7: IoU=0.6504, F1=0.7882, OA=0.9683, Precision=0.8483, Recall=0.7360\n",
      "Epoch: 55, Batch: 1/842, Loss: 0.4468, Time: 1.71s\n",
      "Epoch: 55, Batch: 11/842, Loss: 0.3821, Time: 0.73s\n",
      "Epoch: 55, Batch: 21/842, Loss: 0.3857, Time: 0.76s\n",
      "Epoch: 55, Batch: 31/842, Loss: 0.3792, Time: 0.74s\n",
      "Epoch: 55, Batch: 41/842, Loss: 0.4093, Time: 0.73s\n",
      "Epoch: 55, Batch: 51/842, Loss: 0.4229, Time: 0.72s\n",
      "Epoch: 55, Batch: 61/842, Loss: 0.4779, Time: 0.72s\n",
      "Epoch: 55, Batch: 71/842, Loss: 0.3969, Time: 0.73s\n",
      "Epoch: 55, Batch: 81/842, Loss: 0.5660, Time: 0.79s\n",
      "Epoch: 55, Batch: 91/842, Loss: 0.4452, Time: 0.80s\n",
      "Epoch: 55, Batch: 101/842, Loss: 0.4831, Time: 0.88s\n",
      "Epoch: 55, Batch: 111/842, Loss: 0.3505, Time: 0.91s\n",
      "Epoch: 55, Batch: 121/842, Loss: 0.3758, Time: 0.94s\n",
      "Epoch: 55, Batch: 131/842, Loss: 0.4965, Time: 0.85s\n",
      "Epoch: 55, Batch: 141/842, Loss: 0.4362, Time: 0.80s\n",
      "Epoch: 55, Batch: 151/842, Loss: 0.3653, Time: 0.82s\n",
      "Epoch: 55, Batch: 161/842, Loss: 0.5184, Time: 0.80s\n",
      "Epoch: 55, Batch: 171/842, Loss: 0.4591, Time: 0.94s\n",
      "Epoch: 55, Batch: 181/842, Loss: 0.3317, Time: 0.92s\n",
      "Epoch: 55, Batch: 191/842, Loss: 0.5317, Time: 0.80s\n",
      "Epoch: 55, Batch: 201/842, Loss: 0.4034, Time: 0.80s\n",
      "Epoch: 55, Batch: 211/842, Loss: 0.3954, Time: 0.82s\n",
      "Epoch: 55, Batch: 221/842, Loss: 0.5235, Time: 0.81s\n",
      "Epoch: 55, Batch: 231/842, Loss: 0.4273, Time: 0.80s\n",
      "Epoch: 55, Batch: 241/842, Loss: 0.4455, Time: 0.84s\n",
      "Epoch: 55, Batch: 251/842, Loss: 0.5458, Time: 0.79s\n",
      "Epoch: 55, Batch: 261/842, Loss: 0.4078, Time: 0.86s\n",
      "Epoch: 55, Batch: 271/842, Loss: 0.3822, Time: 0.89s\n",
      "Epoch: 55, Batch: 281/842, Loss: 0.3808, Time: 0.83s\n",
      "Epoch: 55, Batch: 291/842, Loss: 0.4609, Time: 0.82s\n",
      "Epoch: 55, Batch: 301/842, Loss: 0.4159, Time: 0.85s\n",
      "Epoch: 55, Batch: 311/842, Loss: 0.5582, Time: 0.86s\n",
      "Epoch: 55, Batch: 321/842, Loss: 0.4216, Time: 0.78s\n",
      "Epoch: 55, Batch: 331/842, Loss: 0.4910, Time: 0.79s\n",
      "Epoch: 55, Batch: 341/842, Loss: 0.4960, Time: 0.76s\n",
      "Epoch: 55, Batch: 351/842, Loss: 0.4120, Time: 0.75s\n",
      "Epoch: 55, Batch: 361/842, Loss: 0.4775, Time: 0.75s\n",
      "Epoch: 55, Batch: 371/842, Loss: 0.4827, Time: 0.75s\n",
      "Epoch: 55, Batch: 381/842, Loss: 0.3164, Time: 0.75s\n",
      "Epoch: 55, Batch: 391/842, Loss: 0.4058, Time: 0.77s\n",
      "Epoch: 55, Batch: 401/842, Loss: 0.4841, Time: 0.72s\n",
      "Epoch: 55, Batch: 411/842, Loss: 0.4393, Time: 0.73s\n",
      "Epoch: 55, Batch: 421/842, Loss: 0.4623, Time: 0.72s\n",
      "Epoch: 55, Batch: 431/842, Loss: 0.4289, Time: 0.73s\n",
      "Epoch: 55, Batch: 441/842, Loss: 0.5060, Time: 0.72s\n",
      "Epoch: 55, Batch: 451/842, Loss: 0.4155, Time: 0.72s\n",
      "Epoch: 55, Batch: 461/842, Loss: 0.5374, Time: 0.72s\n",
      "Epoch: 55, Batch: 471/842, Loss: 0.5266, Time: 0.73s\n",
      "Epoch: 55, Batch: 481/842, Loss: 0.3563, Time: 0.75s\n",
      "Epoch: 55, Batch: 491/842, Loss: 0.5227, Time: 0.78s\n",
      "Epoch: 55, Batch: 501/842, Loss: 0.5567, Time: 0.79s\n",
      "Epoch: 55, Batch: 511/842, Loss: 0.3275, Time: 0.80s\n",
      "Epoch: 55, Batch: 521/842, Loss: 0.3909, Time: 0.79s\n",
      "Epoch: 55, Batch: 531/842, Loss: 0.4753, Time: 0.82s\n",
      "Epoch: 55, Batch: 541/842, Loss: 0.5026, Time: 0.83s\n",
      "Epoch: 55, Batch: 551/842, Loss: 0.5158, Time: 0.79s\n",
      "Epoch: 55, Batch: 561/842, Loss: 0.5754, Time: 0.81s\n",
      "Epoch: 55, Batch: 571/842, Loss: 0.5064, Time: 0.79s\n",
      "Epoch: 55, Batch: 581/842, Loss: 0.4299, Time: 0.79s\n",
      "Epoch: 55, Batch: 591/842, Loss: 0.6236, Time: 0.81s\n",
      "Epoch: 55, Batch: 601/842, Loss: 0.5689, Time: 0.78s\n",
      "Epoch: 55, Batch: 611/842, Loss: 0.4468, Time: 0.80s\n",
      "Epoch: 55, Batch: 621/842, Loss: 0.4838, Time: 0.79s\n",
      "Epoch: 55, Batch: 631/842, Loss: 0.4381, Time: 0.81s\n",
      "Epoch: 55, Batch: 641/842, Loss: 0.4120, Time: 0.81s\n",
      "Epoch: 55, Batch: 651/842, Loss: 0.4459, Time: 1.45s\n",
      "Epoch: 55, Batch: 661/842, Loss: 0.3977, Time: 0.80s\n",
      "Epoch: 55, Batch: 671/842, Loss: 0.4155, Time: 0.79s\n",
      "Epoch: 55, Batch: 681/842, Loss: 0.5330, Time: 0.85s\n",
      "Epoch: 55, Batch: 691/842, Loss: 0.3571, Time: 0.83s\n",
      "Epoch: 55, Batch: 701/842, Loss: 0.6208, Time: 0.84s\n",
      "Epoch: 55, Batch: 711/842, Loss: 0.6195, Time: 0.89s\n",
      "Epoch: 55, Batch: 721/842, Loss: 0.4221, Time: 0.82s\n",
      "Epoch: 55, Batch: 731/842, Loss: 0.3428, Time: 0.82s\n",
      "Epoch: 55, Batch: 741/842, Loss: 0.5431, Time: 0.81s\n",
      "Epoch: 55, Batch: 751/842, Loss: 0.5710, Time: 0.82s\n",
      "Epoch: 55, Batch: 761/842, Loss: 0.3916, Time: 0.82s\n",
      "Epoch: 55, Batch: 771/842, Loss: 0.5774, Time: 0.86s\n",
      "Epoch: 55, Batch: 781/842, Loss: 0.5855, Time: 0.84s\n",
      "Epoch: 55, Batch: 791/842, Loss: 0.4583, Time: 0.81s\n",
      "Epoch: 55, Batch: 801/842, Loss: 0.6238, Time: 0.82s\n",
      "Epoch: 55, Batch: 811/842, Loss: 0.3414, Time: 0.82s\n",
      "Epoch: 55, Batch: 821/842, Loss: 0.3943, Time: 0.81s\n",
      "Epoch: 55, Batch: 831/842, Loss: 0.3069, Time: 0.86s\n",
      "Epoch: 55, Batch: 841/842, Loss: 0.4403, Time: 0.81s\n",
      "Epoch 55/100: Train Loss: 0.4653, Val Loss: 0.4388, mIoU: 0.6838, F1: 0.8056, OA: 0.9642\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7769, F1=0.8744, OA=0.8705, Precision=0.8212, Recall=0.9351\n",
      "    Class 1: IoU=0.7009, F1=0.8242, OA=0.9964, Precision=0.8671, Recall=0.7852\n",
      "    Class 2: IoU=0.9091, F1=0.9524, OA=0.9809, Precision=0.9610, Recall=0.9440\n",
      "    Class 3: IoU=0.4774, F1=0.6463, OA=0.9636, Precision=0.7909, Recall=0.5464\n",
      "    Class 4: IoU=0.7521, F1=0.8585, OA=0.9813, Precision=0.8873, Recall=0.8315\n",
      "    Class 5: IoU=0.6659, F1=0.7994, OA=0.9964, Precision=0.7869, Recall=0.8123\n",
      "    Class 6: IoU=0.5614, F1=0.7191, OA=0.9574, Precision=0.8405, Recall=0.6284\n",
      "    Class 7: IoU=0.6268, F1=0.7706, OA=0.9673, Precision=0.8796, Recall=0.6856\n",
      "Epoch: 56, Batch: 1/842, Loss: 0.4687, Time: 1.69s\n",
      "Epoch: 56, Batch: 11/842, Loss: 0.5479, Time: 0.77s\n",
      "Epoch: 56, Batch: 21/842, Loss: 0.6108, Time: 0.79s\n",
      "Epoch: 56, Batch: 31/842, Loss: 0.4966, Time: 0.76s\n",
      "Epoch: 56, Batch: 41/842, Loss: 0.6481, Time: 0.76s\n",
      "Epoch: 56, Batch: 51/842, Loss: 0.3984, Time: 0.74s\n",
      "Epoch: 56, Batch: 61/842, Loss: 0.4206, Time: 0.77s\n",
      "Epoch: 56, Batch: 71/842, Loss: 0.5114, Time: 0.77s\n",
      "Epoch: 56, Batch: 81/842, Loss: 0.2948, Time: 0.76s\n",
      "Epoch: 56, Batch: 91/842, Loss: 0.4272, Time: 0.75s\n",
      "Epoch: 56, Batch: 101/842, Loss: 0.5406, Time: 0.76s\n",
      "Epoch: 56, Batch: 111/842, Loss: 0.5057, Time: 0.76s\n",
      "Epoch: 56, Batch: 121/842, Loss: 0.4920, Time: 0.76s\n",
      "Epoch: 56, Batch: 131/842, Loss: 0.5086, Time: 0.77s\n",
      "Epoch: 56, Batch: 141/842, Loss: 0.4747, Time: 0.76s\n",
      "Epoch: 56, Batch: 151/842, Loss: 0.3639, Time: 0.76s\n",
      "Epoch: 56, Batch: 161/842, Loss: 0.3948, Time: 0.75s\n",
      "Epoch: 56, Batch: 171/842, Loss: 0.4599, Time: 0.76s\n",
      "Epoch: 56, Batch: 181/842, Loss: 0.2771, Time: 0.75s\n",
      "Epoch: 56, Batch: 191/842, Loss: 0.4526, Time: 0.75s\n",
      "Epoch: 56, Batch: 201/842, Loss: 0.5322, Time: 0.75s\n",
      "Epoch: 56, Batch: 211/842, Loss: 0.3698, Time: 0.77s\n",
      "Epoch: 56, Batch: 221/842, Loss: 0.5147, Time: 0.76s\n",
      "Epoch: 56, Batch: 231/842, Loss: 0.4838, Time: 0.76s\n",
      "Epoch: 56, Batch: 241/842, Loss: 0.4496, Time: 0.75s\n",
      "Epoch: 56, Batch: 251/842, Loss: 0.5110, Time: 0.76s\n",
      "Epoch: 56, Batch: 261/842, Loss: 0.4526, Time: 0.75s\n",
      "Epoch: 56, Batch: 271/842, Loss: 0.4610, Time: 0.73s\n",
      "Epoch: 56, Batch: 281/842, Loss: 0.5461, Time: 0.74s\n",
      "Epoch: 56, Batch: 291/842, Loss: 0.4088, Time: 0.74s\n",
      "Epoch: 56, Batch: 301/842, Loss: 0.4348, Time: 0.73s\n",
      "Epoch: 56, Batch: 311/842, Loss: 0.5590, Time: 0.74s\n",
      "Epoch: 56, Batch: 321/842, Loss: 0.5011, Time: 0.73s\n",
      "Epoch: 56, Batch: 331/842, Loss: 0.5349, Time: 0.74s\n",
      "Epoch: 56, Batch: 341/842, Loss: 0.5208, Time: 0.72s\n",
      "Epoch: 56, Batch: 351/842, Loss: 0.4458, Time: 0.73s\n",
      "Epoch: 56, Batch: 361/842, Loss: 0.4693, Time: 0.74s\n",
      "Epoch: 56, Batch: 371/842, Loss: 0.6117, Time: 0.82s\n",
      "Epoch: 56, Batch: 381/842, Loss: 0.4115, Time: 0.80s\n",
      "Epoch: 56, Batch: 391/842, Loss: 0.3768, Time: 0.77s\n",
      "Epoch: 56, Batch: 401/842, Loss: 0.4789, Time: 0.72s\n",
      "Epoch: 56, Batch: 411/842, Loss: 0.3900, Time: 0.72s\n",
      "Epoch: 56, Batch: 421/842, Loss: 0.3799, Time: 0.71s\n",
      "Epoch: 56, Batch: 431/842, Loss: 0.5072, Time: 0.74s\n",
      "Epoch: 56, Batch: 441/842, Loss: 0.6123, Time: 0.70s\n",
      "Epoch: 56, Batch: 451/842, Loss: 0.5312, Time: 0.71s\n",
      "Epoch: 56, Batch: 461/842, Loss: 0.3789, Time: 0.70s\n",
      "Epoch: 56, Batch: 471/842, Loss: 0.5152, Time: 0.71s\n",
      "Epoch: 56, Batch: 481/842, Loss: 0.5521, Time: 0.69s\n",
      "Epoch: 56, Batch: 491/842, Loss: 0.4640, Time: 0.71s\n",
      "Epoch: 56, Batch: 501/842, Loss: 0.4266, Time: 0.73s\n",
      "Epoch: 56, Batch: 511/842, Loss: 0.4637, Time: 0.73s\n",
      "Epoch: 56, Batch: 521/842, Loss: 0.5468, Time: 0.72s\n",
      "Epoch: 56, Batch: 531/842, Loss: 0.3043, Time: 0.74s\n",
      "Epoch: 56, Batch: 541/842, Loss: 0.3610, Time: 0.73s\n",
      "Epoch: 56, Batch: 551/842, Loss: 0.5367, Time: 0.73s\n",
      "Epoch: 56, Batch: 561/842, Loss: 0.3511, Time: 0.72s\n",
      "Epoch: 56, Batch: 571/842, Loss: 0.4033, Time: 0.74s\n",
      "Epoch: 56, Batch: 581/842, Loss: 0.4020, Time: 0.73s\n",
      "Epoch: 56, Batch: 591/842, Loss: 0.4644, Time: 0.74s\n",
      "Epoch: 56, Batch: 601/842, Loss: 0.3617, Time: 0.74s\n",
      "Epoch: 56, Batch: 611/842, Loss: 0.3156, Time: 0.73s\n",
      "Epoch: 56, Batch: 621/842, Loss: 0.4067, Time: 0.73s\n",
      "Epoch: 56, Batch: 631/842, Loss: 0.4022, Time: 0.73s\n",
      "Epoch: 56, Batch: 641/842, Loss: 0.4614, Time: 0.72s\n",
      "Epoch: 56, Batch: 651/842, Loss: 0.4998, Time: 0.73s\n",
      "Epoch: 56, Batch: 661/842, Loss: 0.3999, Time: 0.72s\n",
      "Epoch: 56, Batch: 671/842, Loss: 0.4520, Time: 0.73s\n",
      "Epoch: 56, Batch: 681/842, Loss: 0.5978, Time: 0.72s\n",
      "Epoch: 56, Batch: 691/842, Loss: 0.5056, Time: 0.73s\n",
      "Epoch: 56, Batch: 701/842, Loss: 0.5655, Time: 0.72s\n",
      "Epoch: 56, Batch: 711/842, Loss: 0.4643, Time: 0.73s\n",
      "Epoch: 56, Batch: 721/842, Loss: 0.5575, Time: 0.72s\n",
      "Epoch: 56, Batch: 731/842, Loss: 0.4244, Time: 0.72s\n",
      "Epoch: 56, Batch: 741/842, Loss: 0.4630, Time: 0.74s\n",
      "Epoch: 56, Batch: 751/842, Loss: 0.4740, Time: 0.73s\n",
      "Epoch: 56, Batch: 761/842, Loss: 0.4839, Time: 0.73s\n",
      "Epoch: 56, Batch: 771/842, Loss: 0.5162, Time: 0.73s\n",
      "Epoch: 56, Batch: 781/842, Loss: 0.3881, Time: 0.72s\n",
      "Epoch: 56, Batch: 791/842, Loss: 0.4276, Time: 0.73s\n",
      "Epoch: 56, Batch: 801/842, Loss: 0.4743, Time: 0.72s\n",
      "Epoch: 56, Batch: 811/842, Loss: 0.4826, Time: 0.73s\n",
      "Epoch: 56, Batch: 821/842, Loss: 0.4309, Time: 0.74s\n",
      "Epoch: 56, Batch: 831/842, Loss: 0.4401, Time: 0.73s\n",
      "Epoch: 56, Batch: 841/842, Loss: 0.5873, Time: 0.71s\n",
      "Epoch 56/100: Train Loss: 0.4596, Val Loss: 0.4607, mIoU: 0.6596, F1: 0.7882, OA: 0.9623\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7711, F1=0.8707, OA=0.8678, Precision=0.8235, Recall=0.9238\n",
      "    Class 1: IoU=0.6179, F1=0.7639, OA=0.9949, Precision=0.7471, Recall=0.7814\n",
      "    Class 2: IoU=0.8913, F1=0.9425, OA=0.9773, Precision=0.9677, Recall=0.9186\n",
      "    Class 3: IoU=0.4814, F1=0.6499, OA=0.9625, Precision=0.7517, Recall=0.5724\n",
      "    Class 4: IoU=0.7480, F1=0.8558, OA=0.9807, Precision=0.8730, Recall=0.8393\n",
      "    Class 5: IoU=0.6060, F1=0.7546, OA=0.9952, Precision=0.6906, Recall=0.8318\n",
      "    Class 6: IoU=0.5532, F1=0.7123, OA=0.9547, Precision=0.7922, Recall=0.6470\n",
      "    Class 7: IoU=0.6078, F1=0.7561, OA=0.9653, Precision=0.8638, Recall=0.6723\n",
      "Epoch: 57, Batch: 1/842, Loss: 0.4148, Time: 1.96s\n",
      "Epoch: 57, Batch: 11/842, Loss: 0.4753, Time: 0.69s\n",
      "Epoch: 57, Batch: 21/842, Loss: 0.4668, Time: 0.68s\n",
      "Epoch: 57, Batch: 31/842, Loss: 0.4630, Time: 0.68s\n",
      "Epoch: 57, Batch: 41/842, Loss: 0.4110, Time: 0.68s\n",
      "Epoch: 57, Batch: 51/842, Loss: 0.4465, Time: 0.68s\n",
      "Epoch: 57, Batch: 61/842, Loss: 0.5596, Time: 0.71s\n",
      "Epoch: 57, Batch: 71/842, Loss: 0.3277, Time: 0.78s\n",
      "Epoch: 57, Batch: 81/842, Loss: 0.5033, Time: 0.78s\n",
      "Epoch: 57, Batch: 91/842, Loss: 0.3748, Time: 0.79s\n",
      "Epoch: 57, Batch: 101/842, Loss: 0.3891, Time: 0.79s\n",
      "Epoch: 57, Batch: 111/842, Loss: 0.4041, Time: 0.79s\n",
      "Epoch: 57, Batch: 121/842, Loss: 0.4276, Time: 0.79s\n",
      "Epoch: 57, Batch: 131/842, Loss: 0.5262, Time: 0.78s\n",
      "Epoch: 57, Batch: 141/842, Loss: 0.4999, Time: 0.79s\n",
      "Epoch: 57, Batch: 151/842, Loss: 0.4440, Time: 0.78s\n",
      "Epoch: 57, Batch: 161/842, Loss: 0.6090, Time: 0.81s\n",
      "Epoch: 57, Batch: 171/842, Loss: 0.5405, Time: 0.78s\n",
      "Epoch: 57, Batch: 181/842, Loss: 0.3675, Time: 0.82s\n",
      "Epoch: 57, Batch: 191/842, Loss: 0.4843, Time: 0.80s\n",
      "Epoch: 57, Batch: 201/842, Loss: 0.3936, Time: 0.78s\n",
      "Epoch: 57, Batch: 211/842, Loss: 0.2970, Time: 0.81s\n",
      "Epoch: 57, Batch: 221/842, Loss: 0.4329, Time: 0.79s\n",
      "Epoch: 57, Batch: 231/842, Loss: 0.4336, Time: 0.78s\n",
      "Epoch: 57, Batch: 241/842, Loss: 0.5087, Time: 0.77s\n",
      "Epoch: 57, Batch: 251/842, Loss: 0.5541, Time: 0.77s\n",
      "Epoch: 57, Batch: 261/842, Loss: 0.5329, Time: 0.78s\n",
      "Epoch: 57, Batch: 271/842, Loss: 0.5484, Time: 0.80s\n",
      "Epoch: 57, Batch: 281/842, Loss: 0.3989, Time: 0.73s\n",
      "Epoch: 57, Batch: 291/842, Loss: 0.4916, Time: 0.75s\n",
      "Epoch: 57, Batch: 301/842, Loss: 0.3659, Time: 0.81s\n",
      "Epoch: 57, Batch: 311/842, Loss: 0.5086, Time: 0.82s\n",
      "Epoch: 57, Batch: 321/842, Loss: 0.4520, Time: 0.83s\n",
      "Epoch: 57, Batch: 331/842, Loss: 0.4321, Time: 0.81s\n",
      "Epoch: 57, Batch: 341/842, Loss: 0.4391, Time: 0.81s\n",
      "Epoch: 57, Batch: 351/842, Loss: 0.4878, Time: 0.89s\n",
      "Epoch: 57, Batch: 361/842, Loss: 0.3981, Time: 0.81s\n",
      "Epoch: 57, Batch: 371/842, Loss: 0.4556, Time: 0.72s\n",
      "Epoch: 57, Batch: 381/842, Loss: 0.4031, Time: 0.77s\n",
      "Epoch: 57, Batch: 391/842, Loss: 0.4510, Time: 0.79s\n",
      "Epoch: 57, Batch: 401/842, Loss: 0.5705, Time: 0.79s\n",
      "Epoch: 57, Batch: 411/842, Loss: 0.4519, Time: 0.78s\n",
      "Epoch: 57, Batch: 421/842, Loss: 0.4081, Time: 0.79s\n",
      "Epoch: 57, Batch: 431/842, Loss: 0.3881, Time: 0.76s\n",
      "Epoch: 57, Batch: 441/842, Loss: 0.3695, Time: 0.80s\n",
      "Epoch: 57, Batch: 451/842, Loss: 0.4508, Time: 0.83s\n",
      "Epoch: 57, Batch: 461/842, Loss: 0.4311, Time: 0.80s\n",
      "Epoch: 57, Batch: 471/842, Loss: 0.4682, Time: 0.79s\n",
      "Epoch: 57, Batch: 481/842, Loss: 0.5377, Time: 0.80s\n",
      "Epoch: 57, Batch: 491/842, Loss: 0.4105, Time: 0.80s\n",
      "Epoch: 57, Batch: 501/842, Loss: 0.5131, Time: 0.81s\n",
      "Epoch: 57, Batch: 511/842, Loss: 0.3790, Time: 0.80s\n",
      "Epoch: 57, Batch: 521/842, Loss: 0.4515, Time: 0.77s\n",
      "Epoch: 57, Batch: 531/842, Loss: 0.3543, Time: 0.76s\n",
      "Epoch: 57, Batch: 541/842, Loss: 0.6084, Time: 0.75s\n",
      "Epoch: 57, Batch: 551/842, Loss: 0.3938, Time: 0.75s\n",
      "Epoch: 57, Batch: 561/842, Loss: 0.5165, Time: 0.75s\n",
      "Epoch: 57, Batch: 571/842, Loss: 0.6416, Time: 0.77s\n",
      "Epoch: 57, Batch: 581/842, Loss: 0.5170, Time: 0.78s\n",
      "Epoch: 57, Batch: 591/842, Loss: 0.3848, Time: 0.77s\n",
      "Epoch: 57, Batch: 601/842, Loss: 0.5067, Time: 0.77s\n",
      "Epoch: 57, Batch: 611/842, Loss: 0.3147, Time: 0.77s\n",
      "Epoch: 57, Batch: 621/842, Loss: 0.4784, Time: 0.80s\n",
      "Epoch: 57, Batch: 631/842, Loss: 0.4820, Time: 0.76s\n",
      "Epoch: 57, Batch: 641/842, Loss: 0.4999, Time: 0.75s\n",
      "Epoch: 57, Batch: 651/842, Loss: 0.4772, Time: 0.75s\n",
      "Epoch: 57, Batch: 661/842, Loss: 0.4665, Time: 0.76s\n",
      "Epoch: 57, Batch: 671/842, Loss: 0.4191, Time: 0.75s\n",
      "Epoch: 57, Batch: 681/842, Loss: 0.4485, Time: 0.73s\n",
      "Epoch: 57, Batch: 691/842, Loss: 0.3930, Time: 0.80s\n",
      "Epoch: 57, Batch: 701/842, Loss: 0.4476, Time: 0.81s\n",
      "Epoch: 57, Batch: 711/842, Loss: 0.5880, Time: 0.81s\n",
      "Epoch: 57, Batch: 721/842, Loss: 0.4556, Time: 0.77s\n",
      "Epoch: 57, Batch: 731/842, Loss: 0.3953, Time: 0.73s\n",
      "Epoch: 57, Batch: 741/842, Loss: 0.4350, Time: 0.73s\n",
      "Epoch: 57, Batch: 751/842, Loss: 0.3732, Time: 0.72s\n",
      "Epoch: 57, Batch: 761/842, Loss: 0.3670, Time: 0.73s\n",
      "Epoch: 57, Batch: 771/842, Loss: 0.3219, Time: 0.73s\n",
      "Epoch: 57, Batch: 781/842, Loss: 0.5189, Time: 0.73s\n",
      "Epoch: 57, Batch: 791/842, Loss: 0.4071, Time: 0.73s\n",
      "Epoch: 57, Batch: 801/842, Loss: 0.3409, Time: 0.72s\n",
      "Epoch: 57, Batch: 811/842, Loss: 0.4393, Time: 0.73s\n",
      "Epoch: 57, Batch: 821/842, Loss: 0.3476, Time: 0.73s\n",
      "Epoch: 57, Batch: 831/842, Loss: 0.4944, Time: 0.73s\n",
      "Epoch: 57, Batch: 841/842, Loss: 0.5719, Time: 0.73s\n",
      "Epoch 57/100: Train Loss: 0.4638, Val Loss: 0.4377, mIoU: 0.6846, F1: 0.8062, OA: 0.9643\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7779, F1=0.8751, OA=0.8720, Precision=0.8264, Recall=0.9299\n",
      "    Class 1: IoU=0.6976, F1=0.8218, OA=0.9964, Precision=0.8575, Recall=0.7891\n",
      "    Class 2: IoU=0.9110, F1=0.9534, OA=0.9813, Precision=0.9621, Recall=0.9448\n",
      "    Class 3: IoU=0.4735, F1=0.6427, OA=0.9636, Precision=0.7959, Recall=0.5389\n",
      "    Class 4: IoU=0.7484, F1=0.8561, OA=0.9812, Precision=0.8935, Recall=0.8217\n",
      "    Class 5: IoU=0.6683, F1=0.8012, OA=0.9964, Precision=0.8001, Recall=0.8022\n",
      "    Class 6: IoU=0.5683, F1=0.7247, OA=0.9563, Precision=0.7983, Recall=0.6636\n",
      "    Class 7: IoU=0.6322, F1=0.7747, OA=0.9676, Precision=0.8729, Recall=0.6963\n",
      "Epoch: 58, Batch: 1/842, Loss: 0.5570, Time: 2.39s\n",
      "Epoch: 58, Batch: 11/842, Loss: 0.3568, Time: 0.80s\n",
      "Epoch: 58, Batch: 21/842, Loss: 0.4633, Time: 0.80s\n",
      "Epoch: 58, Batch: 31/842, Loss: 0.4809, Time: 0.82s\n",
      "Epoch: 58, Batch: 41/842, Loss: 0.3384, Time: 0.85s\n",
      "Epoch: 58, Batch: 51/842, Loss: 0.3329, Time: 0.85s\n",
      "Epoch: 58, Batch: 61/842, Loss: 0.4935, Time: 0.87s\n",
      "Epoch: 58, Batch: 71/842, Loss: 0.4113, Time: 0.84s\n",
      "Epoch: 58, Batch: 81/842, Loss: 0.3672, Time: 0.86s\n",
      "Epoch: 58, Batch: 91/842, Loss: 0.5547, Time: 0.84s\n",
      "Epoch: 58, Batch: 101/842, Loss: 0.4243, Time: 0.84s\n",
      "Epoch: 58, Batch: 111/842, Loss: 0.4306, Time: 0.88s\n",
      "Epoch: 58, Batch: 121/842, Loss: 0.5245, Time: 0.88s\n",
      "Epoch: 58, Batch: 131/842, Loss: 0.4049, Time: 0.78s\n",
      "Epoch: 58, Batch: 141/842, Loss: 0.3659, Time: 0.76s\n",
      "Epoch: 58, Batch: 151/842, Loss: 0.3823, Time: 0.76s\n",
      "Epoch: 58, Batch: 161/842, Loss: 0.4653, Time: 0.78s\n",
      "Epoch: 58, Batch: 171/842, Loss: 0.4972, Time: 0.81s\n",
      "Epoch: 58, Batch: 181/842, Loss: 0.4164, Time: 0.78s\n",
      "Epoch: 58, Batch: 191/842, Loss: 0.5173, Time: 0.78s\n",
      "Epoch: 58, Batch: 201/842, Loss: 0.4076, Time: 0.78s\n",
      "Epoch: 58, Batch: 211/842, Loss: 0.5205, Time: 0.77s\n",
      "Epoch: 58, Batch: 221/842, Loss: 0.5495, Time: 0.77s\n",
      "Epoch: 58, Batch: 231/842, Loss: 0.3384, Time: 0.79s\n",
      "Epoch: 58, Batch: 241/842, Loss: 0.3947, Time: 0.81s\n",
      "Epoch: 58, Batch: 251/842, Loss: 0.3852, Time: 0.77s\n",
      "Epoch: 58, Batch: 261/842, Loss: 0.5288, Time: 0.78s\n",
      "Epoch: 58, Batch: 271/842, Loss: 0.4786, Time: 0.78s\n",
      "Epoch: 58, Batch: 281/842, Loss: 0.4481, Time: 0.79s\n",
      "Epoch: 58, Batch: 291/842, Loss: 0.4783, Time: 0.78s\n",
      "Epoch: 58, Batch: 301/842, Loss: 0.4695, Time: 0.80s\n",
      "Epoch: 58, Batch: 311/842, Loss: 0.3809, Time: 0.78s\n",
      "Epoch: 58, Batch: 321/842, Loss: 0.4789, Time: 0.79s\n",
      "Epoch: 58, Batch: 331/842, Loss: 0.3690, Time: 0.79s\n",
      "Epoch: 58, Batch: 341/842, Loss: 0.5719, Time: 0.79s\n",
      "Epoch: 58, Batch: 351/842, Loss: 0.4347, Time: 0.80s\n",
      "Epoch: 58, Batch: 361/842, Loss: 0.4802, Time: 0.79s\n",
      "Epoch: 58, Batch: 371/842, Loss: 0.5263, Time: 0.78s\n",
      "Epoch: 58, Batch: 381/842, Loss: 0.6143, Time: 0.78s\n",
      "Epoch: 58, Batch: 391/842, Loss: 0.3598, Time: 0.76s\n",
      "Epoch: 58, Batch: 401/842, Loss: 0.5275, Time: 0.86s\n",
      "Epoch: 58, Batch: 411/842, Loss: 0.4649, Time: 0.77s\n",
      "Epoch: 58, Batch: 421/842, Loss: 0.4249, Time: 0.75s\n",
      "Epoch: 58, Batch: 431/842, Loss: 0.5181, Time: 0.76s\n",
      "Epoch: 58, Batch: 441/842, Loss: 0.4254, Time: 0.75s\n",
      "Epoch: 58, Batch: 451/842, Loss: 0.4422, Time: 0.75s\n",
      "Epoch: 58, Batch: 461/842, Loss: 0.4861, Time: 0.77s\n",
      "Epoch: 58, Batch: 471/842, Loss: 0.4832, Time: 0.74s\n",
      "Epoch: 58, Batch: 481/842, Loss: 0.5416, Time: 0.75s\n",
      "Epoch: 58, Batch: 491/842, Loss: 0.5265, Time: 0.75s\n",
      "Epoch: 58, Batch: 501/842, Loss: 0.3894, Time: 0.78s\n",
      "Epoch: 58, Batch: 511/842, Loss: 0.5056, Time: 0.75s\n",
      "Epoch: 58, Batch: 521/842, Loss: 0.3854, Time: 0.78s\n",
      "Epoch: 58, Batch: 531/842, Loss: 0.5192, Time: 0.76s\n",
      "Epoch: 58, Batch: 541/842, Loss: 0.5521, Time: 0.77s\n",
      "Epoch: 58, Batch: 551/842, Loss: 0.4403, Time: 0.86s\n",
      "Epoch: 58, Batch: 561/842, Loss: 0.4819, Time: 0.76s\n",
      "Epoch: 58, Batch: 571/842, Loss: 0.3594, Time: 0.75s\n",
      "Epoch: 58, Batch: 581/842, Loss: 0.5220, Time: 0.77s\n",
      "Epoch: 58, Batch: 591/842, Loss: 0.5254, Time: 0.75s\n",
      "Epoch: 58, Batch: 601/842, Loss: 0.4923, Time: 0.78s\n",
      "Epoch: 58, Batch: 611/842, Loss: 0.3542, Time: 0.77s\n",
      "Epoch: 58, Batch: 621/842, Loss: 0.4985, Time: 0.75s\n",
      "Epoch: 58, Batch: 631/842, Loss: 0.4769, Time: 0.73s\n",
      "Epoch: 58, Batch: 641/842, Loss: 0.4234, Time: 0.72s\n",
      "Epoch: 58, Batch: 651/842, Loss: 0.3206, Time: 0.72s\n",
      "Epoch: 58, Batch: 661/842, Loss: 0.4401, Time: 0.71s\n",
      "Epoch: 58, Batch: 671/842, Loss: 0.4826, Time: 0.74s\n",
      "Epoch: 58, Batch: 681/842, Loss: 0.5172, Time: 0.77s\n",
      "Epoch: 58, Batch: 691/842, Loss: 0.4631, Time: 0.77s\n",
      "Epoch: 58, Batch: 701/842, Loss: 0.4878, Time: 0.81s\n",
      "Epoch: 58, Batch: 711/842, Loss: 0.4118, Time: 0.76s\n",
      "Epoch: 58, Batch: 721/842, Loss: 0.4105, Time: 0.78s\n",
      "Epoch: 58, Batch: 731/842, Loss: 0.4665, Time: 0.79s\n",
      "Epoch: 58, Batch: 741/842, Loss: 0.4239, Time: 0.79s\n",
      "Epoch: 58, Batch: 751/842, Loss: 0.4970, Time: 0.79s\n",
      "Epoch: 58, Batch: 761/842, Loss: 0.5422, Time: 0.77s\n",
      "Epoch: 58, Batch: 771/842, Loss: 0.5034, Time: 0.77s\n",
      "Epoch: 58, Batch: 781/842, Loss: 0.3006, Time: 0.78s\n",
      "Epoch: 58, Batch: 791/842, Loss: 0.5315, Time: 0.76s\n",
      "Epoch: 58, Batch: 801/842, Loss: 0.4498, Time: 0.79s\n",
      "Epoch: 58, Batch: 811/842, Loss: 0.4518, Time: 0.82s\n",
      "Epoch: 58, Batch: 821/842, Loss: 0.3521, Time: 0.83s\n",
      "Epoch: 58, Batch: 831/842, Loss: 0.4674, Time: 0.82s\n",
      "Epoch: 58, Batch: 841/842, Loss: 0.4394, Time: 0.81s\n",
      "Epoch 58/100: Train Loss: 0.4659, Val Loss: 0.4524, mIoU: 0.6708, F1: 0.7966, OA: 0.9630\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7729, F1=0.8719, OA=0.8679, Precision=0.8188, Recall=0.9324\n",
      "    Class 1: IoU=0.6450, F1=0.7842, OA=0.9954, Precision=0.7777, Recall=0.7908\n",
      "    Class 2: IoU=0.8938, F1=0.9439, OA=0.9778, Precision=0.9666, Recall=0.9223\n",
      "    Class 3: IoU=0.4790, F1=0.6478, OA=0.9621, Precision=0.7452, Recall=0.5728\n",
      "    Class 4: IoU=0.7464, F1=0.8548, OA=0.9807, Precision=0.8769, Recall=0.8338\n",
      "    Class 5: IoU=0.6494, F1=0.7874, OA=0.9961, Precision=0.7597, Recall=0.8173\n",
      "    Class 6: IoU=0.5573, F1=0.7157, OA=0.9572, Precision=0.8447, Recall=0.6209\n",
      "    Class 7: IoU=0.6223, F1=0.7672, OA=0.9669, Precision=0.8791, Recall=0.6805\n",
      "Epoch: 59, Batch: 1/842, Loss: 0.5119, Time: 1.97s\n",
      "Epoch: 59, Batch: 11/842, Loss: 0.4062, Time: 0.82s\n",
      "Epoch: 59, Batch: 21/842, Loss: 0.4631, Time: 0.89s\n",
      "Epoch: 59, Batch: 31/842, Loss: 0.3104, Time: 0.83s\n",
      "Epoch: 59, Batch: 41/842, Loss: 0.4538, Time: 0.89s\n",
      "Epoch: 59, Batch: 51/842, Loss: 0.3861, Time: 0.82s\n",
      "Epoch: 59, Batch: 61/842, Loss: 0.4995, Time: 0.79s\n",
      "Epoch: 59, Batch: 71/842, Loss: 0.3263, Time: 0.78s\n",
      "Epoch: 59, Batch: 81/842, Loss: 0.4827, Time: 0.77s\n",
      "Epoch: 59, Batch: 91/842, Loss: 0.5167, Time: 0.79s\n",
      "Epoch: 59, Batch: 101/842, Loss: 0.4895, Time: 0.79s\n",
      "Epoch: 59, Batch: 111/842, Loss: 0.4768, Time: 0.77s\n",
      "Epoch: 59, Batch: 121/842, Loss: 0.4795, Time: 0.77s\n",
      "Epoch: 59, Batch: 131/842, Loss: 0.5283, Time: 0.79s\n",
      "Epoch: 59, Batch: 141/842, Loss: 0.2999, Time: 0.79s\n",
      "Epoch: 59, Batch: 151/842, Loss: 0.4249, Time: 0.78s\n",
      "Epoch: 59, Batch: 161/842, Loss: 0.4168, Time: 0.78s\n",
      "Epoch: 59, Batch: 171/842, Loss: 0.5765, Time: 0.79s\n",
      "Epoch: 59, Batch: 181/842, Loss: 0.3623, Time: 0.80s\n",
      "Epoch: 59, Batch: 191/842, Loss: 0.4138, Time: 0.77s\n",
      "Epoch: 59, Batch: 201/842, Loss: 0.3301, Time: 1.64s\n",
      "Epoch: 59, Batch: 211/842, Loss: 0.5567, Time: 0.82s\n",
      "Epoch: 59, Batch: 221/842, Loss: 0.4352, Time: 0.82s\n",
      "Epoch: 59, Batch: 231/842, Loss: 0.4784, Time: 0.78s\n",
      "Epoch: 59, Batch: 241/842, Loss: 0.5111, Time: 0.77s\n",
      "Epoch: 59, Batch: 251/842, Loss: 0.4414, Time: 0.77s\n",
      "Epoch: 59, Batch: 261/842, Loss: 0.4989, Time: 0.79s\n",
      "Epoch: 59, Batch: 271/842, Loss: 0.4381, Time: 0.78s\n",
      "Epoch: 59, Batch: 281/842, Loss: 0.3190, Time: 0.77s\n",
      "Epoch: 59, Batch: 291/842, Loss: 0.4899, Time: 0.76s\n",
      "Epoch: 59, Batch: 301/842, Loss: 0.5109, Time: 0.77s\n",
      "Epoch: 59, Batch: 311/842, Loss: 0.4914, Time: 0.78s\n",
      "Epoch: 59, Batch: 321/842, Loss: 0.5337, Time: 0.76s\n",
      "Epoch: 59, Batch: 331/842, Loss: 0.4607, Time: 0.75s\n",
      "Epoch: 59, Batch: 341/842, Loss: 0.4349, Time: 0.80s\n",
      "Epoch: 59, Batch: 351/842, Loss: 0.4477, Time: 0.76s\n",
      "Epoch: 59, Batch: 361/842, Loss: 0.4452, Time: 0.76s\n",
      "Epoch: 59, Batch: 371/842, Loss: 0.4208, Time: 0.78s\n",
      "Epoch: 59, Batch: 381/842, Loss: 0.4748, Time: 0.77s\n",
      "Epoch: 59, Batch: 391/842, Loss: 0.3491, Time: 0.77s\n",
      "Epoch: 59, Batch: 401/842, Loss: 0.3390, Time: 0.77s\n",
      "Epoch: 59, Batch: 411/842, Loss: 0.5046, Time: 0.84s\n",
      "Epoch: 59, Batch: 421/842, Loss: 0.4908, Time: 0.80s\n",
      "Epoch: 59, Batch: 431/842, Loss: 0.5670, Time: 0.77s\n",
      "Epoch: 59, Batch: 441/842, Loss: 0.3893, Time: 0.77s\n",
      "Epoch: 59, Batch: 451/842, Loss: 0.5084, Time: 0.79s\n",
      "Epoch: 59, Batch: 461/842, Loss: 0.4854, Time: 0.76s\n",
      "Epoch: 59, Batch: 471/842, Loss: 0.5375, Time: 0.78s\n",
      "Epoch: 59, Batch: 481/842, Loss: 0.5915, Time: 0.78s\n",
      "Epoch: 59, Batch: 491/842, Loss: 0.4482, Time: 0.80s\n",
      "Epoch: 59, Batch: 501/842, Loss: 0.4248, Time: 0.80s\n",
      "Epoch: 59, Batch: 511/842, Loss: 0.4703, Time: 0.83s\n",
      "Epoch: 59, Batch: 521/842, Loss: 0.5188, Time: 0.80s\n",
      "Epoch: 59, Batch: 531/842, Loss: 0.4344, Time: 0.80s\n",
      "Epoch: 59, Batch: 541/842, Loss: 0.4546, Time: 0.80s\n",
      "Epoch: 59, Batch: 551/842, Loss: 0.4260, Time: 0.80s\n",
      "Epoch: 59, Batch: 561/842, Loss: 0.4347, Time: 0.78s\n",
      "Epoch: 59, Batch: 571/842, Loss: 0.4606, Time: 0.80s\n",
      "Epoch: 59, Batch: 581/842, Loss: 0.3958, Time: 0.78s\n",
      "Epoch: 59, Batch: 591/842, Loss: 0.3476, Time: 0.82s\n",
      "Epoch: 59, Batch: 601/842, Loss: 0.3921, Time: 0.82s\n",
      "Epoch: 59, Batch: 611/842, Loss: 0.4199, Time: 0.82s\n",
      "Epoch: 59, Batch: 621/842, Loss: 0.3819, Time: 0.80s\n",
      "Epoch: 59, Batch: 631/842, Loss: 0.4309, Time: 0.81s\n",
      "Epoch: 59, Batch: 641/842, Loss: 0.4136, Time: 0.80s\n",
      "Epoch: 59, Batch: 651/842, Loss: 0.4396, Time: 0.78s\n",
      "Epoch: 59, Batch: 661/842, Loss: 0.5034, Time: 0.79s\n",
      "Epoch: 59, Batch: 671/842, Loss: 0.3485, Time: 0.79s\n",
      "Epoch: 59, Batch: 681/842, Loss: 0.5035, Time: 0.79s\n",
      "Epoch: 59, Batch: 691/842, Loss: 0.5258, Time: 0.78s\n",
      "Epoch: 59, Batch: 701/842, Loss: 0.4335, Time: 0.79s\n",
      "Epoch: 59, Batch: 711/842, Loss: 0.5023, Time: 0.78s\n",
      "Epoch: 59, Batch: 721/842, Loss: 0.4774, Time: 0.80s\n",
      "Epoch: 59, Batch: 731/842, Loss: 0.3758, Time: 0.79s\n",
      "Epoch: 59, Batch: 741/842, Loss: 0.3791, Time: 0.81s\n",
      "Epoch: 59, Batch: 751/842, Loss: 0.4817, Time: 0.80s\n",
      "Epoch: 59, Batch: 761/842, Loss: 0.4348, Time: 0.81s\n",
      "Epoch: 59, Batch: 771/842, Loss: 0.4867, Time: 0.78s\n",
      "Epoch: 59, Batch: 781/842, Loss: 0.3150, Time: 0.78s\n",
      "Epoch: 59, Batch: 791/842, Loss: 0.3752, Time: 0.77s\n",
      "Epoch: 59, Batch: 801/842, Loss: 0.3987, Time: 0.79s\n",
      "Epoch: 59, Batch: 811/842, Loss: 0.3706, Time: 0.81s\n",
      "Epoch: 59, Batch: 821/842, Loss: 0.4127, Time: 0.81s\n",
      "Epoch: 59, Batch: 831/842, Loss: 0.4359, Time: 0.79s\n",
      "Epoch: 59, Batch: 841/842, Loss: 0.5119, Time: 0.80s\n",
      "Epoch 59/100: Train Loss: 0.4562, Val Loss: 0.4287, mIoU: 0.6911, F1: 0.8111, OA: 0.9652\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7814, F1=0.8773, OA=0.8744, Precision=0.8295, Recall=0.9309\n",
      "    Class 1: IoU=0.7174, F1=0.8355, OA=0.9967, Precision=0.8742, Recall=0.8001\n",
      "    Class 2: IoU=0.9118, F1=0.9538, OA=0.9814, Precision=0.9591, Recall=0.9487\n",
      "    Class 3: IoU=0.4881, F1=0.6560, OA=0.9643, Precision=0.7914, Recall=0.5601\n",
      "    Class 4: IoU=0.7550, F1=0.8604, OA=0.9817, Precision=0.8941, Recall=0.8291\n",
      "    Class 5: IoU=0.6578, F1=0.7936, OA=0.9962, Precision=0.7685, Recall=0.8203\n",
      "    Class 6: IoU=0.5736, F1=0.7290, OA=0.9582, Precision=0.8316, Recall=0.6490\n",
      "    Class 7: IoU=0.6440, F1=0.7834, OA=0.9685, Precision=0.8715, Recall=0.7115\n",
      "Saving best model with mIoU: 0.6911\n",
      "Epoch: 60, Batch: 1/842, Loss: 0.5250, Time: 1.79s\n",
      "Epoch: 60, Batch: 11/842, Loss: 0.4559, Time: 0.85s\n",
      "Epoch: 60, Batch: 21/842, Loss: 0.4659, Time: 0.80s\n",
      "Epoch: 60, Batch: 31/842, Loss: 0.4649, Time: 0.77s\n",
      "Epoch: 60, Batch: 41/842, Loss: 0.4466, Time: 0.83s\n",
      "Epoch: 60, Batch: 51/842, Loss: 0.5058, Time: 0.76s\n",
      "Epoch: 60, Batch: 61/842, Loss: 0.6256, Time: 0.75s\n",
      "Epoch: 60, Batch: 71/842, Loss: 0.4799, Time: 0.79s\n",
      "Epoch: 60, Batch: 81/842, Loss: 0.6856, Time: 0.84s\n",
      "Epoch: 60, Batch: 91/842, Loss: 0.3507, Time: 0.75s\n",
      "Epoch: 60, Batch: 101/842, Loss: 0.4229, Time: 0.76s\n",
      "Epoch: 60, Batch: 111/842, Loss: 0.4576, Time: 0.74s\n",
      "Epoch: 60, Batch: 121/842, Loss: 0.4352, Time: 0.74s\n",
      "Epoch: 60, Batch: 131/842, Loss: 0.4792, Time: 0.74s\n",
      "Epoch: 60, Batch: 141/842, Loss: 0.3229, Time: 0.72s\n",
      "Epoch: 60, Batch: 151/842, Loss: 0.4532, Time: 0.72s\n",
      "Epoch: 60, Batch: 161/842, Loss: 0.5227, Time: 0.74s\n",
      "Epoch: 60, Batch: 171/842, Loss: 0.4529, Time: 0.74s\n",
      "Epoch: 60, Batch: 181/842, Loss: 0.5389, Time: 0.77s\n",
      "Epoch: 60, Batch: 191/842, Loss: 0.4983, Time: 0.84s\n",
      "Epoch: 60, Batch: 201/842, Loss: 0.5300, Time: 0.83s\n",
      "Epoch: 60, Batch: 211/842, Loss: 0.3702, Time: 0.81s\n",
      "Epoch: 60, Batch: 221/842, Loss: 0.4615, Time: 0.80s\n",
      "Epoch: 60, Batch: 231/842, Loss: 0.5092, Time: 0.83s\n",
      "Epoch: 60, Batch: 241/842, Loss: 0.4475, Time: 0.88s\n",
      "Epoch: 60, Batch: 251/842, Loss: 0.4258, Time: 0.84s\n",
      "Epoch: 60, Batch: 261/842, Loss: 0.3459, Time: 0.80s\n",
      "Epoch: 60, Batch: 271/842, Loss: 0.3411, Time: 0.78s\n",
      "Epoch: 60, Batch: 281/842, Loss: 0.5302, Time: 0.80s\n",
      "Epoch: 60, Batch: 291/842, Loss: 0.4297, Time: 0.79s\n",
      "Epoch: 60, Batch: 301/842, Loss: 0.5180, Time: 0.79s\n",
      "Epoch: 60, Batch: 311/842, Loss: 0.3736, Time: 0.79s\n",
      "Epoch: 60, Batch: 321/842, Loss: 0.5620, Time: 0.77s\n",
      "Epoch: 60, Batch: 331/842, Loss: 0.4281, Time: 0.78s\n",
      "Epoch: 60, Batch: 341/842, Loss: 0.5220, Time: 0.74s\n",
      "Epoch: 60, Batch: 351/842, Loss: 0.4447, Time: 0.73s\n",
      "Epoch: 60, Batch: 361/842, Loss: 0.4702, Time: 0.72s\n",
      "Epoch: 60, Batch: 371/842, Loss: 0.4402, Time: 0.73s\n",
      "Epoch: 60, Batch: 381/842, Loss: 0.5158, Time: 0.73s\n",
      "Epoch: 60, Batch: 391/842, Loss: 0.4856, Time: 0.76s\n",
      "Epoch: 60, Batch: 401/842, Loss: 0.4338, Time: 0.74s\n",
      "Epoch: 60, Batch: 411/842, Loss: 0.6792, Time: 0.71s\n",
      "Epoch: 60, Batch: 421/842, Loss: 0.5103, Time: 0.73s\n",
      "Epoch: 60, Batch: 431/842, Loss: 0.3845, Time: 0.73s\n",
      "Epoch: 60, Batch: 441/842, Loss: 0.4604, Time: 0.71s\n",
      "Epoch: 60, Batch: 451/842, Loss: 0.3747, Time: 0.73s\n",
      "Epoch: 60, Batch: 461/842, Loss: 0.4572, Time: 0.71s\n",
      "Epoch: 60, Batch: 471/842, Loss: 0.4237, Time: 0.71s\n",
      "Epoch: 60, Batch: 481/842, Loss: 0.4909, Time: 0.71s\n",
      "Epoch: 60, Batch: 491/842, Loss: 0.4426, Time: 0.72s\n",
      "Epoch: 60, Batch: 501/842, Loss: 0.4820, Time: 0.72s\n",
      "Epoch: 60, Batch: 511/842, Loss: 0.4359, Time: 0.75s\n",
      "Epoch: 60, Batch: 521/842, Loss: 0.4097, Time: 0.75s\n",
      "Epoch: 60, Batch: 531/842, Loss: 0.5326, Time: 0.75s\n",
      "Epoch: 60, Batch: 541/842, Loss: 0.3730, Time: 0.75s\n",
      "Epoch: 60, Batch: 551/842, Loss: 0.4664, Time: 0.76s\n",
      "Epoch: 60, Batch: 561/842, Loss: 0.4348, Time: 0.74s\n",
      "Epoch: 60, Batch: 571/842, Loss: 0.3851, Time: 0.74s\n",
      "Epoch: 60, Batch: 581/842, Loss: 0.4754, Time: 0.74s\n",
      "Epoch: 60, Batch: 591/842, Loss: 0.3982, Time: 0.75s\n",
      "Epoch: 60, Batch: 601/842, Loss: 0.4224, Time: 0.75s\n",
      "Epoch: 60, Batch: 611/842, Loss: 0.4691, Time: 0.74s\n",
      "Epoch: 60, Batch: 621/842, Loss: 0.3934, Time: 0.73s\n",
      "Epoch: 60, Batch: 631/842, Loss: 0.3039, Time: 0.72s\n",
      "Epoch: 60, Batch: 641/842, Loss: 0.4378, Time: 0.74s\n",
      "Epoch: 60, Batch: 651/842, Loss: 0.3926, Time: 0.73s\n",
      "Epoch: 60, Batch: 661/842, Loss: 0.4740, Time: 0.73s\n",
      "Epoch: 60, Batch: 671/842, Loss: 0.3700, Time: 0.73s\n",
      "Epoch: 60, Batch: 681/842, Loss: 0.5519, Time: 0.73s\n",
      "Epoch: 60, Batch: 691/842, Loss: 0.3390, Time: 0.72s\n",
      "Epoch: 60, Batch: 701/842, Loss: 0.5294, Time: 0.73s\n",
      "Epoch: 60, Batch: 711/842, Loss: 0.5000, Time: 0.72s\n",
      "Epoch: 60, Batch: 721/842, Loss: 0.4685, Time: 0.73s\n",
      "Epoch: 60, Batch: 731/842, Loss: 0.5542, Time: 0.72s\n",
      "Epoch: 60, Batch: 741/842, Loss: 0.4291, Time: 0.71s\n",
      "Epoch: 60, Batch: 751/842, Loss: 0.4877, Time: 0.75s\n",
      "Epoch: 60, Batch: 761/842, Loss: 0.3474, Time: 0.83s\n",
      "Epoch: 60, Batch: 771/842, Loss: 0.5056, Time: 0.78s\n",
      "Epoch: 60, Batch: 781/842, Loss: 0.5603, Time: 0.83s\n",
      "Epoch: 60, Batch: 791/842, Loss: 0.4923, Time: 0.83s\n",
      "Epoch: 60, Batch: 801/842, Loss: 0.4308, Time: 0.83s\n",
      "Epoch: 60, Batch: 811/842, Loss: 0.4923, Time: 0.82s\n",
      "Epoch: 60, Batch: 821/842, Loss: 0.4077, Time: 0.76s\n",
      "Epoch: 60, Batch: 831/842, Loss: 0.4793, Time: 0.83s\n",
      "Epoch: 60, Batch: 841/842, Loss: 0.5287, Time: 0.76s\n",
      "Epoch 60/100: Train Loss: 0.4561, Val Loss: 0.4333, mIoU: 0.6858, F1: 0.8072, OA: 0.9646\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7784, F1=0.8754, OA=0.8718, Precision=0.8239, Recall=0.9337\n",
      "    Class 1: IoU=0.7094, F1=0.8300, OA=0.9966, Precision=0.8741, Recall=0.7902\n",
      "    Class 2: IoU=0.9113, F1=0.9536, OA=0.9814, Precision=0.9620, Recall=0.9454\n",
      "    Class 3: IoU=0.4871, F1=0.6551, OA=0.9641, Precision=0.7866, Recall=0.5613\n",
      "    Class 4: IoU=0.7507, F1=0.8576, OA=0.9810, Precision=0.8774, Recall=0.8387\n",
      "    Class 5: IoU=0.6527, F1=0.7899, OA=0.9961, Precision=0.7644, Recall=0.8172\n",
      "    Class 6: IoU=0.5602, F1=0.7181, OA=0.9579, Precision=0.8564, Recall=0.6183\n",
      "    Class 7: IoU=0.6367, F1=0.7781, OA=0.9680, Precision=0.8742, Recall=0.7010\n",
      "Epoch: 61, Batch: 1/842, Loss: 0.3850, Time: 1.86s\n",
      "Epoch: 61, Batch: 11/842, Loss: 0.4090, Time: 0.78s\n",
      "Epoch: 61, Batch: 21/842, Loss: 0.3878, Time: 0.84s\n",
      "Epoch: 61, Batch: 31/842, Loss: 0.4469, Time: 0.88s\n",
      "Epoch: 61, Batch: 41/842, Loss: 0.4716, Time: 0.77s\n",
      "Epoch: 61, Batch: 51/842, Loss: 0.3930, Time: 0.75s\n",
      "Epoch: 61, Batch: 61/842, Loss: 0.5238, Time: 0.77s\n",
      "Epoch: 61, Batch: 71/842, Loss: 0.4048, Time: 0.81s\n",
      "Epoch: 61, Batch: 81/842, Loss: 0.5222, Time: 0.84s\n",
      "Epoch: 61, Batch: 91/842, Loss: 0.3848, Time: 0.86s\n",
      "Epoch: 61, Batch: 101/842, Loss: 0.4739, Time: 0.87s\n",
      "Epoch: 61, Batch: 111/842, Loss: 0.6096, Time: 0.83s\n",
      "Epoch: 61, Batch: 121/842, Loss: 0.3799, Time: 0.82s\n",
      "Epoch: 61, Batch: 131/842, Loss: 0.4930, Time: 0.83s\n",
      "Epoch: 61, Batch: 141/842, Loss: 0.4439, Time: 0.79s\n",
      "Epoch: 61, Batch: 151/842, Loss: 0.3690, Time: 0.73s\n",
      "Epoch: 61, Batch: 161/842, Loss: 0.5411, Time: 0.74s\n",
      "Epoch: 61, Batch: 171/842, Loss: 0.6138, Time: 0.74s\n",
      "Epoch: 61, Batch: 181/842, Loss: 0.5035, Time: 0.73s\n",
      "Epoch: 61, Batch: 191/842, Loss: 0.4190, Time: 0.73s\n",
      "Epoch: 61, Batch: 201/842, Loss: 0.4023, Time: 0.73s\n",
      "Epoch: 61, Batch: 211/842, Loss: 0.5290, Time: 0.73s\n",
      "Epoch: 61, Batch: 221/842, Loss: 0.3991, Time: 0.72s\n",
      "Epoch: 61, Batch: 231/842, Loss: 0.3202, Time: 0.72s\n",
      "Epoch: 61, Batch: 241/842, Loss: 0.3429, Time: 0.73s\n",
      "Epoch: 61, Batch: 251/842, Loss: 0.5117, Time: 0.73s\n",
      "Epoch: 61, Batch: 261/842, Loss: 0.5041, Time: 0.73s\n",
      "Epoch: 61, Batch: 271/842, Loss: 0.4456, Time: 0.73s\n",
      "Epoch: 61, Batch: 281/842, Loss: 0.5461, Time: 0.72s\n",
      "Epoch: 61, Batch: 291/842, Loss: 0.4958, Time: 0.72s\n",
      "Epoch: 61, Batch: 301/842, Loss: 0.5675, Time: 0.75s\n",
      "Epoch: 61, Batch: 311/842, Loss: 0.4025, Time: 0.76s\n",
      "Epoch: 61, Batch: 321/842, Loss: 0.3933, Time: 0.84s\n",
      "Epoch: 61, Batch: 331/842, Loss: 0.3011, Time: 0.79s\n",
      "Epoch: 61, Batch: 341/842, Loss: 0.3706, Time: 0.84s\n",
      "Epoch: 61, Batch: 351/842, Loss: 0.3945, Time: 0.84s\n",
      "Epoch: 61, Batch: 361/842, Loss: 0.4442, Time: 0.77s\n",
      "Epoch: 61, Batch: 371/842, Loss: 0.4789, Time: 0.76s\n",
      "Epoch: 61, Batch: 381/842, Loss: 0.4493, Time: 0.88s\n",
      "Epoch: 61, Batch: 391/842, Loss: 0.4677, Time: 0.90s\n",
      "Epoch: 61, Batch: 401/842, Loss: 0.3989, Time: 0.84s\n",
      "Epoch: 61, Batch: 411/842, Loss: 0.4225, Time: 0.84s\n",
      "Epoch: 61, Batch: 421/842, Loss: 0.5580, Time: 0.79s\n",
      "Epoch: 61, Batch: 431/842, Loss: 0.3522, Time: 0.80s\n",
      "Epoch: 61, Batch: 441/842, Loss: 0.4795, Time: 0.78s\n",
      "Epoch: 61, Batch: 451/842, Loss: 0.5438, Time: 0.78s\n",
      "Epoch: 61, Batch: 461/842, Loss: 0.5697, Time: 0.75s\n",
      "Epoch: 61, Batch: 471/842, Loss: 0.4226, Time: 0.76s\n",
      "Epoch: 61, Batch: 481/842, Loss: 0.4454, Time: 0.81s\n",
      "Epoch: 61, Batch: 491/842, Loss: 0.4014, Time: 0.76s\n",
      "Epoch: 61, Batch: 501/842, Loss: 0.5391, Time: 0.79s\n",
      "Epoch: 61, Batch: 511/842, Loss: 0.4759, Time: 0.75s\n",
      "Epoch: 61, Batch: 521/842, Loss: 0.5250, Time: 0.75s\n",
      "Epoch: 61, Batch: 531/842, Loss: 0.3698, Time: 0.72s\n",
      "Epoch: 61, Batch: 541/842, Loss: 0.3999, Time: 0.74s\n",
      "Epoch: 61, Batch: 551/842, Loss: 0.5064, Time: 0.78s\n",
      "Epoch: 61, Batch: 561/842, Loss: 0.3920, Time: 0.77s\n",
      "Epoch: 61, Batch: 571/842, Loss: 0.3219, Time: 0.79s\n",
      "Epoch: 61, Batch: 581/842, Loss: 0.2882, Time: 0.76s\n",
      "Epoch: 61, Batch: 591/842, Loss: 0.4614, Time: 0.74s\n",
      "Epoch: 61, Batch: 601/842, Loss: 0.3896, Time: 0.75s\n",
      "Epoch: 61, Batch: 611/842, Loss: 0.5633, Time: 0.76s\n",
      "Epoch: 61, Batch: 621/842, Loss: 0.3862, Time: 0.77s\n",
      "Epoch: 61, Batch: 631/842, Loss: 0.5011, Time: 0.81s\n",
      "Epoch: 61, Batch: 641/842, Loss: 0.5462, Time: 0.85s\n",
      "Epoch: 61, Batch: 651/842, Loss: 0.5397, Time: 0.77s\n",
      "Epoch: 61, Batch: 661/842, Loss: 0.3919, Time: 0.79s\n",
      "Epoch: 61, Batch: 671/842, Loss: 0.3672, Time: 0.79s\n",
      "Epoch: 61, Batch: 681/842, Loss: 0.5047, Time: 0.83s\n",
      "Epoch: 61, Batch: 691/842, Loss: 0.4406, Time: 0.83s\n",
      "Epoch: 61, Batch: 701/842, Loss: 0.4407, Time: 0.80s\n",
      "Epoch: 61, Batch: 711/842, Loss: 0.4880, Time: 0.78s\n",
      "Epoch: 61, Batch: 721/842, Loss: 0.4617, Time: 0.79s\n",
      "Epoch: 61, Batch: 731/842, Loss: 0.4301, Time: 0.80s\n",
      "Epoch: 61, Batch: 741/842, Loss: 0.4106, Time: 0.81s\n",
      "Epoch: 61, Batch: 751/842, Loss: 0.4500, Time: 0.77s\n",
      "Epoch: 61, Batch: 761/842, Loss: 0.4621, Time: 0.77s\n",
      "Epoch: 61, Batch: 771/842, Loss: 0.4677, Time: 0.79s\n",
      "Epoch: 61, Batch: 781/842, Loss: 0.4218, Time: 0.80s\n",
      "Epoch: 61, Batch: 791/842, Loss: 0.5016, Time: 0.77s\n",
      "Epoch: 61, Batch: 801/842, Loss: 0.4987, Time: 0.71s\n",
      "Epoch: 61, Batch: 811/842, Loss: 0.4459, Time: 0.76s\n",
      "Epoch: 61, Batch: 821/842, Loss: 0.4092, Time: 0.82s\n",
      "Epoch: 61, Batch: 831/842, Loss: 0.5283, Time: 0.83s\n",
      "Epoch: 61, Batch: 841/842, Loss: 0.5028, Time: 0.82s\n",
      "Epoch 61/100: Train Loss: 0.4576, Val Loss: 0.4331, mIoU: 0.6890, F1: 0.8096, OA: 0.9648\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7799, F1=0.8763, OA=0.8736, Precision=0.8293, Recall=0.9291\n",
      "    Class 1: IoU=0.7014, F1=0.8245, OA=0.9964, Precision=0.8484, Recall=0.8019\n",
      "    Class 2: IoU=0.9092, F1=0.9525, OA=0.9808, Precision=0.9577, Recall=0.9472\n",
      "    Class 3: IoU=0.4795, F1=0.6482, OA=0.9642, Precision=0.8043, Recall=0.5429\n",
      "    Class 4: IoU=0.7525, F1=0.8588, OA=0.9815, Precision=0.8928, Recall=0.8272\n",
      "    Class 5: IoU=0.6775, F1=0.8077, OA=0.9966, Precision=0.8047, Recall=0.8108\n",
      "    Class 6: IoU=0.5737, F1=0.7291, OA=0.9572, Precision=0.8074, Recall=0.6647\n",
      "    Class 7: IoU=0.6385, F1=0.7794, OA=0.9681, Precision=0.8710, Recall=0.7052\n",
      "Epoch: 62, Batch: 1/842, Loss: 0.4656, Time: 2.31s\n",
      "Epoch: 62, Batch: 11/842, Loss: 0.4171, Time: 0.82s\n",
      "Epoch: 62, Batch: 21/842, Loss: 0.4475, Time: 0.79s\n",
      "Epoch: 62, Batch: 31/842, Loss: 0.4306, Time: 0.82s\n",
      "Epoch: 62, Batch: 41/842, Loss: 0.4996, Time: 0.82s\n",
      "Epoch: 62, Batch: 51/842, Loss: 0.4214, Time: 0.81s\n",
      "Epoch: 62, Batch: 61/842, Loss: 0.3540, Time: 0.79s\n",
      "Epoch: 62, Batch: 71/842, Loss: 0.5041, Time: 0.77s\n",
      "Epoch: 62, Batch: 81/842, Loss: 0.4567, Time: 0.81s\n",
      "Epoch: 62, Batch: 91/842, Loss: 0.4875, Time: 0.78s\n",
      "Epoch: 62, Batch: 101/842, Loss: 0.4540, Time: 0.81s\n",
      "Epoch: 62, Batch: 111/842, Loss: 0.4353, Time: 0.80s\n",
      "Epoch: 62, Batch: 121/842, Loss: 0.5083, Time: 0.76s\n",
      "Epoch: 62, Batch: 131/842, Loss: 0.5701, Time: 0.80s\n",
      "Epoch: 62, Batch: 141/842, Loss: 0.5378, Time: 0.80s\n",
      "Epoch: 62, Batch: 151/842, Loss: 0.4256, Time: 0.80s\n",
      "Epoch: 62, Batch: 161/842, Loss: 0.4441, Time: 0.78s\n",
      "Epoch: 62, Batch: 171/842, Loss: 0.4421, Time: 0.82s\n",
      "Epoch: 62, Batch: 181/842, Loss: 0.5211, Time: 0.79s\n",
      "Epoch: 62, Batch: 191/842, Loss: 0.4788, Time: 0.79s\n",
      "Epoch: 62, Batch: 201/842, Loss: 0.5607, Time: 0.78s\n",
      "Epoch: 62, Batch: 211/842, Loss: 0.4041, Time: 0.78s\n",
      "Epoch: 62, Batch: 221/842, Loss: 0.4332, Time: 0.74s\n",
      "Epoch: 62, Batch: 231/842, Loss: 0.5117, Time: 0.74s\n",
      "Epoch: 62, Batch: 241/842, Loss: 0.5133, Time: 0.77s\n",
      "Epoch: 62, Batch: 251/842, Loss: 0.4641, Time: 0.77s\n",
      "Epoch: 62, Batch: 261/842, Loss: 0.4570, Time: 0.77s\n",
      "Epoch: 62, Batch: 271/842, Loss: 0.5276, Time: 0.76s\n",
      "Epoch: 62, Batch: 281/842, Loss: 0.4569, Time: 0.81s\n",
      "Epoch: 62, Batch: 291/842, Loss: 0.4103, Time: 0.77s\n",
      "Epoch: 62, Batch: 301/842, Loss: 0.4077, Time: 0.78s\n",
      "Epoch: 62, Batch: 311/842, Loss: 0.4874, Time: 0.77s\n",
      "Epoch: 62, Batch: 321/842, Loss: 0.5684, Time: 0.76s\n",
      "Epoch: 62, Batch: 331/842, Loss: 0.3803, Time: 0.80s\n",
      "Epoch: 62, Batch: 341/842, Loss: 0.6787, Time: 0.80s\n",
      "Epoch: 62, Batch: 351/842, Loss: 0.4870, Time: 0.79s\n",
      "Epoch: 62, Batch: 361/842, Loss: 0.3113, Time: 0.77s\n",
      "Epoch: 62, Batch: 371/842, Loss: 0.4083, Time: 0.77s\n",
      "Epoch: 62, Batch: 381/842, Loss: 0.3350, Time: 0.79s\n",
      "Epoch: 62, Batch: 391/842, Loss: 0.4807, Time: 0.77s\n",
      "Epoch: 62, Batch: 401/842, Loss: 0.4911, Time: 0.79s\n",
      "Epoch: 62, Batch: 411/842, Loss: 0.5559, Time: 0.80s\n",
      "Epoch: 62, Batch: 421/842, Loss: 0.3715, Time: 0.80s\n",
      "Epoch: 62, Batch: 431/842, Loss: 0.4239, Time: 0.82s\n",
      "Epoch: 62, Batch: 441/842, Loss: 0.4642, Time: 0.78s\n",
      "Epoch: 62, Batch: 451/842, Loss: 0.6158, Time: 0.83s\n",
      "Epoch: 62, Batch: 461/842, Loss: 0.4576, Time: 0.83s\n",
      "Epoch: 62, Batch: 471/842, Loss: 0.4270, Time: 0.81s\n",
      "Epoch: 62, Batch: 481/842, Loss: 0.4631, Time: 0.79s\n",
      "Epoch: 62, Batch: 491/842, Loss: 0.3044, Time: 0.82s\n",
      "Epoch: 62, Batch: 501/842, Loss: 0.3177, Time: 0.83s\n",
      "Epoch: 62, Batch: 511/842, Loss: 0.4781, Time: 0.81s\n",
      "Epoch: 62, Batch: 521/842, Loss: 0.4186, Time: 0.77s\n",
      "Epoch: 62, Batch: 531/842, Loss: 0.3601, Time: 0.77s\n",
      "Epoch: 62, Batch: 541/842, Loss: 0.3185, Time: 1.49s\n",
      "Epoch: 62, Batch: 551/842, Loss: 0.3831, Time: 0.79s\n",
      "Epoch: 62, Batch: 561/842, Loss: 0.4190, Time: 0.81s\n",
      "Epoch: 62, Batch: 571/842, Loss: 0.4792, Time: 0.77s\n",
      "Epoch: 62, Batch: 581/842, Loss: 0.3979, Time: 0.79s\n",
      "Epoch: 62, Batch: 591/842, Loss: 0.4369, Time: 0.80s\n",
      "Epoch: 62, Batch: 601/842, Loss: 0.4247, Time: 0.79s\n",
      "Epoch: 62, Batch: 611/842, Loss: 0.3890, Time: 0.83s\n",
      "Epoch: 62, Batch: 621/842, Loss: 0.4994, Time: 0.83s\n",
      "Epoch: 62, Batch: 631/842, Loss: 0.5536, Time: 0.84s\n",
      "Epoch: 62, Batch: 641/842, Loss: 0.4626, Time: 0.85s\n",
      "Epoch: 62, Batch: 651/842, Loss: 0.5841, Time: 0.85s\n",
      "Epoch: 62, Batch: 661/842, Loss: 0.5591, Time: 0.78s\n",
      "Epoch: 62, Batch: 671/842, Loss: 0.5079, Time: 0.79s\n",
      "Epoch: 62, Batch: 681/842, Loss: 0.6493, Time: 0.73s\n",
      "Epoch: 62, Batch: 691/842, Loss: 0.4381, Time: 0.73s\n",
      "Epoch: 62, Batch: 701/842, Loss: 0.4728, Time: 0.76s\n",
      "Epoch: 62, Batch: 711/842, Loss: 0.4995, Time: 0.73s\n",
      "Epoch: 62, Batch: 721/842, Loss: 0.5613, Time: 0.73s\n",
      "Epoch: 62, Batch: 731/842, Loss: 0.4359, Time: 0.75s\n",
      "Epoch: 62, Batch: 741/842, Loss: 0.4897, Time: 0.73s\n",
      "Epoch: 62, Batch: 751/842, Loss: 0.4352, Time: 0.73s\n",
      "Epoch: 62, Batch: 761/842, Loss: 0.3718, Time: 0.74s\n",
      "Epoch: 62, Batch: 771/842, Loss: 0.3405, Time: 0.75s\n",
      "Epoch: 62, Batch: 781/842, Loss: 0.4095, Time: 0.75s\n",
      "Epoch: 62, Batch: 791/842, Loss: 0.4201, Time: 0.76s\n",
      "Epoch: 62, Batch: 801/842, Loss: 0.3878, Time: 0.80s\n",
      "Epoch: 62, Batch: 811/842, Loss: 0.5270, Time: 0.77s\n",
      "Epoch: 62, Batch: 821/842, Loss: 0.4969, Time: 0.76s\n",
      "Epoch: 62, Batch: 831/842, Loss: 0.4655, Time: 0.76s\n",
      "Epoch: 62, Batch: 841/842, Loss: 0.3478, Time: 0.74s\n",
      "Epoch 62/100: Train Loss: 0.4561, Val Loss: 0.4253, mIoU: 0.6935, F1: 0.8128, OA: 0.9654\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7819, F1=0.8776, OA=0.8745, Precision=0.8283, Recall=0.9333\n",
      "    Class 1: IoU=0.7092, F1=0.8298, OA=0.9965, Precision=0.8655, Recall=0.7970\n",
      "    Class 2: IoU=0.9126, F1=0.9543, OA=0.9816, Precision=0.9589, Recall=0.9498\n",
      "    Class 3: IoU=0.4845, F1=0.6527, OA=0.9643, Precision=0.7999, Recall=0.5513\n",
      "    Class 4: IoU=0.7554, F1=0.8607, OA=0.9816, Precision=0.8893, Recall=0.8338\n",
      "    Class 5: IoU=0.6808, F1=0.8101, OA=0.9966, Precision=0.8132, Recall=0.8070\n",
      "    Class 6: IoU=0.5745, F1=0.7297, OA=0.9588, Precision=0.8452, Recall=0.6420\n",
      "    Class 7: IoU=0.6494, F1=0.7875, OA=0.9690, Precision=0.8731, Recall=0.7171\n",
      "Saving best model with mIoU: 0.6935\n",
      "Epoch: 63, Batch: 1/842, Loss: 0.5219, Time: 2.20s\n",
      "Epoch: 63, Batch: 11/842, Loss: 0.3937, Time: 0.76s\n",
      "Epoch: 63, Batch: 21/842, Loss: 0.5211, Time: 0.76s\n",
      "Epoch: 63, Batch: 31/842, Loss: 0.4214, Time: 0.78s\n",
      "Epoch: 63, Batch: 41/842, Loss: 0.5716, Time: 0.76s\n",
      "Epoch: 63, Batch: 51/842, Loss: 0.5797, Time: 0.77s\n",
      "Epoch: 63, Batch: 61/842, Loss: 0.4916, Time: 0.77s\n",
      "Epoch: 63, Batch: 71/842, Loss: 0.3546, Time: 0.77s\n",
      "Epoch: 63, Batch: 81/842, Loss: 0.4381, Time: 0.82s\n",
      "Epoch: 63, Batch: 91/842, Loss: 0.4013, Time: 0.80s\n",
      "Epoch: 63, Batch: 101/842, Loss: 0.4103, Time: 0.84s\n",
      "Epoch: 63, Batch: 111/842, Loss: 0.5067, Time: 0.81s\n",
      "Epoch: 63, Batch: 121/842, Loss: 0.4688, Time: 0.80s\n",
      "Epoch: 63, Batch: 131/842, Loss: 0.3596, Time: 0.87s\n",
      "Epoch: 63, Batch: 141/842, Loss: 0.6622, Time: 0.83s\n",
      "Epoch: 63, Batch: 151/842, Loss: 0.4922, Time: 0.86s\n",
      "Epoch: 63, Batch: 161/842, Loss: 0.4112, Time: 0.87s\n",
      "Epoch: 63, Batch: 171/842, Loss: 0.5107, Time: 0.92s\n",
      "Epoch: 63, Batch: 181/842, Loss: 0.3943, Time: 0.81s\n",
      "Epoch: 63, Batch: 191/842, Loss: 0.4730, Time: 0.78s\n",
      "Epoch: 63, Batch: 201/842, Loss: 0.4262, Time: 0.85s\n",
      "Epoch: 63, Batch: 211/842, Loss: 0.6021, Time: 0.84s\n",
      "Epoch: 63, Batch: 221/842, Loss: 0.3822, Time: 0.76s\n",
      "Epoch: 63, Batch: 231/842, Loss: 0.4142, Time: 0.86s\n",
      "Epoch: 63, Batch: 241/842, Loss: 0.4884, Time: 0.85s\n",
      "Epoch: 63, Batch: 251/842, Loss: 0.3943, Time: 0.81s\n",
      "Epoch: 63, Batch: 261/842, Loss: 0.4621, Time: 0.79s\n",
      "Epoch: 63, Batch: 271/842, Loss: 0.4508, Time: 0.79s\n",
      "Epoch: 63, Batch: 281/842, Loss: 0.5393, Time: 0.77s\n",
      "Epoch: 63, Batch: 291/842, Loss: 0.3920, Time: 0.91s\n",
      "Epoch: 63, Batch: 301/842, Loss: 0.4695, Time: 0.81s\n",
      "Epoch: 63, Batch: 311/842, Loss: 0.3716, Time: 0.82s\n",
      "Epoch: 63, Batch: 321/842, Loss: 0.4039, Time: 0.81s\n",
      "Epoch: 63, Batch: 331/842, Loss: 0.4206, Time: 0.81s\n",
      "Epoch: 63, Batch: 341/842, Loss: 0.4539, Time: 0.80s\n",
      "Epoch: 63, Batch: 351/842, Loss: 0.5263, Time: 0.80s\n",
      "Epoch: 63, Batch: 361/842, Loss: 0.3840, Time: 0.79s\n",
      "Epoch: 63, Batch: 371/842, Loss: 0.4768, Time: 0.75s\n",
      "Epoch: 63, Batch: 381/842, Loss: 0.4601, Time: 0.76s\n",
      "Epoch: 63, Batch: 391/842, Loss: 0.4062, Time: 0.78s\n",
      "Epoch: 63, Batch: 401/842, Loss: 0.3474, Time: 0.81s\n",
      "Epoch: 63, Batch: 411/842, Loss: 0.5213, Time: 0.78s\n",
      "Epoch: 63, Batch: 421/842, Loss: 0.4343, Time: 0.79s\n",
      "Epoch: 63, Batch: 431/842, Loss: 0.4595, Time: 0.77s\n",
      "Epoch: 63, Batch: 441/842, Loss: 0.5945, Time: 0.84s\n",
      "Epoch: 63, Batch: 451/842, Loss: 0.6916, Time: 0.86s\n",
      "Epoch: 63, Batch: 461/842, Loss: 0.3382, Time: 0.82s\n",
      "Epoch: 63, Batch: 471/842, Loss: 0.4421, Time: 0.75s\n",
      "Epoch: 63, Batch: 481/842, Loss: 0.4212, Time: 0.74s\n",
      "Epoch: 63, Batch: 491/842, Loss: 0.5122, Time: 0.79s\n",
      "Epoch: 63, Batch: 501/842, Loss: 0.4593, Time: 0.75s\n",
      "Epoch: 63, Batch: 511/842, Loss: 0.4609, Time: 0.78s\n",
      "Epoch: 63, Batch: 521/842, Loss: 0.4192, Time: 0.83s\n",
      "Epoch: 63, Batch: 531/842, Loss: 0.4771, Time: 0.81s\n",
      "Epoch: 63, Batch: 541/842, Loss: 0.3910, Time: 0.79s\n",
      "Epoch: 63, Batch: 551/842, Loss: 0.4472, Time: 0.80s\n",
      "Epoch: 63, Batch: 561/842, Loss: 0.4742, Time: 0.80s\n",
      "Epoch: 63, Batch: 571/842, Loss: 0.3704, Time: 0.81s\n",
      "Epoch: 63, Batch: 581/842, Loss: 0.4227, Time: 0.81s\n",
      "Epoch: 63, Batch: 591/842, Loss: 0.3886, Time: 0.77s\n",
      "Epoch: 63, Batch: 601/842, Loss: 0.4022, Time: 0.80s\n",
      "Epoch: 63, Batch: 611/842, Loss: 0.4947, Time: 0.79s\n",
      "Epoch: 63, Batch: 621/842, Loss: 0.4758, Time: 0.77s\n",
      "Epoch: 63, Batch: 631/842, Loss: 0.4542, Time: 0.74s\n",
      "Epoch: 63, Batch: 641/842, Loss: 0.5910, Time: 0.70s\n",
      "Epoch: 63, Batch: 651/842, Loss: 0.4442, Time: 0.76s\n",
      "Epoch: 63, Batch: 661/842, Loss: 0.5416, Time: 0.78s\n",
      "Epoch: 63, Batch: 671/842, Loss: 0.6805, Time: 0.78s\n",
      "Epoch: 63, Batch: 681/842, Loss: 0.5030, Time: 0.78s\n",
      "Epoch: 63, Batch: 691/842, Loss: 0.3524, Time: 0.75s\n",
      "Epoch: 63, Batch: 701/842, Loss: 0.4854, Time: 0.74s\n",
      "Epoch: 63, Batch: 711/842, Loss: 0.6247, Time: 0.80s\n",
      "Epoch: 63, Batch: 721/842, Loss: 0.4615, Time: 0.79s\n",
      "Epoch: 63, Batch: 731/842, Loss: 0.4383, Time: 0.75s\n",
      "Epoch: 63, Batch: 741/842, Loss: 0.3873, Time: 0.77s\n",
      "Epoch: 63, Batch: 751/842, Loss: 0.4151, Time: 0.77s\n",
      "Epoch: 63, Batch: 761/842, Loss: 0.5657, Time: 0.75s\n",
      "Epoch: 63, Batch: 771/842, Loss: 0.5262, Time: 0.77s\n",
      "Epoch: 63, Batch: 781/842, Loss: 0.5599, Time: 0.81s\n",
      "Epoch: 63, Batch: 791/842, Loss: 0.4607, Time: 0.80s\n",
      "Epoch: 63, Batch: 801/842, Loss: 0.4201, Time: 0.80s\n",
      "Epoch: 63, Batch: 811/842, Loss: 0.4943, Time: 0.81s\n",
      "Epoch: 63, Batch: 821/842, Loss: 0.6111, Time: 0.79s\n",
      "Epoch: 63, Batch: 831/842, Loss: 0.4040, Time: 0.77s\n",
      "Epoch: 63, Batch: 841/842, Loss: 0.5214, Time: 0.75s\n",
      "Epoch 63/100: Train Loss: 0.4545, Val Loss: 0.4272, mIoU: 0.6934, F1: 0.8129, OA: 0.9653\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7816, F1=0.8774, OA=0.8746, Precision=0.8299, Recall=0.9306\n",
      "    Class 1: IoU=0.7100, F1=0.8304, OA=0.9965, Precision=0.8607, Recall=0.8021\n",
      "    Class 2: IoU=0.9117, F1=0.9538, OA=0.9814, Precision=0.9592, Recall=0.9485\n",
      "    Class 3: IoU=0.4898, F1=0.6576, OA=0.9642, Precision=0.7859, Recall=0.5653\n",
      "    Class 4: IoU=0.7551, F1=0.8604, OA=0.9816, Precision=0.8890, Recall=0.8336\n",
      "    Class 5: IoU=0.6784, F1=0.8084, OA=0.9966, Precision=0.8068, Recall=0.8100\n",
      "    Class 6: IoU=0.5752, F1=0.7303, OA=0.9584, Precision=0.8346, Recall=0.6492\n",
      "    Class 7: IoU=0.6458, F1=0.7848, OA=0.9688, Precision=0.8752, Recall=0.7114\n",
      "Epoch: 64, Batch: 1/842, Loss: 0.3702, Time: 2.20s\n",
      "Epoch: 64, Batch: 11/842, Loss: 0.4408, Time: 0.79s\n",
      "Epoch: 64, Batch: 21/842, Loss: 0.5649, Time: 0.78s\n",
      "Epoch: 64, Batch: 31/842, Loss: 0.5451, Time: 0.79s\n",
      "Epoch: 64, Batch: 41/842, Loss: 0.4479, Time: 0.80s\n",
      "Epoch: 64, Batch: 51/842, Loss: 0.3825, Time: 0.79s\n",
      "Epoch: 64, Batch: 61/842, Loss: 0.3134, Time: 0.80s\n",
      "Epoch: 64, Batch: 71/842, Loss: 0.5839, Time: 0.80s\n",
      "Epoch: 64, Batch: 81/842, Loss: 0.4610, Time: 0.76s\n",
      "Epoch: 64, Batch: 91/842, Loss: 0.3038, Time: 0.77s\n",
      "Epoch: 64, Batch: 101/842, Loss: 0.4315, Time: 0.86s\n",
      "Epoch: 64, Batch: 111/842, Loss: 0.6093, Time: 0.84s\n",
      "Epoch: 64, Batch: 121/842, Loss: 0.3623, Time: 0.83s\n",
      "Epoch: 64, Batch: 131/842, Loss: 0.3397, Time: 0.86s\n",
      "Epoch: 64, Batch: 141/842, Loss: 0.5885, Time: 0.80s\n",
      "Epoch: 64, Batch: 151/842, Loss: 0.3857, Time: 0.77s\n",
      "Epoch: 64, Batch: 161/842, Loss: 0.3892, Time: 0.80s\n",
      "Epoch: 64, Batch: 171/842, Loss: 0.4887, Time: 0.84s\n",
      "Epoch: 64, Batch: 181/842, Loss: 0.4888, Time: 0.85s\n",
      "Epoch: 64, Batch: 191/842, Loss: 0.3355, Time: 0.85s\n",
      "Epoch: 64, Batch: 201/842, Loss: 0.4877, Time: 0.82s\n",
      "Epoch: 64, Batch: 211/842, Loss: 0.4769, Time: 0.79s\n",
      "Epoch: 64, Batch: 221/842, Loss: 0.6305, Time: 0.81s\n",
      "Epoch: 64, Batch: 231/842, Loss: 0.5386, Time: 0.84s\n",
      "Epoch: 64, Batch: 241/842, Loss: 0.4632, Time: 0.79s\n",
      "Epoch: 64, Batch: 251/842, Loss: 0.4404, Time: 0.79s\n",
      "Epoch: 64, Batch: 261/842, Loss: 0.3877, Time: 0.80s\n",
      "Epoch: 64, Batch: 271/842, Loss: 0.5024, Time: 0.78s\n",
      "Epoch: 64, Batch: 281/842, Loss: 0.4964, Time: 0.81s\n",
      "Epoch: 64, Batch: 291/842, Loss: 0.4865, Time: 0.84s\n",
      "Epoch: 64, Batch: 301/842, Loss: 0.4533, Time: 0.80s\n",
      "Epoch: 64, Batch: 311/842, Loss: 0.3978, Time: 0.76s\n",
      "Epoch: 64, Batch: 321/842, Loss: 0.4627, Time: 0.82s\n",
      "Epoch: 64, Batch: 331/842, Loss: 0.4414, Time: 0.80s\n",
      "Epoch: 64, Batch: 341/842, Loss: 0.4332, Time: 0.83s\n",
      "Epoch: 64, Batch: 351/842, Loss: 0.4004, Time: 0.82s\n",
      "Epoch: 64, Batch: 361/842, Loss: 0.3927, Time: 0.84s\n",
      "Epoch: 64, Batch: 371/842, Loss: 0.3944, Time: 0.83s\n",
      "Epoch: 64, Batch: 381/842, Loss: 0.3897, Time: 0.80s\n",
      "Epoch: 64, Batch: 391/842, Loss: 0.5248, Time: 0.89s\n",
      "Epoch: 64, Batch: 401/842, Loss: 0.5464, Time: 0.82s\n",
      "Epoch: 64, Batch: 411/842, Loss: 0.4414, Time: 0.80s\n",
      "Epoch: 64, Batch: 421/842, Loss: 0.3524, Time: 0.80s\n",
      "Epoch: 64, Batch: 431/842, Loss: 0.5767, Time: 0.83s\n",
      "Epoch: 64, Batch: 441/842, Loss: 0.4234, Time: 0.79s\n",
      "Epoch: 64, Batch: 451/842, Loss: 0.5221, Time: 0.78s\n",
      "Epoch: 64, Batch: 461/842, Loss: 0.5658, Time: 0.76s\n",
      "Epoch: 64, Batch: 471/842, Loss: 0.5018, Time: 0.77s\n",
      "Epoch: 64, Batch: 481/842, Loss: 0.5112, Time: 0.74s\n",
      "Epoch: 64, Batch: 491/842, Loss: 0.5855, Time: 0.75s\n",
      "Epoch: 64, Batch: 501/842, Loss: 0.4937, Time: 0.75s\n",
      "Epoch: 64, Batch: 511/842, Loss: 0.4532, Time: 0.78s\n",
      "Epoch: 64, Batch: 521/842, Loss: 0.5151, Time: 0.74s\n",
      "Epoch: 64, Batch: 531/842, Loss: 0.4021, Time: 0.75s\n",
      "Epoch: 64, Batch: 541/842, Loss: 0.4076, Time: 0.74s\n",
      "Epoch: 64, Batch: 551/842, Loss: 0.6068, Time: 0.75s\n",
      "Epoch: 64, Batch: 561/842, Loss: 0.4192, Time: 0.78s\n",
      "Epoch: 64, Batch: 571/842, Loss: 0.4578, Time: 0.84s\n",
      "Epoch: 64, Batch: 581/842, Loss: 0.4419, Time: 0.90s\n",
      "Epoch: 64, Batch: 591/842, Loss: 0.4492, Time: 0.92s\n",
      "Epoch: 64, Batch: 601/842, Loss: 0.5158, Time: 0.88s\n",
      "Epoch: 64, Batch: 611/842, Loss: 0.5512, Time: 0.88s\n",
      "Epoch: 64, Batch: 621/842, Loss: 0.5145, Time: 0.84s\n",
      "Epoch: 64, Batch: 631/842, Loss: 0.4041, Time: 0.83s\n",
      "Epoch: 64, Batch: 641/842, Loss: 0.4374, Time: 0.88s\n",
      "Epoch: 64, Batch: 651/842, Loss: 0.4085, Time: 0.85s\n",
      "Epoch: 64, Batch: 661/842, Loss: 0.5786, Time: 0.83s\n",
      "Epoch: 64, Batch: 671/842, Loss: 0.4546, Time: 0.85s\n",
      "Epoch: 64, Batch: 681/842, Loss: 0.3793, Time: 0.88s\n",
      "Epoch: 64, Batch: 691/842, Loss: 0.4357, Time: 0.89s\n",
      "Epoch: 64, Batch: 701/842, Loss: 0.4007, Time: 0.80s\n",
      "Epoch: 64, Batch: 711/842, Loss: 0.4334, Time: 0.83s\n",
      "Epoch: 64, Batch: 721/842, Loss: 0.4970, Time: 0.85s\n",
      "Epoch: 64, Batch: 731/842, Loss: 0.4838, Time: 0.82s\n",
      "Epoch: 64, Batch: 741/842, Loss: 0.4081, Time: 0.89s\n",
      "Epoch: 64, Batch: 751/842, Loss: 0.4745, Time: 0.84s\n",
      "Epoch: 64, Batch: 761/842, Loss: 0.5491, Time: 0.88s\n",
      "Epoch: 64, Batch: 771/842, Loss: 0.5179, Time: 0.81s\n",
      "Epoch: 64, Batch: 781/842, Loss: 0.3947, Time: 0.79s\n",
      "Epoch: 64, Batch: 791/842, Loss: 0.3905, Time: 0.85s\n",
      "Epoch: 64, Batch: 801/842, Loss: 0.3819, Time: 0.82s\n",
      "Epoch: 64, Batch: 811/842, Loss: 0.4591, Time: 0.81s\n",
      "Epoch: 64, Batch: 821/842, Loss: 0.3486, Time: 0.80s\n",
      "Epoch: 64, Batch: 831/842, Loss: 0.4841, Time: 0.82s\n",
      "Epoch: 64, Batch: 841/842, Loss: 0.4220, Time: 0.79s\n",
      "Epoch 64/100: Train Loss: 0.4573, Val Loss: 0.4235, mIoU: 0.6973, F1: 0.8154, OA: 0.9655\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7826, F1=0.8781, OA=0.8754, Precision=0.8310, Recall=0.9307\n",
      "    Class 1: IoU=0.7195, F1=0.8368, OA=0.9967, Precision=0.8762, Recall=0.8008\n",
      "    Class 2: IoU=0.9137, F1=0.9549, OA=0.9818, Precision=0.9592, Recall=0.9506\n",
      "    Class 3: IoU=0.4814, F1=0.6499, OA=0.9645, Precision=0.8104, Recall=0.5425\n",
      "    Class 4: IoU=0.7549, F1=0.8603, OA=0.9818, Precision=0.8997, Recall=0.8243\n",
      "    Class 5: IoU=0.6935, F1=0.8190, OA=0.9968, Precision=0.8389, Recall=0.8000\n",
      "    Class 6: IoU=0.5783, F1=0.7328, OA=0.9581, Precision=0.8182, Recall=0.6636\n",
      "    Class 7: IoU=0.6546, F1=0.7912, OA=0.9692, Precision=0.8644, Recall=0.7295\n",
      "Saving best model with mIoU: 0.6973\n",
      "Epoch: 65, Batch: 1/842, Loss: 0.5531, Time: 2.19s\n",
      "Epoch: 65, Batch: 11/842, Loss: 0.4656, Time: 0.89s\n",
      "Epoch: 65, Batch: 21/842, Loss: 0.3581, Time: 0.83s\n",
      "Epoch: 65, Batch: 31/842, Loss: 0.4720, Time: 0.85s\n",
      "Epoch: 65, Batch: 41/842, Loss: 0.4628, Time: 0.85s\n",
      "Epoch: 65, Batch: 51/842, Loss: 0.4986, Time: 0.93s\n",
      "Epoch: 65, Batch: 61/842, Loss: 0.4509, Time: 0.95s\n",
      "Epoch: 65, Batch: 71/842, Loss: 0.4462, Time: 0.83s\n",
      "Epoch: 65, Batch: 81/842, Loss: 0.5190, Time: 0.83s\n",
      "Epoch: 65, Batch: 91/842, Loss: 0.4608, Time: 0.85s\n",
      "Epoch: 65, Batch: 101/842, Loss: 0.4187, Time: 0.83s\n",
      "Epoch: 65, Batch: 111/842, Loss: 0.4511, Time: 0.86s\n",
      "Epoch: 65, Batch: 121/842, Loss: 0.4242, Time: 0.86s\n",
      "Epoch: 65, Batch: 131/842, Loss: 0.4043, Time: 0.83s\n",
      "Epoch: 65, Batch: 141/842, Loss: 0.4657, Time: 0.81s\n",
      "Epoch: 65, Batch: 151/842, Loss: 0.4690, Time: 0.81s\n",
      "Epoch: 65, Batch: 161/842, Loss: 0.5125, Time: 0.84s\n",
      "Epoch: 65, Batch: 171/842, Loss: 0.5059, Time: 0.81s\n",
      "Epoch: 65, Batch: 181/842, Loss: 0.4096, Time: 0.86s\n",
      "Epoch: 65, Batch: 191/842, Loss: 0.5122, Time: 0.85s\n",
      "Epoch: 65, Batch: 201/842, Loss: 0.4772, Time: 0.83s\n",
      "Epoch: 65, Batch: 211/842, Loss: 0.4230, Time: 0.85s\n",
      "Epoch: 65, Batch: 221/842, Loss: 0.5274, Time: 0.86s\n",
      "Epoch: 65, Batch: 231/842, Loss: 0.3855, Time: 0.84s\n",
      "Epoch: 65, Batch: 241/842, Loss: 0.4577, Time: 0.85s\n",
      "Epoch: 65, Batch: 251/842, Loss: 0.6612, Time: 0.90s\n",
      "Epoch: 65, Batch: 261/842, Loss: 0.3519, Time: 0.95s\n",
      "Epoch: 65, Batch: 271/842, Loss: 0.4752, Time: 0.83s\n",
      "Epoch: 65, Batch: 281/842, Loss: 0.5609, Time: 0.82s\n",
      "Epoch: 65, Batch: 291/842, Loss: 0.5707, Time: 0.92s\n",
      "Epoch: 65, Batch: 301/842, Loss: 0.4554, Time: 0.87s\n",
      "Epoch: 65, Batch: 311/842, Loss: 0.5107, Time: 0.84s\n",
      "Epoch: 65, Batch: 321/842, Loss: 0.5985, Time: 0.82s\n",
      "Epoch: 65, Batch: 331/842, Loss: 0.4452, Time: 0.82s\n",
      "Epoch: 65, Batch: 341/842, Loss: 0.3984, Time: 0.82s\n",
      "Epoch: 65, Batch: 351/842, Loss: 0.3291, Time: 0.79s\n",
      "Epoch: 65, Batch: 361/842, Loss: 0.4489, Time: 0.81s\n",
      "Epoch: 65, Batch: 371/842, Loss: 0.4718, Time: 0.79s\n",
      "Epoch: 65, Batch: 381/842, Loss: 0.4001, Time: 0.81s\n",
      "Epoch: 65, Batch: 391/842, Loss: 0.3667, Time: 0.82s\n",
      "Epoch: 65, Batch: 401/842, Loss: 0.4135, Time: 0.81s\n",
      "Epoch: 65, Batch: 411/842, Loss: 0.4759, Time: 0.84s\n",
      "Epoch: 65, Batch: 421/842, Loss: 0.5308, Time: 0.82s\n",
      "Epoch: 65, Batch: 431/842, Loss: 0.4923, Time: 0.80s\n",
      "Epoch: 65, Batch: 441/842, Loss: 0.4491, Time: 0.77s\n",
      "Epoch: 65, Batch: 451/842, Loss: 0.6425, Time: 0.76s\n",
      "Epoch: 65, Batch: 461/842, Loss: 0.4301, Time: 0.82s\n",
      "Epoch: 65, Batch: 471/842, Loss: 0.4031, Time: 0.82s\n",
      "Epoch: 65, Batch: 481/842, Loss: 0.4341, Time: 0.79s\n",
      "Epoch: 65, Batch: 491/842, Loss: 0.4858, Time: 0.81s\n",
      "Epoch: 65, Batch: 501/842, Loss: 0.5419, Time: 0.84s\n",
      "Epoch: 65, Batch: 511/842, Loss: 0.3842, Time: 0.89s\n",
      "Epoch: 65, Batch: 521/842, Loss: 0.4852, Time: 0.85s\n",
      "Epoch: 65, Batch: 531/842, Loss: 0.3322, Time: 0.85s\n",
      "Epoch: 65, Batch: 541/842, Loss: 0.5460, Time: 0.87s\n",
      "Epoch: 65, Batch: 551/842, Loss: 0.5701, Time: 0.78s\n",
      "Epoch: 65, Batch: 561/842, Loss: 0.5616, Time: 0.78s\n",
      "Epoch: 65, Batch: 571/842, Loss: 0.3329, Time: 0.81s\n",
      "Epoch: 65, Batch: 581/842, Loss: 0.4385, Time: 0.84s\n",
      "Epoch: 65, Batch: 591/842, Loss: 0.4969, Time: 0.86s\n",
      "Epoch: 65, Batch: 601/842, Loss: 0.4851, Time: 0.85s\n",
      "Epoch: 65, Batch: 611/842, Loss: 0.3568, Time: 0.84s\n",
      "Epoch: 65, Batch: 621/842, Loss: 0.4799, Time: 0.79s\n",
      "Epoch: 65, Batch: 631/842, Loss: 0.3528, Time: 0.88s\n",
      "Epoch: 65, Batch: 641/842, Loss: 0.4280, Time: 0.82s\n",
      "Epoch: 65, Batch: 651/842, Loss: 0.4079, Time: 0.77s\n",
      "Epoch: 65, Batch: 661/842, Loss: 0.3483, Time: 0.79s\n",
      "Epoch: 65, Batch: 671/842, Loss: 0.5279, Time: 0.88s\n",
      "Epoch: 65, Batch: 681/842, Loss: 0.5196, Time: 0.90s\n",
      "Epoch: 65, Batch: 691/842, Loss: 0.6501, Time: 0.88s\n",
      "Epoch: 65, Batch: 701/842, Loss: 0.4671, Time: 0.86s\n",
      "Epoch: 65, Batch: 711/842, Loss: 0.4910, Time: 0.92s\n",
      "Epoch: 65, Batch: 721/842, Loss: 0.3845, Time: 0.85s\n",
      "Epoch: 65, Batch: 731/842, Loss: 0.4332, Time: 0.86s\n",
      "Epoch: 65, Batch: 741/842, Loss: 0.5941, Time: 0.88s\n",
      "Epoch: 65, Batch: 751/842, Loss: 0.4883, Time: 0.87s\n",
      "Epoch: 65, Batch: 761/842, Loss: 0.5902, Time: 0.88s\n",
      "Epoch: 65, Batch: 771/842, Loss: 0.3498, Time: 0.87s\n",
      "Epoch: 65, Batch: 781/842, Loss: 0.3893, Time: 0.88s\n",
      "Epoch: 65, Batch: 791/842, Loss: 0.5160, Time: 0.88s\n",
      "Epoch: 65, Batch: 801/842, Loss: 0.4874, Time: 0.89s\n",
      "Epoch: 65, Batch: 811/842, Loss: 0.3623, Time: 0.87s\n",
      "Epoch: 65, Batch: 821/842, Loss: 0.4835, Time: 0.87s\n",
      "Epoch: 65, Batch: 831/842, Loss: 0.4603, Time: 0.81s\n",
      "Epoch: 65, Batch: 841/842, Loss: 0.4501, Time: 0.79s\n",
      "Epoch 65/100: Train Loss: 0.4581, Val Loss: 0.4237, mIoU: 0.6961, F1: 0.8146, OA: 0.9655\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7828, F1=0.8781, OA=0.8755, Precision=0.8311, Recall=0.9308\n",
      "    Class 1: IoU=0.7169, F1=0.8351, OA=0.9966, Precision=0.8709, Recall=0.8021\n",
      "    Class 2: IoU=0.9136, F1=0.9548, OA=0.9817, Precision=0.9557, Recall=0.9540\n",
      "    Class 3: IoU=0.4837, F1=0.6520, OA=0.9641, Precision=0.7940, Recall=0.5531\n",
      "    Class 4: IoU=0.7563, F1=0.8612, OA=0.9817, Precision=0.8920, Recall=0.8325\n",
      "    Class 5: IoU=0.6874, F1=0.8148, OA=0.9967, Precision=0.8209, Recall=0.8087\n",
      "    Class 6: IoU=0.5775, F1=0.7322, OA=0.9587, Precision=0.8353, Recall=0.6517\n",
      "    Class 7: IoU=0.6506, F1=0.7884, OA=0.9692, Precision=0.8752, Recall=0.7172\n",
      "Epoch: 66, Batch: 1/842, Loss: 0.5460, Time: 1.97s\n",
      "Epoch: 66, Batch: 11/842, Loss: 0.4566, Time: 0.87s\n",
      "Epoch: 66, Batch: 21/842, Loss: 0.4176, Time: 0.81s\n",
      "Epoch: 66, Batch: 31/842, Loss: 0.4702, Time: 0.82s\n",
      "Epoch: 66, Batch: 41/842, Loss: 0.3359, Time: 0.86s\n",
      "Epoch: 66, Batch: 51/842, Loss: 0.3504, Time: 0.80s\n",
      "Epoch: 66, Batch: 61/842, Loss: 0.3662, Time: 1.39s\n",
      "Epoch: 66, Batch: 71/842, Loss: 0.4097, Time: 0.81s\n",
      "Epoch: 66, Batch: 81/842, Loss: 0.4295, Time: 0.86s\n",
      "Epoch: 66, Batch: 91/842, Loss: 0.5095, Time: 0.78s\n",
      "Epoch: 66, Batch: 101/842, Loss: 0.6368, Time: 0.85s\n",
      "Epoch: 66, Batch: 111/842, Loss: 0.5657, Time: 0.82s\n",
      "Epoch: 66, Batch: 121/842, Loss: 0.3233, Time: 0.83s\n",
      "Epoch: 66, Batch: 131/842, Loss: 0.5234, Time: 0.78s\n",
      "Epoch: 66, Batch: 141/842, Loss: 0.5550, Time: 0.79s\n",
      "Epoch: 66, Batch: 151/842, Loss: 0.4028, Time: 0.77s\n",
      "Epoch: 66, Batch: 161/842, Loss: 0.4270, Time: 0.78s\n",
      "Epoch: 66, Batch: 171/842, Loss: 0.4216, Time: 0.78s\n",
      "Epoch: 66, Batch: 181/842, Loss: 0.5481, Time: 0.81s\n",
      "Epoch: 66, Batch: 191/842, Loss: 0.4248, Time: 0.79s\n",
      "Epoch: 66, Batch: 201/842, Loss: 0.6443, Time: 0.76s\n",
      "Epoch: 66, Batch: 211/842, Loss: 0.4778, Time: 0.75s\n",
      "Epoch: 66, Batch: 221/842, Loss: 0.2891, Time: 0.79s\n",
      "Epoch: 66, Batch: 231/842, Loss: 0.4386, Time: 0.78s\n",
      "Epoch: 66, Batch: 241/842, Loss: 0.4875, Time: 0.76s\n",
      "Epoch: 66, Batch: 251/842, Loss: 0.4955, Time: 0.74s\n",
      "Epoch: 66, Batch: 261/842, Loss: 0.5326, Time: 0.77s\n",
      "Epoch: 66, Batch: 271/842, Loss: 0.3663, Time: 0.79s\n",
      "Epoch: 66, Batch: 281/842, Loss: 0.5054, Time: 0.79s\n",
      "Epoch: 66, Batch: 291/842, Loss: 0.4157, Time: 0.86s\n",
      "Epoch: 66, Batch: 301/842, Loss: 0.6255, Time: 0.92s\n",
      "Epoch: 66, Batch: 311/842, Loss: 0.3761, Time: 0.92s\n",
      "Epoch: 66, Batch: 321/842, Loss: 0.5437, Time: 0.85s\n",
      "Epoch: 66, Batch: 331/842, Loss: 0.5086, Time: 0.83s\n",
      "Epoch: 66, Batch: 341/842, Loss: 0.4376, Time: 0.81s\n",
      "Epoch: 66, Batch: 351/842, Loss: 0.5522, Time: 0.85s\n",
      "Epoch: 66, Batch: 361/842, Loss: 0.6347, Time: 0.80s\n",
      "Epoch: 66, Batch: 371/842, Loss: 0.5268, Time: 0.81s\n",
      "Epoch: 66, Batch: 381/842, Loss: 0.3967, Time: 0.89s\n",
      "Epoch: 66, Batch: 391/842, Loss: 0.4015, Time: 0.81s\n",
      "Epoch: 66, Batch: 401/842, Loss: 0.6484, Time: 0.83s\n",
      "Epoch: 66, Batch: 411/842, Loss: 0.3640, Time: 0.84s\n",
      "Epoch: 66, Batch: 421/842, Loss: 0.5034, Time: 0.92s\n",
      "Epoch: 66, Batch: 431/842, Loss: 0.5390, Time: 0.94s\n",
      "Epoch: 66, Batch: 441/842, Loss: 0.2958, Time: 0.84s\n",
      "Epoch: 66, Batch: 451/842, Loss: 0.4149, Time: 0.86s\n",
      "Epoch: 66, Batch: 461/842, Loss: 0.5369, Time: 0.84s\n",
      "Epoch: 66, Batch: 471/842, Loss: 0.4206, Time: 0.87s\n",
      "Epoch: 66, Batch: 481/842, Loss: 0.5496, Time: 0.91s\n",
      "Epoch: 66, Batch: 491/842, Loss: 0.3684, Time: 0.88s\n",
      "Epoch: 66, Batch: 501/842, Loss: 0.5109, Time: 0.83s\n",
      "Epoch: 66, Batch: 511/842, Loss: 0.5162, Time: 0.85s\n",
      "Epoch: 66, Batch: 521/842, Loss: 0.5025, Time: 0.83s\n",
      "Epoch: 66, Batch: 531/842, Loss: 0.4286, Time: 0.90s\n",
      "Epoch: 66, Batch: 541/842, Loss: 0.5403, Time: 0.89s\n",
      "Epoch: 66, Batch: 551/842, Loss: 0.4688, Time: 0.85s\n",
      "Epoch: 66, Batch: 561/842, Loss: 0.4497, Time: 0.83s\n",
      "Epoch: 66, Batch: 571/842, Loss: 0.5862, Time: 0.83s\n",
      "Epoch: 66, Batch: 581/842, Loss: 0.4270, Time: 0.87s\n",
      "Epoch: 66, Batch: 591/842, Loss: 0.3432, Time: 0.79s\n",
      "Epoch: 66, Batch: 601/842, Loss: 0.4299, Time: 0.83s\n",
      "Epoch: 66, Batch: 611/842, Loss: 0.4437, Time: 0.97s\n",
      "Epoch: 66, Batch: 621/842, Loss: 0.4424, Time: 0.89s\n",
      "Epoch: 66, Batch: 631/842, Loss: 0.4420, Time: 0.87s\n",
      "Epoch: 66, Batch: 641/842, Loss: 0.4634, Time: 0.81s\n",
      "Epoch: 66, Batch: 651/842, Loss: 0.5822, Time: 0.78s\n",
      "Epoch: 66, Batch: 661/842, Loss: 0.4851, Time: 0.84s\n",
      "Epoch: 66, Batch: 671/842, Loss: 0.4297, Time: 0.81s\n",
      "Epoch: 66, Batch: 681/842, Loss: 0.6200, Time: 0.79s\n",
      "Epoch: 66, Batch: 691/842, Loss: 0.3586, Time: 0.81s\n",
      "Epoch: 66, Batch: 701/842, Loss: 0.5528, Time: 0.84s\n",
      "Epoch: 66, Batch: 711/842, Loss: 0.4943, Time: 0.77s\n",
      "Epoch: 66, Batch: 721/842, Loss: 0.5148, Time: 0.74s\n",
      "Epoch: 66, Batch: 731/842, Loss: 0.5106, Time: 0.77s\n",
      "Epoch: 66, Batch: 741/842, Loss: 0.4939, Time: 0.89s\n",
      "Epoch: 66, Batch: 751/842, Loss: 0.3819, Time: 0.83s\n",
      "Epoch: 66, Batch: 761/842, Loss: 0.5975, Time: 0.82s\n",
      "Epoch: 66, Batch: 771/842, Loss: 0.4167, Time: 0.87s\n",
      "Epoch: 66, Batch: 781/842, Loss: 0.6075, Time: 0.93s\n",
      "Epoch: 66, Batch: 791/842, Loss: 0.4343, Time: 0.82s\n",
      "Epoch: 66, Batch: 801/842, Loss: 0.4706, Time: 0.80s\n",
      "Epoch: 66, Batch: 811/842, Loss: 0.3229, Time: 0.74s\n",
      "Epoch: 66, Batch: 821/842, Loss: 0.5383, Time: 0.83s\n",
      "Epoch: 66, Batch: 831/842, Loss: 0.6477, Time: 0.81s\n",
      "Epoch: 66, Batch: 841/842, Loss: 0.4968, Time: 0.74s\n",
      "Epoch 66/100: Train Loss: 0.4576, Val Loss: 0.4245, mIoU: 0.6950, F1: 0.8137, OA: 0.9654\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7822, F1=0.8778, OA=0.8744, Precision=0.8270, Recall=0.9351\n",
      "    Class 1: IoU=0.7184, F1=0.8361, OA=0.9967, Precision=0.8834, Recall=0.7937\n",
      "    Class 2: IoU=0.9127, F1=0.9543, OA=0.9815, Precision=0.9557, Recall=0.9530\n",
      "    Class 3: IoU=0.4782, F1=0.6470, OA=0.9648, Precision=0.8306, Recall=0.5299\n",
      "    Class 4: IoU=0.7554, F1=0.8607, OA=0.9818, Precision=0.8994, Recall=0.8251\n",
      "    Class 5: IoU=0.6890, F1=0.8158, OA=0.9967, Precision=0.8247, Recall=0.8072\n",
      "    Class 6: IoU=0.5760, F1=0.7309, OA=0.9587, Precision=0.8388, Recall=0.6476\n",
      "    Class 7: IoU=0.6484, F1=0.7867, OA=0.9688, Precision=0.8682, Recall=0.7191\n",
      "Epoch: 67, Batch: 1/842, Loss: 0.4949, Time: 1.74s\n",
      "Epoch: 67, Batch: 11/842, Loss: 0.3419, Time: 0.75s\n",
      "Epoch: 67, Batch: 21/842, Loss: 0.4495, Time: 0.82s\n",
      "Epoch: 67, Batch: 31/842, Loss: 0.4737, Time: 0.75s\n",
      "Epoch: 67, Batch: 41/842, Loss: 0.4460, Time: 0.75s\n",
      "Epoch: 67, Batch: 51/842, Loss: 0.4516, Time: 0.74s\n",
      "Epoch: 67, Batch: 61/842, Loss: 0.4479, Time: 0.77s\n",
      "Epoch: 67, Batch: 71/842, Loss: 0.4196, Time: 0.74s\n",
      "Epoch: 67, Batch: 81/842, Loss: 0.4522, Time: 0.75s\n",
      "Epoch: 67, Batch: 91/842, Loss: 0.4839, Time: 0.77s\n",
      "Epoch: 67, Batch: 101/842, Loss: 0.4608, Time: 0.74s\n",
      "Epoch: 67, Batch: 111/842, Loss: 0.3460, Time: 0.74s\n",
      "Epoch: 67, Batch: 121/842, Loss: 0.4568, Time: 0.75s\n",
      "Epoch: 67, Batch: 131/842, Loss: 0.6070, Time: 0.81s\n",
      "Epoch: 67, Batch: 141/842, Loss: 0.5977, Time: 0.83s\n",
      "Epoch: 67, Batch: 151/842, Loss: 0.3758, Time: 0.79s\n",
      "Epoch: 67, Batch: 161/842, Loss: 0.4574, Time: 0.74s\n",
      "Epoch: 67, Batch: 171/842, Loss: 0.3929, Time: 0.73s\n",
      "Epoch: 67, Batch: 181/842, Loss: 0.4241, Time: 0.74s\n",
      "Epoch: 67, Batch: 191/842, Loss: 0.2675, Time: 0.74s\n",
      "Epoch: 67, Batch: 201/842, Loss: 0.3841, Time: 0.81s\n",
      "Epoch: 67, Batch: 211/842, Loss: 0.5240, Time: 0.74s\n",
      "Epoch: 67, Batch: 221/842, Loss: 0.4783, Time: 0.74s\n",
      "Epoch: 67, Batch: 231/842, Loss: 0.4741, Time: 0.77s\n",
      "Epoch: 67, Batch: 241/842, Loss: 0.5558, Time: 0.76s\n",
      "Epoch: 67, Batch: 251/842, Loss: 0.4263, Time: 0.75s\n",
      "Epoch: 67, Batch: 261/842, Loss: 0.4666, Time: 0.75s\n",
      "Epoch: 67, Batch: 271/842, Loss: 0.3785, Time: 0.77s\n",
      "Epoch: 67, Batch: 281/842, Loss: 0.4611, Time: 0.74s\n",
      "Epoch: 67, Batch: 291/842, Loss: 0.5159, Time: 0.77s\n",
      "Epoch: 67, Batch: 301/842, Loss: 0.4541, Time: 0.79s\n",
      "Epoch: 67, Batch: 311/842, Loss: 0.5047, Time: 0.81s\n",
      "Epoch: 67, Batch: 321/842, Loss: 0.4028, Time: 0.77s\n",
      "Epoch: 67, Batch: 331/842, Loss: 0.4879, Time: 0.80s\n",
      "Epoch: 67, Batch: 341/842, Loss: 0.4074, Time: 0.79s\n",
      "Epoch: 67, Batch: 351/842, Loss: 0.4459, Time: 0.82s\n",
      "Epoch: 67, Batch: 361/842, Loss: 0.5163, Time: 0.87s\n",
      "Epoch: 67, Batch: 371/842, Loss: 0.4595, Time: 0.84s\n",
      "Epoch: 67, Batch: 381/842, Loss: 0.4124, Time: 0.84s\n",
      "Epoch: 67, Batch: 391/842, Loss: 0.5007, Time: 0.81s\n",
      "Epoch: 67, Batch: 401/842, Loss: 0.4034, Time: 0.77s\n",
      "Epoch: 67, Batch: 411/842, Loss: 0.4048, Time: 0.77s\n",
      "Epoch: 67, Batch: 421/842, Loss: 0.4163, Time: 0.77s\n",
      "Epoch: 67, Batch: 431/842, Loss: 0.5112, Time: 0.79s\n",
      "Epoch: 67, Batch: 441/842, Loss: 0.3778, Time: 0.77s\n",
      "Epoch: 67, Batch: 451/842, Loss: 0.3571, Time: 0.80s\n",
      "Epoch: 67, Batch: 461/842, Loss: 0.4486, Time: 0.81s\n",
      "Epoch: 67, Batch: 471/842, Loss: 0.4149, Time: 0.76s\n",
      "Epoch: 67, Batch: 481/842, Loss: 0.4637, Time: 0.73s\n",
      "Epoch: 67, Batch: 491/842, Loss: 0.4260, Time: 0.76s\n",
      "Epoch: 67, Batch: 501/842, Loss: 0.4288, Time: 0.80s\n",
      "Epoch: 67, Batch: 511/842, Loss: 0.4819, Time: 0.80s\n",
      "Epoch: 67, Batch: 521/842, Loss: 0.4314, Time: 0.80s\n",
      "Epoch: 67, Batch: 531/842, Loss: 0.5827, Time: 0.85s\n",
      "Epoch: 67, Batch: 541/842, Loss: 0.3977, Time: 0.76s\n",
      "Epoch: 67, Batch: 551/842, Loss: 0.5495, Time: 0.77s\n",
      "Epoch: 67, Batch: 561/842, Loss: 0.4929, Time: 0.75s\n",
      "Epoch: 67, Batch: 571/842, Loss: 0.3935, Time: 0.77s\n",
      "Epoch: 67, Batch: 581/842, Loss: 0.4181, Time: 0.79s\n",
      "Epoch: 67, Batch: 591/842, Loss: 0.4979, Time: 0.76s\n",
      "Epoch: 67, Batch: 601/842, Loss: 0.4282, Time: 0.78s\n",
      "Epoch: 67, Batch: 611/842, Loss: 0.5121, Time: 0.82s\n",
      "Epoch: 67, Batch: 621/842, Loss: 0.4894, Time: 0.79s\n",
      "Epoch: 67, Batch: 631/842, Loss: 0.4668, Time: 0.87s\n",
      "Epoch: 67, Batch: 641/842, Loss: 0.5448, Time: 0.91s\n",
      "Epoch: 67, Batch: 651/842, Loss: 0.5278, Time: 0.83s\n",
      "Epoch: 67, Batch: 661/842, Loss: 0.3646, Time: 0.86s\n",
      "Epoch: 67, Batch: 671/842, Loss: 0.4469, Time: 0.80s\n",
      "Epoch: 67, Batch: 681/842, Loss: 0.4532, Time: 0.86s\n",
      "Epoch: 67, Batch: 691/842, Loss: 0.5180, Time: 0.88s\n",
      "Epoch: 67, Batch: 701/842, Loss: 0.4225, Time: 0.84s\n",
      "Epoch: 67, Batch: 711/842, Loss: 0.5861, Time: 0.84s\n",
      "Epoch: 67, Batch: 721/842, Loss: 0.5346, Time: 0.83s\n",
      "Epoch: 67, Batch: 731/842, Loss: 0.5011, Time: 0.82s\n",
      "Epoch: 67, Batch: 741/842, Loss: 0.3406, Time: 0.81s\n",
      "Epoch: 67, Batch: 751/842, Loss: 0.4649, Time: 0.83s\n",
      "Epoch: 67, Batch: 761/842, Loss: 0.4495, Time: 0.80s\n",
      "Epoch: 67, Batch: 771/842, Loss: 0.4708, Time: 0.84s\n",
      "Epoch: 67, Batch: 781/842, Loss: 0.5213, Time: 0.86s\n",
      "Epoch: 67, Batch: 791/842, Loss: 0.5118, Time: 0.84s\n",
      "Epoch: 67, Batch: 801/842, Loss: 0.4113, Time: 0.81s\n",
      "Epoch: 67, Batch: 811/842, Loss: 0.3371, Time: 0.77s\n",
      "Epoch: 67, Batch: 821/842, Loss: 0.5662, Time: 0.78s\n",
      "Epoch: 67, Batch: 831/842, Loss: 0.5429, Time: 0.79s\n",
      "Epoch: 67, Batch: 841/842, Loss: 0.4333, Time: 0.75s\n",
      "Epoch 67/100: Train Loss: 0.4579, Val Loss: 0.4248, mIoU: 0.6942, F1: 0.8134, OA: 0.9654\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7823, F1=0.8779, OA=0.8755, Precision=0.8331, Recall=0.9277\n",
      "    Class 1: IoU=0.7114, F1=0.8313, OA=0.9965, Precision=0.8575, Recall=0.8067\n",
      "    Class 2: IoU=0.9122, F1=0.9541, OA=0.9815, Precision=0.9593, Recall=0.9490\n",
      "    Class 3: IoU=0.4880, F1=0.6559, OA=0.9648, Precision=0.8082, Recall=0.5519\n",
      "    Class 4: IoU=0.7557, F1=0.8609, OA=0.9813, Precision=0.8734, Recall=0.8487\n",
      "    Class 5: IoU=0.6757, F1=0.8065, OA=0.9965, Precision=0.7952, Recall=0.8180\n",
      "    Class 6: IoU=0.5785, F1=0.7330, OA=0.9586, Precision=0.8317, Recall=0.6552\n",
      "    Class 7: IoU=0.6496, F1=0.7876, OA=0.9687, Precision=0.8610, Recall=0.7257\n",
      "Epoch: 68, Batch: 1/842, Loss: 0.5056, Time: 1.42s\n",
      "Epoch: 68, Batch: 11/842, Loss: 0.4890, Time: 0.79s\n",
      "Epoch: 68, Batch: 21/842, Loss: 0.5076, Time: 0.80s\n",
      "Epoch: 68, Batch: 31/842, Loss: 0.6105, Time: 0.80s\n",
      "Epoch: 68, Batch: 41/842, Loss: 0.4107, Time: 0.76s\n",
      "Epoch: 68, Batch: 51/842, Loss: 0.5487, Time: 0.80s\n",
      "Epoch: 68, Batch: 61/842, Loss: 0.4547, Time: 0.79s\n",
      "Epoch: 68, Batch: 71/842, Loss: 0.3817, Time: 0.82s\n",
      "Epoch: 68, Batch: 81/842, Loss: 0.4243, Time: 0.81s\n",
      "Epoch: 68, Batch: 91/842, Loss: 0.4886, Time: 0.75s\n",
      "Epoch: 68, Batch: 101/842, Loss: 0.4564, Time: 0.81s\n",
      "Epoch: 68, Batch: 111/842, Loss: 0.4247, Time: 0.83s\n",
      "Epoch: 68, Batch: 121/842, Loss: 0.4175, Time: 0.79s\n",
      "Epoch: 68, Batch: 131/842, Loss: 0.5011, Time: 0.76s\n",
      "Epoch: 68, Batch: 141/842, Loss: 0.4760, Time: 0.74s\n",
      "Epoch: 68, Batch: 151/842, Loss: 0.4436, Time: 0.71s\n",
      "Epoch: 68, Batch: 161/842, Loss: 0.4862, Time: 0.77s\n",
      "Epoch: 68, Batch: 171/842, Loss: 0.3890, Time: 0.72s\n",
      "Epoch: 68, Batch: 181/842, Loss: 0.5104, Time: 0.74s\n",
      "Epoch: 68, Batch: 191/842, Loss: 0.3860, Time: 0.80s\n",
      "Epoch: 68, Batch: 201/842, Loss: 0.3644, Time: 0.78s\n",
      "Epoch: 68, Batch: 211/842, Loss: 0.4020, Time: 0.72s\n",
      "Epoch: 68, Batch: 221/842, Loss: 0.4878, Time: 0.73s\n",
      "Epoch: 68, Batch: 231/842, Loss: 0.4329, Time: 0.73s\n",
      "Epoch: 68, Batch: 241/842, Loss: 0.5633, Time: 0.74s\n",
      "Epoch: 68, Batch: 251/842, Loss: 0.3762, Time: 0.73s\n",
      "Epoch: 68, Batch: 261/842, Loss: 0.3854, Time: 0.74s\n",
      "Epoch: 68, Batch: 271/842, Loss: 0.4459, Time: 0.73s\n",
      "Epoch: 68, Batch: 281/842, Loss: 0.5939, Time: 0.76s\n",
      "Epoch: 68, Batch: 291/842, Loss: 0.5034, Time: 0.72s\n",
      "Epoch: 68, Batch: 301/842, Loss: 0.5418, Time: 0.74s\n",
      "Epoch: 68, Batch: 311/842, Loss: 0.3472, Time: 0.74s\n",
      "Epoch: 68, Batch: 321/842, Loss: 0.6385, Time: 0.76s\n",
      "Epoch: 68, Batch: 331/842, Loss: 0.2850, Time: 0.78s\n",
      "Epoch: 68, Batch: 341/842, Loss: 0.5802, Time: 0.75s\n",
      "Epoch: 68, Batch: 351/842, Loss: 0.4434, Time: 0.73s\n",
      "Epoch: 68, Batch: 361/842, Loss: 0.3663, Time: 0.76s\n",
      "Epoch: 68, Batch: 371/842, Loss: 0.4936, Time: 0.71s\n",
      "Epoch: 68, Batch: 381/842, Loss: 0.4809, Time: 0.72s\n",
      "Epoch: 68, Batch: 391/842, Loss: 0.4879, Time: 0.76s\n",
      "Epoch: 68, Batch: 401/842, Loss: 0.4225, Time: 0.75s\n",
      "Epoch: 68, Batch: 411/842, Loss: 0.3693, Time: 0.77s\n",
      "Epoch: 68, Batch: 421/842, Loss: 0.2876, Time: 0.78s\n",
      "Epoch: 68, Batch: 431/842, Loss: 0.4385, Time: 0.78s\n",
      "Epoch: 68, Batch: 441/842, Loss: 0.3580, Time: 0.75s\n",
      "Epoch: 68, Batch: 451/842, Loss: 0.4849, Time: 0.71s\n",
      "Epoch: 68, Batch: 461/842, Loss: 0.4159, Time: 0.74s\n",
      "Epoch: 68, Batch: 471/842, Loss: 0.5709, Time: 0.77s\n",
      "Epoch: 68, Batch: 481/842, Loss: 0.5172, Time: 0.75s\n",
      "Epoch: 68, Batch: 491/842, Loss: 0.3963, Time: 0.74s\n",
      "Epoch: 68, Batch: 501/842, Loss: 0.4460, Time: 0.72s\n",
      "Epoch: 68, Batch: 511/842, Loss: 0.5424, Time: 0.76s\n",
      "Epoch: 68, Batch: 521/842, Loss: 0.4474, Time: 0.78s\n",
      "Epoch: 68, Batch: 531/842, Loss: 0.3403, Time: 0.74s\n",
      "Epoch: 68, Batch: 541/842, Loss: 0.4763, Time: 0.78s\n",
      "Epoch: 68, Batch: 551/842, Loss: 0.3376, Time: 0.78s\n",
      "Epoch: 68, Batch: 561/842, Loss: 0.3762, Time: 0.80s\n",
      "Epoch: 68, Batch: 571/842, Loss: 0.3828, Time: 0.73s\n",
      "Epoch: 68, Batch: 581/842, Loss: 0.4576, Time: 0.75s\n",
      "Epoch: 68, Batch: 591/842, Loss: 0.2874, Time: 0.80s\n",
      "Epoch: 68, Batch: 601/842, Loss: 0.4048, Time: 0.75s\n",
      "Epoch: 68, Batch: 611/842, Loss: 0.4105, Time: 0.77s\n",
      "Epoch: 68, Batch: 621/842, Loss: 0.5856, Time: 0.78s\n",
      "Epoch: 68, Batch: 631/842, Loss: 0.4249, Time: 0.78s\n",
      "Epoch: 68, Batch: 641/842, Loss: 0.3030, Time: 0.76s\n",
      "Epoch: 68, Batch: 651/842, Loss: 0.3750, Time: 0.75s\n",
      "Epoch: 68, Batch: 661/842, Loss: 0.4628, Time: 0.72s\n",
      "Epoch: 68, Batch: 671/842, Loss: 0.5162, Time: 0.78s\n",
      "Epoch: 68, Batch: 681/842, Loss: 0.3789, Time: 0.76s\n",
      "Epoch: 68, Batch: 691/842, Loss: 0.3996, Time: 0.74s\n",
      "Epoch: 68, Batch: 701/842, Loss: 0.4592, Time: 0.75s\n",
      "Epoch: 68, Batch: 711/842, Loss: 0.5746, Time: 0.77s\n",
      "Epoch: 68, Batch: 721/842, Loss: 0.5265, Time: 0.74s\n",
      "Epoch: 68, Batch: 731/842, Loss: 0.3393, Time: 0.75s\n",
      "Epoch: 68, Batch: 741/842, Loss: 0.4196, Time: 0.77s\n",
      "Epoch: 68, Batch: 751/842, Loss: 0.3814, Time: 0.77s\n",
      "Epoch: 68, Batch: 761/842, Loss: 0.5141, Time: 0.78s\n",
      "Epoch: 68, Batch: 771/842, Loss: 0.4229, Time: 0.79s\n",
      "Epoch: 68, Batch: 781/842, Loss: 0.6359, Time: 0.80s\n",
      "Epoch: 68, Batch: 791/842, Loss: 0.4546, Time: 0.84s\n",
      "Epoch: 68, Batch: 801/842, Loss: 0.5331, Time: 0.80s\n",
      "Epoch: 68, Batch: 811/842, Loss: 0.5248, Time: 0.81s\n",
      "Epoch: 68, Batch: 821/842, Loss: 0.4436, Time: 0.73s\n",
      "Epoch: 68, Batch: 831/842, Loss: 0.6001, Time: 0.73s\n",
      "Epoch: 68, Batch: 841/842, Loss: 0.4741, Time: 0.71s\n",
      "Epoch 68/100: Train Loss: 0.4584, Val Loss: 0.4244, mIoU: 0.6940, F1: 0.8131, OA: 0.9655\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7823, F1=0.8779, OA=0.8745, Precision=0.8268, Recall=0.9357\n",
      "    Class 1: IoU=0.7204, F1=0.8375, OA=0.9967, Precision=0.8799, Recall=0.7989\n",
      "    Class 2: IoU=0.9133, F1=0.9547, OA=0.9817, Precision=0.9592, Recall=0.9502\n",
      "    Class 3: IoU=0.4853, F1=0.6534, OA=0.9648, Precision=0.8156, Recall=0.5450\n",
      "    Class 4: IoU=0.7553, F1=0.8606, OA=0.9816, Precision=0.8913, Recall=0.8319\n",
      "    Class 5: IoU=0.6713, F1=0.8034, OA=0.9964, Precision=0.7888, Recall=0.8185\n",
      "    Class 6: IoU=0.5748, F1=0.7300, OA=0.9589, Precision=0.8488, Recall=0.6404\n",
      "    Class 7: IoU=0.6498, F1=0.7877, OA=0.9692, Precision=0.8784, Recall=0.7140\n",
      "Epoch: 69, Batch: 1/842, Loss: 0.6076, Time: 1.53s\n",
      "Epoch: 69, Batch: 11/842, Loss: 0.4857, Time: 0.80s\n",
      "Epoch: 69, Batch: 21/842, Loss: 0.4621, Time: 0.73s\n",
      "Epoch: 69, Batch: 31/842, Loss: 0.4913, Time: 0.74s\n",
      "Epoch: 69, Batch: 41/842, Loss: 0.3888, Time: 0.75s\n",
      "Epoch: 69, Batch: 51/842, Loss: 0.4896, Time: 0.81s\n",
      "Epoch: 69, Batch: 61/842, Loss: 0.4529, Time: 0.73s\n",
      "Epoch: 69, Batch: 71/842, Loss: 0.5979, Time: 0.72s\n",
      "Epoch: 69, Batch: 81/842, Loss: 0.4282, Time: 0.72s\n",
      "Epoch: 69, Batch: 91/842, Loss: 0.4545, Time: 0.74s\n",
      "Epoch: 69, Batch: 101/842, Loss: 0.3034, Time: 0.76s\n",
      "Epoch: 69, Batch: 111/842, Loss: 0.3722, Time: 0.73s\n",
      "Epoch: 69, Batch: 121/842, Loss: 0.3376, Time: 0.78s\n",
      "Epoch: 69, Batch: 131/842, Loss: 0.5675, Time: 0.78s\n",
      "Epoch: 69, Batch: 141/842, Loss: 0.5363, Time: 0.77s\n",
      "Epoch: 69, Batch: 151/842, Loss: 0.4087, Time: 0.78s\n",
      "Epoch: 69, Batch: 161/842, Loss: 0.5032, Time: 0.75s\n",
      "Epoch: 69, Batch: 171/842, Loss: 0.5333, Time: 0.85s\n",
      "Epoch: 69, Batch: 181/842, Loss: 0.5117, Time: 0.86s\n",
      "Epoch: 69, Batch: 191/842, Loss: 0.4817, Time: 0.77s\n",
      "Epoch: 69, Batch: 201/842, Loss: 0.4211, Time: 0.72s\n",
      "Epoch: 69, Batch: 211/842, Loss: 0.4997, Time: 0.78s\n",
      "Epoch: 69, Batch: 221/842, Loss: 0.4318, Time: 0.75s\n",
      "Epoch: 69, Batch: 231/842, Loss: 0.2734, Time: 0.73s\n",
      "Epoch: 69, Batch: 241/842, Loss: 0.6792, Time: 0.70s\n",
      "Epoch: 69, Batch: 251/842, Loss: 0.4062, Time: 0.71s\n",
      "Epoch: 69, Batch: 261/842, Loss: 0.4625, Time: 0.72s\n",
      "Epoch: 69, Batch: 271/842, Loss: 0.3937, Time: 0.71s\n",
      "Epoch: 69, Batch: 281/842, Loss: 0.4511, Time: 0.77s\n",
      "Epoch: 69, Batch: 291/842, Loss: 0.6171, Time: 0.78s\n",
      "Epoch: 69, Batch: 301/842, Loss: 0.5378, Time: 0.74s\n",
      "Epoch: 69, Batch: 311/842, Loss: 0.4609, Time: 0.67s\n",
      "Epoch: 69, Batch: 321/842, Loss: 0.3645, Time: 0.75s\n",
      "Epoch: 69, Batch: 331/842, Loss: 0.5462, Time: 0.80s\n",
      "Epoch: 69, Batch: 341/842, Loss: 0.4376, Time: 0.81s\n",
      "Epoch: 69, Batch: 351/842, Loss: 0.4097, Time: 0.77s\n",
      "Epoch: 69, Batch: 361/842, Loss: 0.3983, Time: 0.74s\n",
      "Epoch: 69, Batch: 371/842, Loss: 0.4475, Time: 0.79s\n",
      "Epoch: 69, Batch: 381/842, Loss: 0.5225, Time: 0.74s\n",
      "Epoch: 69, Batch: 391/842, Loss: 0.4511, Time: 0.72s\n",
      "Epoch: 69, Batch: 401/842, Loss: 0.4183, Time: 0.75s\n",
      "Epoch: 69, Batch: 411/842, Loss: 0.4112, Time: 0.74s\n",
      "Epoch: 69, Batch: 421/842, Loss: 0.5817, Time: 0.76s\n",
      "Epoch: 69, Batch: 431/842, Loss: 0.5103, Time: 0.76s\n",
      "Epoch: 69, Batch: 441/842, Loss: 0.3220, Time: 0.74s\n",
      "Epoch: 69, Batch: 451/842, Loss: 0.4808, Time: 0.72s\n",
      "Epoch: 69, Batch: 461/842, Loss: 0.4861, Time: 0.75s\n",
      "Epoch: 69, Batch: 471/842, Loss: 0.4899, Time: 0.76s\n",
      "Epoch: 69, Batch: 481/842, Loss: 0.3485, Time: 0.70s\n",
      "Epoch: 69, Batch: 491/842, Loss: 0.4244, Time: 0.73s\n",
      "Epoch: 69, Batch: 501/842, Loss: 0.5346, Time: 0.74s\n",
      "Epoch: 69, Batch: 511/842, Loss: 0.4421, Time: 0.77s\n",
      "Epoch: 69, Batch: 521/842, Loss: 0.4661, Time: 1.63s\n",
      "Epoch: 69, Batch: 531/842, Loss: 0.4456, Time: 0.76s\n",
      "Epoch: 69, Batch: 541/842, Loss: 0.5102, Time: 0.76s\n",
      "Epoch: 69, Batch: 551/842, Loss: 0.5079, Time: 0.77s\n",
      "Epoch: 69, Batch: 561/842, Loss: 0.4444, Time: 0.79s\n",
      "Epoch: 69, Batch: 571/842, Loss: 0.4771, Time: 0.72s\n",
      "Epoch: 69, Batch: 581/842, Loss: 0.4994, Time: 0.75s\n",
      "Epoch: 69, Batch: 591/842, Loss: 0.4106, Time: 0.75s\n",
      "Epoch: 69, Batch: 601/842, Loss: 0.6146, Time: 0.73s\n",
      "Epoch: 69, Batch: 611/842, Loss: 0.3991, Time: 0.73s\n",
      "Epoch: 69, Batch: 621/842, Loss: 0.3388, Time: 0.75s\n",
      "Epoch: 69, Batch: 631/842, Loss: 0.4482, Time: 0.75s\n",
      "Epoch: 69, Batch: 641/842, Loss: 0.6099, Time: 0.75s\n",
      "Epoch: 69, Batch: 651/842, Loss: 0.3234, Time: 0.74s\n",
      "Epoch: 69, Batch: 661/842, Loss: 0.4868, Time: 0.75s\n",
      "Epoch: 69, Batch: 671/842, Loss: 0.4973, Time: 0.80s\n",
      "Epoch: 69, Batch: 681/842, Loss: 0.4356, Time: 0.78s\n",
      "Epoch: 69, Batch: 691/842, Loss: 0.5094, Time: 0.75s\n",
      "Epoch: 69, Batch: 701/842, Loss: 0.3771, Time: 0.77s\n",
      "Epoch: 69, Batch: 711/842, Loss: 0.3938, Time: 0.69s\n",
      "Epoch: 69, Batch: 721/842, Loss: 0.4657, Time: 0.72s\n",
      "Epoch: 69, Batch: 731/842, Loss: 0.4045, Time: 0.75s\n",
      "Epoch: 69, Batch: 741/842, Loss: 0.4453, Time: 0.78s\n",
      "Epoch: 69, Batch: 751/842, Loss: 0.4260, Time: 0.76s\n",
      "Epoch: 69, Batch: 761/842, Loss: 0.4926, Time: 0.74s\n",
      "Epoch: 69, Batch: 771/842, Loss: 0.3349, Time: 0.75s\n",
      "Epoch: 69, Batch: 781/842, Loss: 0.4752, Time: 0.73s\n",
      "Epoch: 69, Batch: 791/842, Loss: 0.4425, Time: 0.74s\n",
      "Epoch: 69, Batch: 801/842, Loss: 0.4384, Time: 0.74s\n",
      "Epoch: 69, Batch: 811/842, Loss: 0.4097, Time: 0.72s\n",
      "Epoch: 69, Batch: 821/842, Loss: 0.4504, Time: 0.77s\n",
      "Epoch: 69, Batch: 831/842, Loss: 0.4956, Time: 0.79s\n",
      "Epoch: 69, Batch: 841/842, Loss: 0.4572, Time: 0.78s\n",
      "Epoch 69/100: Train Loss: 0.4545, Val Loss: 0.4299, mIoU: 0.6898, F1: 0.8103, OA: 0.9650\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7809, F1=0.8770, OA=0.8751, Precision=0.8352, Recall=0.9231\n",
      "    Class 1: IoU=0.7055, F1=0.8273, OA=0.9964, Precision=0.8441, Recall=0.8112\n",
      "    Class 2: IoU=0.9106, F1=0.9532, OA=0.9811, Precision=0.9567, Recall=0.9498\n",
      "    Class 3: IoU=0.4879, F1=0.6558, OA=0.9639, Precision=0.7815, Recall=0.5649\n",
      "    Class 4: IoU=0.7541, F1=0.8598, OA=0.9812, Precision=0.8746, Recall=0.8455\n",
      "    Class 5: IoU=0.6597, F1=0.7949, OA=0.9963, Precision=0.7762, Recall=0.8146\n",
      "    Class 6: IoU=0.5750, F1=0.7302, OA=0.9575, Precision=0.8109, Recall=0.6640\n",
      "    Class 7: IoU=0.6444, F1=0.7838, OA=0.9685, Precision=0.8707, Recall=0.7127\n",
      "Epoch: 70, Batch: 1/842, Loss: 0.4667, Time: 1.49s\n",
      "Epoch: 70, Batch: 11/842, Loss: 0.4749, Time: 0.76s\n",
      "Epoch: 70, Batch: 21/842, Loss: 0.4745, Time: 0.78s\n",
      "Epoch: 70, Batch: 31/842, Loss: 0.4380, Time: 0.76s\n",
      "Epoch: 70, Batch: 41/842, Loss: 0.3954, Time: 0.73s\n",
      "Epoch: 70, Batch: 51/842, Loss: 0.4527, Time: 0.75s\n",
      "Epoch: 70, Batch: 61/842, Loss: 0.4547, Time: 0.79s\n",
      "Epoch: 70, Batch: 71/842, Loss: 0.3867, Time: 0.77s\n",
      "Epoch: 70, Batch: 81/842, Loss: 0.4199, Time: 0.77s\n",
      "Epoch: 70, Batch: 91/842, Loss: 0.4532, Time: 0.77s\n",
      "Epoch: 70, Batch: 101/842, Loss: 0.4265, Time: 0.77s\n",
      "Epoch: 70, Batch: 111/842, Loss: 0.4573, Time: 0.76s\n",
      "Epoch: 70, Batch: 121/842, Loss: 0.5038, Time: 0.78s\n",
      "Epoch: 70, Batch: 131/842, Loss: 0.4510, Time: 0.76s\n",
      "Epoch: 70, Batch: 141/842, Loss: 0.5396, Time: 0.77s\n",
      "Epoch: 70, Batch: 151/842, Loss: 0.4694, Time: 0.83s\n",
      "Epoch: 70, Batch: 161/842, Loss: 0.4496, Time: 0.79s\n",
      "Epoch: 70, Batch: 171/842, Loss: 0.4667, Time: 0.78s\n",
      "Epoch: 70, Batch: 181/842, Loss: 0.3891, Time: 0.76s\n",
      "Epoch: 70, Batch: 191/842, Loss: 0.5218, Time: 0.75s\n",
      "Epoch: 70, Batch: 201/842, Loss: 0.6213, Time: 0.80s\n",
      "Epoch: 70, Batch: 211/842, Loss: 0.5076, Time: 0.77s\n",
      "Epoch: 70, Batch: 221/842, Loss: 0.3986, Time: 0.80s\n",
      "Epoch: 70, Batch: 231/842, Loss: 0.4200, Time: 0.74s\n",
      "Epoch: 70, Batch: 241/842, Loss: 0.5554, Time: 0.77s\n",
      "Epoch: 70, Batch: 251/842, Loss: 0.3802, Time: 0.80s\n",
      "Epoch: 70, Batch: 261/842, Loss: 0.5383, Time: 0.79s\n",
      "Epoch: 70, Batch: 271/842, Loss: 0.5306, Time: 0.73s\n",
      "Epoch: 70, Batch: 281/842, Loss: 0.4116, Time: 0.74s\n",
      "Epoch: 70, Batch: 291/842, Loss: 0.6186, Time: 0.77s\n",
      "Epoch: 70, Batch: 301/842, Loss: 0.5519, Time: 0.74s\n",
      "Epoch: 70, Batch: 311/842, Loss: 0.4696, Time: 0.76s\n",
      "Epoch: 70, Batch: 321/842, Loss: 0.3679, Time: 0.75s\n",
      "Epoch: 70, Batch: 331/842, Loss: 0.4618, Time: 0.75s\n",
      "Epoch: 70, Batch: 341/842, Loss: 0.6056, Time: 0.74s\n",
      "Epoch: 70, Batch: 351/842, Loss: 0.4414, Time: 0.76s\n",
      "Epoch: 70, Batch: 361/842, Loss: 0.5113, Time: 0.75s\n",
      "Epoch: 70, Batch: 371/842, Loss: 0.4731, Time: 0.74s\n",
      "Epoch: 70, Batch: 381/842, Loss: 0.4563, Time: 0.77s\n",
      "Epoch: 70, Batch: 391/842, Loss: 0.4514, Time: 0.75s\n",
      "Epoch: 70, Batch: 401/842, Loss: 0.4504, Time: 0.84s\n",
      "Epoch: 70, Batch: 411/842, Loss: 0.3624, Time: 0.80s\n",
      "Epoch: 70, Batch: 421/842, Loss: 0.4272, Time: 0.81s\n",
      "Epoch: 70, Batch: 431/842, Loss: 0.4539, Time: 0.83s\n",
      "Epoch: 70, Batch: 441/842, Loss: 0.4647, Time: 0.75s\n",
      "Epoch: 70, Batch: 451/842, Loss: 0.5310, Time: 0.74s\n",
      "Epoch: 70, Batch: 461/842, Loss: 0.5767, Time: 0.78s\n",
      "Epoch: 70, Batch: 471/842, Loss: 0.4869, Time: 0.82s\n",
      "Epoch: 70, Batch: 481/842, Loss: 0.5493, Time: 0.87s\n",
      "Epoch: 70, Batch: 491/842, Loss: 0.5053, Time: 0.84s\n",
      "Epoch: 70, Batch: 501/842, Loss: 0.4930, Time: 0.83s\n",
      "Epoch: 70, Batch: 511/842, Loss: 0.4324, Time: 0.84s\n",
      "Epoch: 70, Batch: 521/842, Loss: 0.4253, Time: 0.79s\n",
      "Epoch: 70, Batch: 531/842, Loss: 0.3664, Time: 0.86s\n",
      "Epoch: 70, Batch: 541/842, Loss: 0.4478, Time: 0.84s\n",
      "Epoch: 70, Batch: 551/842, Loss: 0.4643, Time: 0.84s\n",
      "Epoch: 70, Batch: 561/842, Loss: 0.4818, Time: 0.87s\n",
      "Epoch: 70, Batch: 571/842, Loss: 0.6236, Time: 0.87s\n",
      "Epoch: 70, Batch: 581/842, Loss: 0.5352, Time: 0.88s\n",
      "Epoch: 70, Batch: 591/842, Loss: 0.4164, Time: 0.83s\n",
      "Epoch: 70, Batch: 601/842, Loss: 0.4401, Time: 0.86s\n",
      "Epoch: 70, Batch: 611/842, Loss: 0.5538, Time: 0.75s\n",
      "Epoch: 70, Batch: 621/842, Loss: 0.4785, Time: 0.83s\n",
      "Epoch: 70, Batch: 631/842, Loss: 0.3934, Time: 0.84s\n",
      "Epoch: 70, Batch: 641/842, Loss: 0.5103, Time: 0.85s\n",
      "Epoch: 70, Batch: 651/842, Loss: 0.2825, Time: 0.80s\n",
      "Epoch: 70, Batch: 661/842, Loss: 0.4911, Time: 0.76s\n",
      "Epoch: 70, Batch: 671/842, Loss: 0.3689, Time: 0.73s\n",
      "Epoch: 70, Batch: 681/842, Loss: 0.4196, Time: 0.78s\n",
      "Epoch: 70, Batch: 691/842, Loss: 0.4553, Time: 0.78s\n",
      "Epoch: 70, Batch: 701/842, Loss: 0.5733, Time: 0.80s\n",
      "Epoch: 70, Batch: 711/842, Loss: 0.4100, Time: 0.73s\n",
      "Epoch: 70, Batch: 721/842, Loss: 0.4371, Time: 0.75s\n",
      "Epoch: 70, Batch: 731/842, Loss: 0.4852, Time: 0.78s\n",
      "Epoch: 70, Batch: 741/842, Loss: 0.4852, Time: 0.79s\n",
      "Epoch: 70, Batch: 751/842, Loss: 0.4388, Time: 0.75s\n",
      "Epoch: 70, Batch: 761/842, Loss: 0.4020, Time: 0.74s\n",
      "Epoch: 70, Batch: 771/842, Loss: 0.2869, Time: 0.79s\n",
      "Epoch: 70, Batch: 781/842, Loss: 0.4413, Time: 0.83s\n",
      "Epoch: 70, Batch: 791/842, Loss: 0.5199, Time: 0.79s\n",
      "Epoch: 70, Batch: 801/842, Loss: 0.3917, Time: 0.80s\n",
      "Epoch: 70, Batch: 811/842, Loss: 0.3177, Time: 0.75s\n",
      "Epoch: 70, Batch: 821/842, Loss: 0.4624, Time: 0.71s\n",
      "Epoch: 70, Batch: 831/842, Loss: 0.5204, Time: 0.74s\n",
      "Epoch: 70, Batch: 841/842, Loss: 0.5616, Time: 0.73s\n",
      "Epoch 70/100: Train Loss: 0.4563, Val Loss: 0.4247, mIoU: 0.6963, F1: 0.8148, OA: 0.9654\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7822, F1=0.8778, OA=0.8760, Precision=0.8363, Recall=0.9236\n",
      "    Class 1: IoU=0.7116, F1=0.8315, OA=0.9965, Precision=0.8569, Recall=0.8076\n",
      "    Class 2: IoU=0.9117, F1=0.9538, OA=0.9813, Precision=0.9536, Recall=0.9540\n",
      "    Class 3: IoU=0.4857, F1=0.6539, OA=0.9637, Precision=0.7785, Recall=0.5636\n",
      "    Class 4: IoU=0.7569, F1=0.8616, OA=0.9816, Precision=0.8821, Recall=0.8421\n",
      "    Class 5: IoU=0.6918, F1=0.8178, OA=0.9968, Precision=0.8267, Recall=0.8091\n",
      "    Class 6: IoU=0.5791, F1=0.7334, OA=0.9581, Precision=0.8174, Recall=0.6651\n",
      "    Class 7: IoU=0.6513, F1=0.7888, OA=0.9691, Precision=0.8696, Recall=0.7218\n",
      "Epoch: 71, Batch: 1/842, Loss: 0.4004, Time: 1.31s\n",
      "Epoch: 71, Batch: 11/842, Loss: 0.5282, Time: 0.75s\n",
      "Epoch: 71, Batch: 21/842, Loss: 0.4621, Time: 0.69s\n",
      "Epoch: 71, Batch: 31/842, Loss: 0.5066, Time: 0.71s\n",
      "Epoch: 71, Batch: 41/842, Loss: 0.3857, Time: 0.74s\n",
      "Epoch: 71, Batch: 51/842, Loss: 0.3877, Time: 0.73s\n",
      "Epoch: 71, Batch: 61/842, Loss: 0.3651, Time: 0.75s\n",
      "Epoch: 71, Batch: 71/842, Loss: 0.5454, Time: 0.76s\n",
      "Epoch: 71, Batch: 81/842, Loss: 0.3355, Time: 0.76s\n",
      "Epoch: 71, Batch: 91/842, Loss: 0.5319, Time: 0.77s\n",
      "Epoch: 71, Batch: 101/842, Loss: 0.3795, Time: 0.78s\n",
      "Epoch: 71, Batch: 111/842, Loss: 0.4304, Time: 0.83s\n",
      "Epoch: 71, Batch: 121/842, Loss: 0.3110, Time: 0.77s\n",
      "Epoch: 71, Batch: 131/842, Loss: 0.4274, Time: 0.71s\n",
      "Epoch: 71, Batch: 141/842, Loss: 0.4793, Time: 0.74s\n",
      "Epoch: 71, Batch: 151/842, Loss: 0.6212, Time: 0.77s\n",
      "Epoch: 71, Batch: 161/842, Loss: 0.4650, Time: 0.77s\n",
      "Epoch: 71, Batch: 171/842, Loss: 0.4462, Time: 0.74s\n",
      "Epoch: 71, Batch: 181/842, Loss: 0.5483, Time: 0.75s\n",
      "Epoch: 71, Batch: 191/842, Loss: 0.4451, Time: 0.77s\n",
      "Epoch: 71, Batch: 201/842, Loss: 0.4060, Time: 0.76s\n",
      "Epoch: 71, Batch: 211/842, Loss: 0.4557, Time: 0.81s\n",
      "Epoch: 71, Batch: 221/842, Loss: 0.3478, Time: 0.80s\n",
      "Epoch: 71, Batch: 231/842, Loss: 0.3170, Time: 0.79s\n",
      "Epoch: 71, Batch: 241/842, Loss: 0.2953, Time: 0.73s\n",
      "Epoch: 71, Batch: 251/842, Loss: 0.5067, Time: 0.75s\n",
      "Epoch: 71, Batch: 261/842, Loss: 0.6004, Time: 0.78s\n",
      "Epoch: 71, Batch: 271/842, Loss: 0.2782, Time: 0.82s\n",
      "Epoch: 71, Batch: 281/842, Loss: 0.3599, Time: 0.77s\n",
      "Epoch: 71, Batch: 291/842, Loss: 0.3965, Time: 0.79s\n",
      "Epoch: 71, Batch: 301/842, Loss: 0.3821, Time: 0.81s\n",
      "Epoch: 71, Batch: 311/842, Loss: 0.5247, Time: 0.77s\n",
      "Epoch: 71, Batch: 321/842, Loss: 0.4669, Time: 0.77s\n",
      "Epoch: 71, Batch: 331/842, Loss: 0.3981, Time: 0.74s\n",
      "Epoch: 71, Batch: 341/842, Loss: 0.5759, Time: 0.76s\n",
      "Epoch: 71, Batch: 351/842, Loss: 0.4260, Time: 0.76s\n",
      "Epoch: 71, Batch: 361/842, Loss: 0.3214, Time: 0.76s\n",
      "Epoch: 71, Batch: 371/842, Loss: 0.5354, Time: 0.80s\n",
      "Epoch: 71, Batch: 381/842, Loss: 0.3692, Time: 0.74s\n",
      "Epoch: 71, Batch: 391/842, Loss: 0.3999, Time: 0.73s\n",
      "Epoch: 71, Batch: 401/842, Loss: 0.5286, Time: 0.73s\n",
      "Epoch: 71, Batch: 411/842, Loss: 0.6364, Time: 0.75s\n",
      "Epoch: 71, Batch: 421/842, Loss: 0.4056, Time: 0.76s\n",
      "Epoch: 71, Batch: 431/842, Loss: 0.5371, Time: 0.77s\n",
      "Epoch: 71, Batch: 441/842, Loss: 0.5153, Time: 0.77s\n",
      "Epoch: 71, Batch: 451/842, Loss: 0.5480, Time: 0.84s\n",
      "Epoch: 71, Batch: 461/842, Loss: 0.3688, Time: 0.85s\n",
      "Epoch: 71, Batch: 471/842, Loss: 0.4826, Time: 0.80s\n",
      "Epoch: 71, Batch: 481/842, Loss: 0.4078, Time: 0.71s\n",
      "Epoch: 71, Batch: 491/842, Loss: 0.5243, Time: 0.85s\n",
      "Epoch: 71, Batch: 501/842, Loss: 0.5247, Time: 0.80s\n",
      "Epoch: 71, Batch: 511/842, Loss: 0.4119, Time: 0.78s\n",
      "Epoch: 71, Batch: 521/842, Loss: 0.4843, Time: 0.77s\n",
      "Epoch: 71, Batch: 531/842, Loss: 0.5548, Time: 0.77s\n",
      "Epoch: 71, Batch: 541/842, Loss: 0.4445, Time: 0.76s\n",
      "Epoch: 71, Batch: 551/842, Loss: 0.3517, Time: 0.77s\n",
      "Epoch: 71, Batch: 561/842, Loss: 0.3992, Time: 0.75s\n",
      "Epoch: 71, Batch: 571/842, Loss: 0.2926, Time: 0.74s\n",
      "Epoch: 71, Batch: 581/842, Loss: 0.4709, Time: 0.77s\n",
      "Epoch: 71, Batch: 591/842, Loss: 0.4346, Time: 0.87s\n",
      "Epoch: 71, Batch: 601/842, Loss: 0.5105, Time: 0.81s\n",
      "Epoch: 71, Batch: 611/842, Loss: 0.4274, Time: 0.80s\n",
      "Epoch: 71, Batch: 621/842, Loss: 0.3579, Time: 0.75s\n",
      "Epoch: 71, Batch: 631/842, Loss: 0.4847, Time: 0.77s\n",
      "Epoch: 71, Batch: 641/842, Loss: 0.4461, Time: 0.82s\n",
      "Epoch: 71, Batch: 651/842, Loss: 0.5455, Time: 0.85s\n",
      "Epoch: 71, Batch: 661/842, Loss: 0.3592, Time: 0.80s\n",
      "Epoch: 71, Batch: 671/842, Loss: 0.4891, Time: 0.76s\n",
      "Epoch: 71, Batch: 681/842, Loss: 0.3178, Time: 0.78s\n",
      "Epoch: 71, Batch: 691/842, Loss: 0.4751, Time: 0.74s\n",
      "Epoch: 71, Batch: 701/842, Loss: 0.5199, Time: 0.80s\n",
      "Epoch: 71, Batch: 711/842, Loss: 0.4771, Time: 0.80s\n",
      "Epoch: 71, Batch: 721/842, Loss: 0.4977, Time: 0.81s\n",
      "Epoch: 71, Batch: 731/842, Loss: 0.4907, Time: 0.82s\n",
      "Epoch: 71, Batch: 741/842, Loss: 0.5871, Time: 0.82s\n",
      "Epoch: 71, Batch: 751/842, Loss: 0.5856, Time: 0.81s\n",
      "Epoch: 71, Batch: 761/842, Loss: 0.4544, Time: 0.89s\n",
      "Epoch: 71, Batch: 771/842, Loss: 0.4640, Time: 0.77s\n",
      "Epoch: 71, Batch: 781/842, Loss: 0.3551, Time: 0.83s\n",
      "Epoch: 71, Batch: 791/842, Loss: 0.4662, Time: 0.81s\n",
      "Epoch: 71, Batch: 801/842, Loss: 0.5011, Time: 0.82s\n",
      "Epoch: 71, Batch: 811/842, Loss: 0.5257, Time: 0.83s\n",
      "Epoch: 71, Batch: 821/842, Loss: 0.3910, Time: 0.81s\n",
      "Epoch: 71, Batch: 831/842, Loss: 0.3927, Time: 0.80s\n",
      "Epoch: 71, Batch: 841/842, Loss: 0.5079, Time: 0.80s\n",
      "Epoch 71/100: Train Loss: 0.4559, Val Loss: 0.4250, mIoU: 0.6956, F1: 0.8144, OA: 0.9654\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7821, F1=0.8777, OA=0.8744, Precision=0.8268, Recall=0.9354\n",
      "    Class 1: IoU=0.7082, F1=0.8292, OA=0.9965, Precision=0.8583, Recall=0.8020\n",
      "    Class 2: IoU=0.9110, F1=0.9534, OA=0.9813, Precision=0.9612, Recall=0.9458\n",
      "    Class 3: IoU=0.4896, F1=0.6574, OA=0.9644, Precision=0.7915, Recall=0.5621\n",
      "    Class 4: IoU=0.7573, F1=0.8619, OA=0.9820, Precision=0.9032, Recall=0.8243\n",
      "    Class 5: IoU=0.6948, F1=0.8199, OA=0.9968, Precision=0.8329, Recall=0.8074\n",
      "    Class 6: IoU=0.5766, F1=0.7314, OA=0.9588, Precision=0.8400, Recall=0.6477\n",
      "    Class 7: IoU=0.6454, F1=0.7845, OA=0.9689, Precision=0.8806, Recall=0.7073\n",
      "Epoch: 72, Batch: 1/842, Loss: 0.4581, Time: 1.46s\n",
      "Epoch: 72, Batch: 11/842, Loss: 0.3895, Time: 0.75s\n",
      "Epoch: 72, Batch: 21/842, Loss: 0.4460, Time: 0.74s\n",
      "Epoch: 72, Batch: 31/842, Loss: 0.3702, Time: 0.72s\n",
      "Epoch: 72, Batch: 41/842, Loss: 0.4360, Time: 0.76s\n",
      "Epoch: 72, Batch: 51/842, Loss: 0.4035, Time: 0.73s\n",
      "Epoch: 72, Batch: 61/842, Loss: 0.4255, Time: 0.74s\n",
      "Epoch: 72, Batch: 71/842, Loss: 0.4563, Time: 0.86s\n",
      "Epoch: 72, Batch: 81/842, Loss: 0.3777, Time: 0.80s\n",
      "Epoch: 72, Batch: 91/842, Loss: 0.4665, Time: 0.74s\n",
      "Epoch: 72, Batch: 101/842, Loss: 0.3932, Time: 0.74s\n",
      "Epoch: 72, Batch: 111/842, Loss: 0.4043, Time: 0.75s\n",
      "Epoch: 72, Batch: 121/842, Loss: 0.6060, Time: 0.79s\n",
      "Epoch: 72, Batch: 131/842, Loss: 0.3343, Time: 0.76s\n",
      "Epoch: 72, Batch: 141/842, Loss: 0.4606, Time: 0.76s\n",
      "Epoch: 72, Batch: 151/842, Loss: 0.5342, Time: 0.76s\n",
      "Epoch: 72, Batch: 161/842, Loss: 0.4072, Time: 0.75s\n",
      "Epoch: 72, Batch: 171/842, Loss: 0.3967, Time: 0.76s\n",
      "Epoch: 72, Batch: 181/842, Loss: 0.4697, Time: 0.76s\n",
      "Epoch: 72, Batch: 191/842, Loss: 0.4262, Time: 0.77s\n",
      "Epoch: 72, Batch: 201/842, Loss: 0.4974, Time: 0.76s\n",
      "Epoch: 72, Batch: 211/842, Loss: 0.4531, Time: 0.76s\n",
      "Epoch: 72, Batch: 221/842, Loss: 0.4377, Time: 0.75s\n",
      "Epoch: 72, Batch: 231/842, Loss: 0.3800, Time: 0.75s\n",
      "Epoch: 72, Batch: 241/842, Loss: 0.3949, Time: 0.74s\n",
      "Epoch: 72, Batch: 251/842, Loss: 0.3690, Time: 0.72s\n",
      "Epoch: 72, Batch: 261/842, Loss: 0.4413, Time: 0.70s\n",
      "Epoch: 72, Batch: 271/842, Loss: 0.4807, Time: 0.76s\n",
      "Epoch: 72, Batch: 281/842, Loss: 0.4447, Time: 0.74s\n",
      "Epoch: 72, Batch: 291/842, Loss: 0.4778, Time: 0.75s\n",
      "Epoch: 72, Batch: 301/842, Loss: 0.4382, Time: 0.74s\n",
      "Epoch: 72, Batch: 311/842, Loss: 0.4688, Time: 0.76s\n",
      "Epoch: 72, Batch: 321/842, Loss: 0.4786, Time: 0.75s\n",
      "Epoch: 72, Batch: 331/842, Loss: 0.5534, Time: 0.74s\n",
      "Epoch: 72, Batch: 341/842, Loss: 0.3757, Time: 0.77s\n",
      "Epoch: 72, Batch: 351/842, Loss: 0.4906, Time: 0.75s\n",
      "Epoch: 72, Batch: 361/842, Loss: 0.4339, Time: 0.78s\n",
      "Epoch: 72, Batch: 371/842, Loss: 0.5534, Time: 0.76s\n",
      "Epoch: 72, Batch: 381/842, Loss: 0.4331, Time: 0.73s\n",
      "Epoch: 72, Batch: 391/842, Loss: 0.4251, Time: 0.73s\n",
      "Epoch: 72, Batch: 401/842, Loss: 0.4313, Time: 0.79s\n",
      "Epoch: 72, Batch: 411/842, Loss: 0.3501, Time: 0.80s\n",
      "Epoch: 72, Batch: 421/842, Loss: 0.2498, Time: 0.73s\n",
      "Epoch: 72, Batch: 431/842, Loss: 0.4888, Time: 0.76s\n",
      "Epoch: 72, Batch: 441/842, Loss: 0.3871, Time: 0.78s\n",
      "Epoch: 72, Batch: 451/842, Loss: 0.5255, Time: 0.77s\n",
      "Epoch: 72, Batch: 461/842, Loss: 0.4122, Time: 0.75s\n",
      "Epoch: 72, Batch: 471/842, Loss: 0.4084, Time: 0.76s\n",
      "Epoch: 72, Batch: 481/842, Loss: 0.4039, Time: 0.74s\n",
      "Epoch: 72, Batch: 491/842, Loss: 0.3694, Time: 0.72s\n",
      "Epoch: 72, Batch: 501/842, Loss: 0.5619, Time: 0.76s\n",
      "Epoch: 72, Batch: 511/842, Loss: 0.4469, Time: 0.79s\n",
      "Epoch: 72, Batch: 521/842, Loss: 0.4542, Time: 0.79s\n",
      "Epoch: 72, Batch: 531/842, Loss: 0.3875, Time: 0.75s\n",
      "Epoch: 72, Batch: 541/842, Loss: 0.4871, Time: 0.74s\n",
      "Epoch: 72, Batch: 551/842, Loss: 0.3417, Time: 0.78s\n",
      "Epoch: 72, Batch: 561/842, Loss: 0.5653, Time: 0.82s\n",
      "Epoch: 72, Batch: 571/842, Loss: 0.4115, Time: 0.78s\n",
      "Epoch: 72, Batch: 581/842, Loss: 0.3691, Time: 0.82s\n",
      "Epoch: 72, Batch: 591/842, Loss: 0.4716, Time: 0.82s\n",
      "Epoch: 72, Batch: 601/842, Loss: 0.3972, Time: 0.81s\n",
      "Epoch: 72, Batch: 611/842, Loss: 0.5269, Time: 0.78s\n",
      "Epoch: 72, Batch: 621/842, Loss: 0.2554, Time: 0.77s\n",
      "Epoch: 72, Batch: 631/842, Loss: 0.4657, Time: 0.79s\n",
      "Epoch: 72, Batch: 641/842, Loss: 0.4250, Time: 0.78s\n",
      "Epoch: 72, Batch: 651/842, Loss: 0.4237, Time: 0.79s\n",
      "Epoch: 72, Batch: 661/842, Loss: 0.4500, Time: 0.80s\n",
      "Epoch: 72, Batch: 671/842, Loss: 0.5715, Time: 0.80s\n",
      "Epoch: 72, Batch: 681/842, Loss: 0.5532, Time: 0.78s\n",
      "Epoch: 72, Batch: 691/842, Loss: 0.6108, Time: 0.78s\n",
      "Epoch: 72, Batch: 701/842, Loss: 0.4806, Time: 0.81s\n",
      "Epoch: 72, Batch: 711/842, Loss: 0.4691, Time: 0.77s\n",
      "Epoch: 72, Batch: 721/842, Loss: 0.6241, Time: 0.77s\n",
      "Epoch: 72, Batch: 731/842, Loss: 0.4040, Time: 0.75s\n",
      "Epoch: 72, Batch: 741/842, Loss: 0.5457, Time: 0.76s\n",
      "Epoch: 72, Batch: 751/842, Loss: 0.3766, Time: 0.71s\n",
      "Epoch: 72, Batch: 761/842, Loss: 0.3906, Time: 0.77s\n",
      "Epoch: 72, Batch: 771/842, Loss: 0.4900, Time: 0.79s\n",
      "Epoch: 72, Batch: 781/842, Loss: 0.4820, Time: 0.75s\n",
      "Epoch: 72, Batch: 791/842, Loss: 0.4297, Time: 0.77s\n",
      "Epoch: 72, Batch: 801/842, Loss: 0.4866, Time: 0.75s\n",
      "Epoch: 72, Batch: 811/842, Loss: 0.5551, Time: 0.78s\n",
      "Epoch: 72, Batch: 821/842, Loss: 0.5087, Time: 0.80s\n",
      "Epoch: 72, Batch: 831/842, Loss: 0.4035, Time: 0.76s\n",
      "Epoch: 72, Batch: 841/842, Loss: 0.4558, Time: 0.76s\n",
      "Epoch 72/100: Train Loss: 0.4493, Val Loss: 0.4265, mIoU: 0.6935, F1: 0.8129, OA: 0.9653\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7816, F1=0.8774, OA=0.8741, Precision=0.8265, Recall=0.9351\n",
      "    Class 1: IoU=0.7099, F1=0.8303, OA=0.9965, Precision=0.8648, Recall=0.7984\n",
      "    Class 2: IoU=0.9105, F1=0.9531, OA=0.9812, Precision=0.9618, Recall=0.9446\n",
      "    Class 3: IoU=0.4902, F1=0.6579, OA=0.9642, Precision=0.7842, Recall=0.5667\n",
      "    Class 4: IoU=0.7579, F1=0.8623, OA=0.9819, Precision=0.8961, Recall=0.8309\n",
      "    Class 5: IoU=0.6800, F1=0.8095, OA=0.9966, Precision=0.8010, Recall=0.8182\n",
      "    Class 6: IoU=0.5749, F1=0.7301, OA=0.9588, Precision=0.8453, Recall=0.6426\n",
      "    Class 7: IoU=0.6426, F1=0.7824, OA=0.9688, Precision=0.8842, Recall=0.7017\n",
      "Epoch: 73, Batch: 1/842, Loss: 0.4636, Time: 1.49s\n",
      "Epoch: 73, Batch: 11/842, Loss: 0.3535, Time: 0.78s\n",
      "Epoch: 73, Batch: 21/842, Loss: 0.3542, Time: 0.75s\n",
      "Epoch: 73, Batch: 31/842, Loss: 0.4086, Time: 0.81s\n",
      "Epoch: 73, Batch: 41/842, Loss: 0.3907, Time: 0.80s\n",
      "Epoch: 73, Batch: 51/842, Loss: 0.4520, Time: 0.77s\n",
      "Epoch: 73, Batch: 61/842, Loss: 0.5626, Time: 0.74s\n",
      "Epoch: 73, Batch: 71/842, Loss: 0.3483, Time: 0.77s\n",
      "Epoch: 73, Batch: 81/842, Loss: 0.3610, Time: 1.36s\n",
      "Epoch: 73, Batch: 91/842, Loss: 0.4423, Time: 0.78s\n",
      "Epoch: 73, Batch: 101/842, Loss: 0.4184, Time: 0.83s\n",
      "Epoch: 73, Batch: 111/842, Loss: 0.5709, Time: 0.83s\n",
      "Epoch: 73, Batch: 121/842, Loss: 0.4958, Time: 0.76s\n",
      "Epoch: 73, Batch: 131/842, Loss: 0.4389, Time: 0.76s\n",
      "Epoch: 73, Batch: 141/842, Loss: 0.5581, Time: 0.78s\n",
      "Epoch: 73, Batch: 151/842, Loss: 0.5233, Time: 0.80s\n",
      "Epoch: 73, Batch: 161/842, Loss: 0.4690, Time: 0.85s\n",
      "Epoch: 73, Batch: 171/842, Loss: 0.4340, Time: 0.80s\n",
      "Epoch: 73, Batch: 181/842, Loss: 0.3984, Time: 0.79s\n",
      "Epoch: 73, Batch: 191/842, Loss: 0.4355, Time: 0.83s\n",
      "Epoch: 73, Batch: 201/842, Loss: 0.4082, Time: 0.81s\n",
      "Epoch: 73, Batch: 211/842, Loss: 0.5271, Time: 0.79s\n",
      "Epoch: 73, Batch: 221/842, Loss: 0.4591, Time: 0.78s\n",
      "Epoch: 73, Batch: 231/842, Loss: 0.4163, Time: 0.78s\n",
      "Epoch: 73, Batch: 241/842, Loss: 0.3818, Time: 0.78s\n",
      "Epoch: 73, Batch: 251/842, Loss: 0.4529, Time: 0.78s\n",
      "Epoch: 73, Batch: 261/842, Loss: 0.4519, Time: 0.75s\n",
      "Epoch: 73, Batch: 271/842, Loss: 0.4635, Time: 0.75s\n",
      "Epoch: 73, Batch: 281/842, Loss: 0.5156, Time: 0.76s\n",
      "Epoch: 73, Batch: 291/842, Loss: 0.5093, Time: 0.81s\n",
      "Epoch: 73, Batch: 301/842, Loss: 0.4453, Time: 0.78s\n",
      "Epoch: 73, Batch: 311/842, Loss: 0.5203, Time: 0.76s\n",
      "Epoch: 73, Batch: 321/842, Loss: 0.4029, Time: 0.73s\n",
      "Epoch: 73, Batch: 331/842, Loss: 0.5576, Time: 0.75s\n",
      "Epoch: 73, Batch: 341/842, Loss: 0.4086, Time: 0.76s\n",
      "Epoch: 73, Batch: 351/842, Loss: 0.4500, Time: 0.78s\n",
      "Epoch: 73, Batch: 361/842, Loss: 0.4615, Time: 0.76s\n",
      "Epoch: 73, Batch: 371/842, Loss: 0.3993, Time: 0.78s\n",
      "Epoch: 73, Batch: 381/842, Loss: 0.4160, Time: 0.77s\n",
      "Epoch: 73, Batch: 391/842, Loss: 0.5785, Time: 0.77s\n",
      "Epoch: 73, Batch: 401/842, Loss: 0.4333, Time: 0.77s\n",
      "Epoch: 73, Batch: 411/842, Loss: 0.4775, Time: 0.78s\n",
      "Epoch: 73, Batch: 421/842, Loss: 0.5717, Time: 0.73s\n",
      "Epoch: 73, Batch: 431/842, Loss: 0.5366, Time: 0.74s\n",
      "Epoch: 73, Batch: 441/842, Loss: 0.4801, Time: 0.74s\n",
      "Epoch: 73, Batch: 451/842, Loss: 0.4773, Time: 0.76s\n",
      "Epoch: 73, Batch: 461/842, Loss: 0.4095, Time: 0.79s\n",
      "Epoch: 73, Batch: 471/842, Loss: 0.5557, Time: 0.80s\n",
      "Epoch: 73, Batch: 481/842, Loss: 0.3045, Time: 0.80s\n",
      "Epoch: 73, Batch: 491/842, Loss: 0.4213, Time: 0.79s\n",
      "Epoch: 73, Batch: 501/842, Loss: 0.4624, Time: 0.79s\n",
      "Epoch: 73, Batch: 511/842, Loss: 0.4666, Time: 0.79s\n",
      "Epoch: 73, Batch: 521/842, Loss: 0.5444, Time: 0.81s\n",
      "Epoch: 73, Batch: 531/842, Loss: 0.3410, Time: 0.78s\n",
      "Epoch: 73, Batch: 541/842, Loss: 0.4054, Time: 0.74s\n",
      "Epoch: 73, Batch: 551/842, Loss: 0.5891, Time: 0.76s\n",
      "Epoch: 73, Batch: 561/842, Loss: 0.5769, Time: 0.74s\n",
      "Epoch: 73, Batch: 571/842, Loss: 0.4888, Time: 0.76s\n",
      "Epoch: 73, Batch: 581/842, Loss: 0.5118, Time: 0.75s\n",
      "Epoch: 73, Batch: 591/842, Loss: 0.3353, Time: 0.76s\n",
      "Epoch: 73, Batch: 601/842, Loss: 0.5319, Time: 0.76s\n",
      "Epoch: 73, Batch: 611/842, Loss: 0.5239, Time: 0.78s\n",
      "Epoch: 73, Batch: 621/842, Loss: 0.4142, Time: 0.76s\n",
      "Epoch: 73, Batch: 631/842, Loss: 0.3292, Time: 0.75s\n",
      "Epoch: 73, Batch: 641/842, Loss: 0.5105, Time: 0.75s\n",
      "Epoch: 73, Batch: 651/842, Loss: 0.4702, Time: 0.78s\n",
      "Epoch: 73, Batch: 661/842, Loss: 0.4320, Time: 0.78s\n",
      "Epoch: 73, Batch: 671/842, Loss: 0.5792, Time: 0.75s\n",
      "Epoch: 73, Batch: 681/842, Loss: 0.4066, Time: 0.76s\n",
      "Epoch: 73, Batch: 691/842, Loss: 0.6346, Time: 0.76s\n",
      "Epoch: 73, Batch: 701/842, Loss: 0.6124, Time: 0.76s\n",
      "Epoch: 73, Batch: 711/842, Loss: 0.3762, Time: 0.76s\n",
      "Epoch: 73, Batch: 721/842, Loss: 0.4549, Time: 0.79s\n",
      "Epoch: 73, Batch: 731/842, Loss: 0.4551, Time: 0.77s\n",
      "Epoch: 73, Batch: 741/842, Loss: 0.4185, Time: 0.81s\n",
      "Epoch: 73, Batch: 751/842, Loss: 0.5340, Time: 0.78s\n",
      "Epoch: 73, Batch: 761/842, Loss: 0.5285, Time: 0.77s\n",
      "Epoch: 73, Batch: 771/842, Loss: 0.4432, Time: 0.79s\n",
      "Epoch: 73, Batch: 781/842, Loss: 0.3827, Time: 0.83s\n",
      "Epoch: 73, Batch: 791/842, Loss: 0.3908, Time: 0.82s\n",
      "Epoch: 73, Batch: 801/842, Loss: 0.5475, Time: 0.79s\n",
      "Epoch: 73, Batch: 811/842, Loss: 0.3698, Time: 0.76s\n",
      "Epoch: 73, Batch: 821/842, Loss: 0.3593, Time: 0.81s\n",
      "Epoch: 73, Batch: 831/842, Loss: 0.5738, Time: 0.86s\n",
      "Epoch: 73, Batch: 841/842, Loss: 0.5443, Time: 0.80s\n",
      "Epoch 73/100: Train Loss: 0.4515, Val Loss: 0.4227, mIoU: 0.6968, F1: 0.8151, OA: 0.9656\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7829, F1=0.8782, OA=0.8749, Precision=0.8276, Recall=0.9355\n",
      "    Class 1: IoU=0.7162, F1=0.8347, OA=0.9966, Precision=0.8655, Recall=0.8059\n",
      "    Class 2: IoU=0.9127, F1=0.9544, OA=0.9816, Precision=0.9597, Recall=0.9491\n",
      "    Class 3: IoU=0.4856, F1=0.6537, OA=0.9648, Precision=0.8125, Recall=0.5469\n",
      "    Class 4: IoU=0.7561, F1=0.8611, OA=0.9818, Precision=0.8995, Recall=0.8259\n",
      "    Class 5: IoU=0.6919, F1=0.8179, OA=0.9968, Precision=0.8276, Recall=0.8085\n",
      "    Class 6: IoU=0.5780, F1=0.7326, OA=0.9591, Precision=0.8443, Recall=0.6470\n",
      "    Class 7: IoU=0.6506, F1=0.7883, OA=0.9691, Precision=0.8718, Recall=0.7195\n",
      "Epoch: 74, Batch: 1/842, Loss: 0.5442, Time: 1.52s\n",
      "Epoch: 74, Batch: 11/842, Loss: 0.2647, Time: 0.80s\n",
      "Epoch: 74, Batch: 21/842, Loss: 0.4329, Time: 0.78s\n",
      "Epoch: 74, Batch: 31/842, Loss: 0.4483, Time: 0.77s\n",
      "Epoch: 74, Batch: 41/842, Loss: 0.4352, Time: 0.81s\n",
      "Epoch: 74, Batch: 51/842, Loss: 0.4501, Time: 0.84s\n",
      "Epoch: 74, Batch: 61/842, Loss: 0.4317, Time: 0.84s\n",
      "Epoch: 74, Batch: 71/842, Loss: 0.2547, Time: 0.84s\n",
      "Epoch: 74, Batch: 81/842, Loss: 0.3930, Time: 0.80s\n",
      "Epoch: 74, Batch: 91/842, Loss: 0.3364, Time: 0.77s\n",
      "Epoch: 74, Batch: 101/842, Loss: 0.5190, Time: 0.80s\n",
      "Epoch: 74, Batch: 111/842, Loss: 0.4170, Time: 0.77s\n",
      "Epoch: 74, Batch: 121/842, Loss: 0.5350, Time: 0.80s\n",
      "Epoch: 74, Batch: 131/842, Loss: 0.3525, Time: 0.76s\n",
      "Epoch: 74, Batch: 141/842, Loss: 0.6084, Time: 0.76s\n",
      "Epoch: 74, Batch: 151/842, Loss: 0.6093, Time: 0.74s\n",
      "Epoch: 74, Batch: 161/842, Loss: 0.3238, Time: 0.76s\n",
      "Epoch: 74, Batch: 171/842, Loss: 0.3689, Time: 0.73s\n",
      "Epoch: 74, Batch: 181/842, Loss: 0.5007, Time: 0.72s\n",
      "Epoch: 74, Batch: 191/842, Loss: 0.5313, Time: 0.79s\n",
      "Epoch: 74, Batch: 201/842, Loss: 0.4262, Time: 0.79s\n",
      "Epoch: 74, Batch: 211/842, Loss: 0.3355, Time: 0.76s\n",
      "Epoch: 74, Batch: 221/842, Loss: 0.3329, Time: 0.77s\n",
      "Epoch: 74, Batch: 231/842, Loss: 0.4431, Time: 0.76s\n",
      "Epoch: 74, Batch: 241/842, Loss: 0.4123, Time: 0.76s\n",
      "Epoch: 74, Batch: 251/842, Loss: 0.4776, Time: 0.76s\n",
      "Epoch: 74, Batch: 261/842, Loss: 0.4806, Time: 0.77s\n",
      "Epoch: 74, Batch: 271/842, Loss: 0.4317, Time: 0.83s\n",
      "Epoch: 74, Batch: 281/842, Loss: 0.5970, Time: 0.81s\n",
      "Epoch: 74, Batch: 291/842, Loss: 0.4404, Time: 0.82s\n",
      "Epoch: 74, Batch: 301/842, Loss: 0.5039, Time: 0.80s\n",
      "Epoch: 74, Batch: 311/842, Loss: 0.3375, Time: 0.82s\n",
      "Epoch: 74, Batch: 321/842, Loss: 0.4310, Time: 0.78s\n",
      "Epoch: 74, Batch: 331/842, Loss: 0.2696, Time: 0.82s\n",
      "Epoch: 74, Batch: 341/842, Loss: 0.5488, Time: 0.82s\n",
      "Epoch: 74, Batch: 351/842, Loss: 0.4244, Time: 0.83s\n",
      "Epoch: 74, Batch: 361/842, Loss: 0.4873, Time: 0.83s\n",
      "Epoch: 74, Batch: 371/842, Loss: 0.4208, Time: 0.77s\n",
      "Epoch: 74, Batch: 381/842, Loss: 0.4578, Time: 0.80s\n",
      "Epoch: 74, Batch: 391/842, Loss: 0.3549, Time: 0.81s\n",
      "Epoch: 74, Batch: 401/842, Loss: 0.6211, Time: 0.84s\n",
      "Epoch: 74, Batch: 411/842, Loss: 0.3189, Time: 0.76s\n",
      "Epoch: 74, Batch: 421/842, Loss: 0.5237, Time: 0.77s\n",
      "Epoch: 74, Batch: 431/842, Loss: 0.5847, Time: 0.79s\n",
      "Epoch: 74, Batch: 441/842, Loss: 0.5198, Time: 0.77s\n",
      "Epoch: 74, Batch: 451/842, Loss: 0.3226, Time: 0.80s\n",
      "Epoch: 74, Batch: 461/842, Loss: 0.3919, Time: 0.78s\n",
      "Epoch: 74, Batch: 471/842, Loss: 0.6934, Time: 0.75s\n",
      "Epoch: 74, Batch: 481/842, Loss: 0.4989, Time: 0.73s\n",
      "Epoch: 74, Batch: 491/842, Loss: 0.4616, Time: 0.77s\n",
      "Epoch: 74, Batch: 501/842, Loss: 0.5063, Time: 0.77s\n",
      "Epoch: 74, Batch: 511/842, Loss: 0.5216, Time: 0.75s\n",
      "Epoch: 74, Batch: 521/842, Loss: 0.3832, Time: 0.75s\n",
      "Epoch: 74, Batch: 531/842, Loss: 0.4983, Time: 0.74s\n",
      "Epoch: 74, Batch: 541/842, Loss: 0.4196, Time: 0.76s\n",
      "Epoch: 74, Batch: 551/842, Loss: 0.5324, Time: 0.81s\n",
      "Epoch: 74, Batch: 561/842, Loss: 0.4866, Time: 0.82s\n",
      "Epoch: 74, Batch: 571/842, Loss: 0.5807, Time: 0.83s\n",
      "Epoch: 74, Batch: 581/842, Loss: 0.6149, Time: 0.86s\n",
      "Epoch: 74, Batch: 591/842, Loss: 0.4188, Time: 0.80s\n",
      "Epoch: 74, Batch: 601/842, Loss: 0.3454, Time: 0.79s\n",
      "Epoch: 74, Batch: 611/842, Loss: 0.5519, Time: 0.77s\n",
      "Epoch: 74, Batch: 621/842, Loss: 0.5266, Time: 0.76s\n",
      "Epoch: 74, Batch: 631/842, Loss: 0.5459, Time: 0.76s\n",
      "Epoch: 74, Batch: 641/842, Loss: 0.5253, Time: 0.75s\n",
      "Epoch: 74, Batch: 651/842, Loss: 0.5528, Time: 0.75s\n",
      "Epoch: 74, Batch: 661/842, Loss: 0.4844, Time: 0.78s\n",
      "Epoch: 74, Batch: 671/842, Loss: 0.3724, Time: 0.76s\n",
      "Epoch: 74, Batch: 681/842, Loss: 0.3751, Time: 0.79s\n",
      "Epoch: 74, Batch: 691/842, Loss: 0.4820, Time: 0.75s\n",
      "Epoch: 74, Batch: 701/842, Loss: 0.5275, Time: 0.78s\n",
      "Epoch: 74, Batch: 711/842, Loss: 0.3992, Time: 0.76s\n",
      "Epoch: 74, Batch: 721/842, Loss: 0.5105, Time: 0.75s\n",
      "Epoch: 74, Batch: 731/842, Loss: 0.4418, Time: 0.74s\n",
      "Epoch: 74, Batch: 741/842, Loss: 0.4901, Time: 0.78s\n",
      "Epoch: 74, Batch: 751/842, Loss: 0.4949, Time: 0.80s\n",
      "Epoch: 74, Batch: 761/842, Loss: 0.3663, Time: 0.78s\n",
      "Epoch: 74, Batch: 771/842, Loss: 0.3206, Time: 0.76s\n",
      "Epoch: 74, Batch: 781/842, Loss: 0.4299, Time: 0.83s\n",
      "Epoch: 74, Batch: 791/842, Loss: 0.4980, Time: 0.82s\n",
      "Epoch: 74, Batch: 801/842, Loss: 0.2338, Time: 0.85s\n",
      "Epoch: 74, Batch: 811/842, Loss: 0.3936, Time: 0.82s\n",
      "Epoch: 74, Batch: 821/842, Loss: 0.5304, Time: 0.80s\n",
      "Epoch: 74, Batch: 831/842, Loss: 0.5289, Time: 0.76s\n",
      "Epoch: 74, Batch: 841/842, Loss: 0.5075, Time: 0.74s\n",
      "Epoch 74/100: Train Loss: 0.4532, Val Loss: 0.4231, mIoU: 0.6958, F1: 0.8145, OA: 0.9655\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7832, F1=0.8784, OA=0.8762, Precision=0.8342, Recall=0.9276\n",
      "    Class 1: IoU=0.7154, F1=0.8341, OA=0.9966, Precision=0.8681, Recall=0.8027\n",
      "    Class 2: IoU=0.9126, F1=0.9543, OA=0.9816, Precision=0.9584, Recall=0.9502\n",
      "    Class 3: IoU=0.4927, F1=0.6602, OA=0.9645, Precision=0.7885, Recall=0.5677\n",
      "    Class 4: IoU=0.7591, F1=0.8630, OA=0.9818, Precision=0.8842, Recall=0.8429\n",
      "    Class 5: IoU=0.6782, F1=0.8082, OA=0.9965, Precision=0.7968, Recall=0.8200\n",
      "    Class 6: IoU=0.5760, F1=0.7310, OA=0.9582, Precision=0.8274, Recall=0.6546\n",
      "    Class 7: IoU=0.6489, F1=0.7870, OA=0.9688, Precision=0.8663, Recall=0.7211\n",
      "Epoch: 75, Batch: 1/842, Loss: 0.4272, Time: 1.95s\n",
      "Epoch: 75, Batch: 11/842, Loss: 0.3729, Time: 0.83s\n",
      "Epoch: 75, Batch: 21/842, Loss: 0.3077, Time: 0.82s\n",
      "Epoch: 75, Batch: 31/842, Loss: 0.3566, Time: 0.76s\n",
      "Epoch: 75, Batch: 41/842, Loss: 0.3711, Time: 0.79s\n",
      "Epoch: 75, Batch: 51/842, Loss: 0.3584, Time: 0.78s\n",
      "Epoch: 75, Batch: 61/842, Loss: 0.4715, Time: 0.79s\n",
      "Epoch: 75, Batch: 71/842, Loss: 0.5044, Time: 0.79s\n",
      "Epoch: 75, Batch: 81/842, Loss: 0.5544, Time: 0.82s\n",
      "Epoch: 75, Batch: 91/842, Loss: 0.4689, Time: 0.87s\n",
      "Epoch: 75, Batch: 101/842, Loss: 0.3802, Time: 0.83s\n",
      "Epoch: 75, Batch: 111/842, Loss: 0.3694, Time: 0.80s\n",
      "Epoch: 75, Batch: 121/842, Loss: 0.4874, Time: 0.82s\n",
      "Epoch: 75, Batch: 131/842, Loss: 0.3424, Time: 0.84s\n",
      "Epoch: 75, Batch: 141/842, Loss: 0.3718, Time: 0.84s\n",
      "Epoch: 75, Batch: 151/842, Loss: 0.4895, Time: 0.87s\n",
      "Epoch: 75, Batch: 161/842, Loss: 0.4849, Time: 0.82s\n",
      "Epoch: 75, Batch: 171/842, Loss: 0.5315, Time: 0.81s\n",
      "Epoch: 75, Batch: 181/842, Loss: 0.3488, Time: 0.79s\n",
      "Epoch: 75, Batch: 191/842, Loss: 0.3988, Time: 0.80s\n",
      "Epoch: 75, Batch: 201/842, Loss: 0.5650, Time: 0.84s\n",
      "Epoch: 75, Batch: 211/842, Loss: 0.4704, Time: 0.85s\n",
      "Epoch: 75, Batch: 221/842, Loss: 0.3737, Time: 0.80s\n",
      "Epoch: 75, Batch: 231/842, Loss: 0.5437, Time: 0.82s\n",
      "Epoch: 75, Batch: 241/842, Loss: 0.4730, Time: 0.79s\n",
      "Epoch: 75, Batch: 251/842, Loss: 0.4734, Time: 0.78s\n",
      "Epoch: 75, Batch: 261/842, Loss: 0.3759, Time: 0.77s\n",
      "Epoch: 75, Batch: 271/842, Loss: 0.4855, Time: 0.77s\n",
      "Epoch: 75, Batch: 281/842, Loss: 0.6022, Time: 0.81s\n",
      "Epoch: 75, Batch: 291/842, Loss: 0.3877, Time: 0.74s\n",
      "Epoch: 75, Batch: 301/842, Loss: 0.4400, Time: 0.67s\n",
      "Epoch: 75, Batch: 311/842, Loss: 0.4504, Time: 0.78s\n",
      "Epoch: 75, Batch: 321/842, Loss: 0.4363, Time: 0.82s\n",
      "Epoch: 75, Batch: 331/842, Loss: 0.4707, Time: 0.81s\n",
      "Epoch: 75, Batch: 341/842, Loss: 0.4820, Time: 0.82s\n",
      "Epoch: 75, Batch: 351/842, Loss: 0.3830, Time: 0.85s\n",
      "Epoch: 75, Batch: 361/842, Loss: 0.4119, Time: 0.92s\n",
      "Epoch: 75, Batch: 371/842, Loss: 0.3940, Time: 0.85s\n",
      "Epoch: 75, Batch: 381/842, Loss: 0.4616, Time: 0.87s\n",
      "Epoch: 75, Batch: 391/842, Loss: 0.4920, Time: 0.82s\n",
      "Epoch: 75, Batch: 401/842, Loss: 0.3720, Time: 0.87s\n",
      "Epoch: 75, Batch: 411/842, Loss: 0.4058, Time: 0.84s\n",
      "Epoch: 75, Batch: 421/842, Loss: 0.4414, Time: 0.82s\n",
      "Epoch: 75, Batch: 431/842, Loss: 0.6530, Time: 0.89s\n",
      "Epoch: 75, Batch: 441/842, Loss: 0.3820, Time: 0.94s\n",
      "Epoch: 75, Batch: 451/842, Loss: 0.5117, Time: 0.91s\n",
      "Epoch: 75, Batch: 461/842, Loss: 0.4106, Time: 0.82s\n",
      "Epoch: 75, Batch: 471/842, Loss: 0.3033, Time: 0.80s\n",
      "Epoch: 75, Batch: 481/842, Loss: 0.4022, Time: 0.84s\n",
      "Epoch: 75, Batch: 491/842, Loss: 0.4441, Time: 0.83s\n",
      "Epoch: 75, Batch: 501/842, Loss: 0.4616, Time: 0.82s\n",
      "Epoch: 75, Batch: 511/842, Loss: 0.5636, Time: 0.82s\n",
      "Epoch: 75, Batch: 521/842, Loss: 0.4252, Time: 0.89s\n",
      "Epoch: 75, Batch: 531/842, Loss: 0.5681, Time: 0.81s\n",
      "Epoch: 75, Batch: 541/842, Loss: 0.3706, Time: 0.81s\n",
      "Epoch: 75, Batch: 551/842, Loss: 0.4976, Time: 0.85s\n",
      "Epoch: 75, Batch: 561/842, Loss: 0.5718, Time: 0.80s\n",
      "Epoch: 75, Batch: 571/842, Loss: 0.4338, Time: 0.80s\n",
      "Epoch: 75, Batch: 581/842, Loss: 0.4170, Time: 0.74s\n",
      "Epoch: 75, Batch: 591/842, Loss: 0.4456, Time: 0.89s\n",
      "Epoch: 75, Batch: 601/842, Loss: 0.5626, Time: 0.82s\n",
      "Epoch: 75, Batch: 611/842, Loss: 0.4153, Time: 0.83s\n",
      "Epoch: 75, Batch: 621/842, Loss: 0.5367, Time: 0.85s\n",
      "Epoch: 75, Batch: 631/842, Loss: 0.5278, Time: 0.81s\n",
      "Epoch: 75, Batch: 641/842, Loss: 0.4247, Time: 0.82s\n",
      "Epoch: 75, Batch: 651/842, Loss: 0.5996, Time: 0.89s\n",
      "Epoch: 75, Batch: 661/842, Loss: 0.4820, Time: 0.94s\n",
      "Epoch: 75, Batch: 671/842, Loss: 0.5484, Time: 0.85s\n",
      "Epoch: 75, Batch: 681/842, Loss: 0.3977, Time: 0.88s\n",
      "Epoch: 75, Batch: 691/842, Loss: 0.4267, Time: 0.95s\n",
      "Epoch: 75, Batch: 701/842, Loss: 0.5568, Time: 0.86s\n",
      "Epoch: 75, Batch: 711/842, Loss: 0.4409, Time: 0.82s\n",
      "Epoch: 75, Batch: 721/842, Loss: 0.4090, Time: 0.85s\n",
      "Epoch: 75, Batch: 731/842, Loss: 0.4686, Time: 0.82s\n",
      "Epoch: 75, Batch: 741/842, Loss: 0.4086, Time: 0.84s\n",
      "Epoch: 75, Batch: 751/842, Loss: 0.3978, Time: 0.81s\n",
      "Epoch: 75, Batch: 761/842, Loss: 0.5433, Time: 0.79s\n",
      "Epoch: 75, Batch: 771/842, Loss: 0.5410, Time: 0.81s\n",
      "Epoch: 75, Batch: 781/842, Loss: 0.4741, Time: 0.78s\n",
      "Epoch: 75, Batch: 791/842, Loss: 0.4671, Time: 0.83s\n",
      "Epoch: 75, Batch: 801/842, Loss: 0.4781, Time: 0.76s\n",
      "Epoch: 75, Batch: 811/842, Loss: 0.5008, Time: 0.77s\n",
      "Epoch: 75, Batch: 821/842, Loss: 0.4740, Time: 0.81s\n",
      "Epoch: 75, Batch: 831/842, Loss: 0.4512, Time: 0.75s\n",
      "Epoch: 75, Batch: 841/842, Loss: 0.3858, Time: 0.81s\n",
      "Epoch 75/100: Train Loss: 0.4521, Val Loss: 0.4209, mIoU: 0.6973, F1: 0.8157, OA: 0.9658\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7840, F1=0.8790, OA=0.8764, Precision=0.8326, Recall=0.9307\n",
      "    Class 1: IoU=0.7183, F1=0.8360, OA=0.9966, Precision=0.8698, Recall=0.8049\n",
      "    Class 2: IoU=0.9130, F1=0.9545, OA=0.9817, Precision=0.9599, Recall=0.9492\n",
      "    Class 3: IoU=0.4930, F1=0.6604, OA=0.9646, Precision=0.7930, Recall=0.5658\n",
      "    Class 4: IoU=0.7575, F1=0.8620, OA=0.9819, Precision=0.8965, Recall=0.8302\n",
      "    Class 5: IoU=0.6746, F1=0.8057, OA=0.9965, Precision=0.7945, Recall=0.8172\n",
      "    Class 6: IoU=0.5783, F1=0.7328, OA=0.9590, Precision=0.8433, Recall=0.6480\n",
      "    Class 7: IoU=0.6595, F1=0.7948, OA=0.9695, Precision=0.8612, Recall=0.7379\n",
      "Epoch: 76, Batch: 1/842, Loss: 0.3494, Time: 1.89s\n",
      "Epoch: 76, Batch: 11/842, Loss: 0.5125, Time: 0.85s\n",
      "Epoch: 76, Batch: 21/842, Loss: 0.4304, Time: 0.84s\n",
      "Epoch: 76, Batch: 31/842, Loss: 0.5023, Time: 0.83s\n",
      "Epoch: 76, Batch: 41/842, Loss: 0.4546, Time: 0.84s\n",
      "Epoch: 76, Batch: 51/842, Loss: 0.4302, Time: 0.83s\n",
      "Epoch: 76, Batch: 61/842, Loss: 0.3940, Time: 0.86s\n",
      "Epoch: 76, Batch: 71/842, Loss: 0.3697, Time: 0.83s\n",
      "Epoch: 76, Batch: 81/842, Loss: 0.5118, Time: 0.84s\n",
      "Epoch: 76, Batch: 91/842, Loss: 0.4744, Time: 0.83s\n",
      "Epoch: 76, Batch: 101/842, Loss: 0.5162, Time: 0.82s\n",
      "Epoch: 76, Batch: 111/842, Loss: 0.5172, Time: 0.85s\n",
      "Epoch: 76, Batch: 121/842, Loss: 0.4217, Time: 0.83s\n",
      "Epoch: 76, Batch: 131/842, Loss: 0.5046, Time: 0.86s\n",
      "Epoch: 76, Batch: 141/842, Loss: 0.4432, Time: 0.86s\n",
      "Epoch: 76, Batch: 151/842, Loss: 0.5148, Time: 0.82s\n",
      "Epoch: 76, Batch: 161/842, Loss: 0.5404, Time: 0.81s\n",
      "Epoch: 76, Batch: 171/842, Loss: 0.3569, Time: 0.77s\n",
      "Epoch: 76, Batch: 181/842, Loss: 0.4400, Time: 0.79s\n",
      "Epoch: 76, Batch: 191/842, Loss: 0.4296, Time: 0.81s\n",
      "Epoch: 76, Batch: 201/842, Loss: 0.5200, Time: 0.83s\n",
      "Epoch: 76, Batch: 211/842, Loss: 0.3863, Time: 0.80s\n",
      "Epoch: 76, Batch: 221/842, Loss: 0.3476, Time: 0.81s\n",
      "Epoch: 76, Batch: 231/842, Loss: 0.4489, Time: 0.77s\n",
      "Epoch: 76, Batch: 241/842, Loss: 0.4051, Time: 0.75s\n",
      "Epoch: 76, Batch: 251/842, Loss: 0.4487, Time: 0.87s\n",
      "Epoch: 76, Batch: 261/842, Loss: 0.5514, Time: 0.83s\n",
      "Epoch: 76, Batch: 271/842, Loss: 0.4283, Time: 0.81s\n",
      "Epoch: 76, Batch: 281/842, Loss: 0.5004, Time: 0.83s\n",
      "Epoch: 76, Batch: 291/842, Loss: 0.5142, Time: 0.76s\n",
      "Epoch: 76, Batch: 301/842, Loss: 0.4033, Time: 0.81s\n",
      "Epoch: 76, Batch: 311/842, Loss: 0.4503, Time: 0.82s\n",
      "Epoch: 76, Batch: 321/842, Loss: 0.4688, Time: 0.81s\n",
      "Epoch: 76, Batch: 331/842, Loss: 0.5619, Time: 0.88s\n",
      "Epoch: 76, Batch: 341/842, Loss: 0.3816, Time: 0.91s\n",
      "Epoch: 76, Batch: 351/842, Loss: 0.5363, Time: 0.91s\n",
      "Epoch: 76, Batch: 361/842, Loss: 0.4441, Time: 0.80s\n",
      "Epoch: 76, Batch: 371/842, Loss: 0.4139, Time: 0.82s\n",
      "Epoch: 76, Batch: 381/842, Loss: 0.2930, Time: 0.82s\n",
      "Epoch: 76, Batch: 391/842, Loss: 0.6175, Time: 0.89s\n",
      "Epoch: 76, Batch: 401/842, Loss: 0.3775, Time: 0.84s\n",
      "Epoch: 76, Batch: 411/842, Loss: 0.5144, Time: 0.85s\n",
      "Epoch: 76, Batch: 421/842, Loss: 0.4903, Time: 0.84s\n",
      "Epoch: 76, Batch: 431/842, Loss: 0.3916, Time: 1.36s\n",
      "Epoch: 76, Batch: 441/842, Loss: 0.3804, Time: 0.77s\n",
      "Epoch: 76, Batch: 451/842, Loss: 0.4233, Time: 0.78s\n",
      "Epoch: 76, Batch: 461/842, Loss: 0.4694, Time: 0.81s\n",
      "Epoch: 76, Batch: 471/842, Loss: 0.3692, Time: 0.82s\n",
      "Epoch: 76, Batch: 481/842, Loss: 0.5293, Time: 0.81s\n",
      "Epoch: 76, Batch: 491/842, Loss: 0.5393, Time: 0.88s\n",
      "Epoch: 76, Batch: 501/842, Loss: 0.4488, Time: 0.83s\n",
      "Epoch: 76, Batch: 511/842, Loss: 0.5116, Time: 0.82s\n",
      "Epoch: 76, Batch: 521/842, Loss: 0.4265, Time: 0.80s\n",
      "Epoch: 76, Batch: 531/842, Loss: 0.4999, Time: 0.81s\n",
      "Epoch: 76, Batch: 541/842, Loss: 0.4331, Time: 0.82s\n",
      "Epoch: 76, Batch: 551/842, Loss: 0.3937, Time: 0.82s\n",
      "Epoch: 76, Batch: 561/842, Loss: 0.4209, Time: 0.84s\n",
      "Epoch: 76, Batch: 571/842, Loss: 0.4509, Time: 0.84s\n",
      "Epoch: 76, Batch: 581/842, Loss: 0.4824, Time: 0.81s\n",
      "Epoch: 76, Batch: 591/842, Loss: 0.3466, Time: 0.84s\n",
      "Epoch: 76, Batch: 601/842, Loss: 0.4730, Time: 0.83s\n",
      "Epoch: 76, Batch: 611/842, Loss: 0.4783, Time: 0.85s\n",
      "Epoch: 76, Batch: 621/842, Loss: 0.4053, Time: 0.84s\n",
      "Epoch: 76, Batch: 631/842, Loss: 0.4100, Time: 0.84s\n",
      "Epoch: 76, Batch: 641/842, Loss: 0.4696, Time: 0.84s\n",
      "Epoch: 76, Batch: 651/842, Loss: 0.3454, Time: 0.83s\n",
      "Epoch: 76, Batch: 661/842, Loss: 0.6058, Time: 0.89s\n",
      "Epoch: 76, Batch: 671/842, Loss: 0.3551, Time: 0.86s\n",
      "Epoch: 76, Batch: 681/842, Loss: 0.4225, Time: 0.82s\n",
      "Epoch: 76, Batch: 691/842, Loss: 0.4006, Time: 0.79s\n",
      "Epoch: 76, Batch: 701/842, Loss: 0.3743, Time: 0.85s\n",
      "Epoch: 76, Batch: 711/842, Loss: 0.3938, Time: 0.83s\n",
      "Epoch: 76, Batch: 721/842, Loss: 0.3563, Time: 0.86s\n",
      "Epoch: 76, Batch: 731/842, Loss: 0.3921, Time: 0.83s\n",
      "Epoch: 76, Batch: 741/842, Loss: 0.4725, Time: 0.80s\n",
      "Epoch: 76, Batch: 751/842, Loss: 0.5012, Time: 0.81s\n",
      "Epoch: 76, Batch: 761/842, Loss: 0.3511, Time: 0.81s\n",
      "Epoch: 76, Batch: 771/842, Loss: 0.5276, Time: 0.80s\n",
      "Epoch: 76, Batch: 781/842, Loss: 0.4881, Time: 0.79s\n",
      "Epoch: 76, Batch: 791/842, Loss: 0.5000, Time: 0.80s\n",
      "Epoch: 76, Batch: 801/842, Loss: 0.5162, Time: 0.82s\n",
      "Epoch: 76, Batch: 811/842, Loss: 0.6108, Time: 0.80s\n",
      "Epoch: 76, Batch: 821/842, Loss: 0.3775, Time: 0.82s\n",
      "Epoch: 76, Batch: 831/842, Loss: 0.5560, Time: 0.84s\n",
      "Epoch: 76, Batch: 841/842, Loss: 0.5269, Time: 0.83s\n",
      "Epoch 76/100: Train Loss: 0.4532, Val Loss: 0.4220, mIoU: 0.6970, F1: 0.8153, OA: 0.9657\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7840, F1=0.8789, OA=0.8767, Precision=0.8349, Recall=0.9279\n",
      "    Class 1: IoU=0.7143, F1=0.8334, OA=0.9966, Precision=0.8739, Recall=0.7964\n",
      "    Class 2: IoU=0.9130, F1=0.9545, OA=0.9817, Precision=0.9600, Recall=0.9490\n",
      "    Class 3: IoU=0.4885, F1=0.6563, OA=0.9647, Precision=0.8057, Recall=0.5537\n",
      "    Class 4: IoU=0.7587, F1=0.8628, OA=0.9818, Precision=0.8876, Recall=0.8393\n",
      "    Class 5: IoU=0.6804, F1=0.8098, OA=0.9966, Precision=0.7992, Recall=0.8206\n",
      "    Class 6: IoU=0.5802, F1=0.7343, OA=0.9580, Precision=0.8137, Recall=0.6691\n",
      "    Class 7: IoU=0.6567, F1=0.7927, OA=0.9694, Precision=0.8665, Recall=0.7305\n",
      "Epoch: 77, Batch: 1/842, Loss: 0.5845, Time: 1.48s\n",
      "Epoch: 77, Batch: 11/842, Loss: 0.4249, Time: 0.87s\n",
      "Epoch: 77, Batch: 21/842, Loss: 0.5342, Time: 0.83s\n",
      "Epoch: 77, Batch: 31/842, Loss: 0.4430, Time: 0.83s\n",
      "Epoch: 77, Batch: 41/842, Loss: 0.3908, Time: 0.83s\n",
      "Epoch: 77, Batch: 51/842, Loss: 0.3918, Time: 0.80s\n",
      "Epoch: 77, Batch: 61/842, Loss: 0.3392, Time: 0.81s\n",
      "Epoch: 77, Batch: 71/842, Loss: 0.3724, Time: 0.84s\n",
      "Epoch: 77, Batch: 81/842, Loss: 0.4087, Time: 0.81s\n",
      "Epoch: 77, Batch: 91/842, Loss: 0.5130, Time: 0.80s\n",
      "Epoch: 77, Batch: 101/842, Loss: 0.5863, Time: 0.75s\n",
      "Epoch: 77, Batch: 111/842, Loss: 0.3473, Time: 0.80s\n",
      "Epoch: 77, Batch: 121/842, Loss: 0.2384, Time: 0.84s\n",
      "Epoch: 77, Batch: 131/842, Loss: 0.3777, Time: 0.74s\n",
      "Epoch: 77, Batch: 141/842, Loss: 0.4778, Time: 0.77s\n",
      "Epoch: 77, Batch: 151/842, Loss: 0.5126, Time: 0.81s\n",
      "Epoch: 77, Batch: 161/842, Loss: 0.3062, Time: 0.73s\n",
      "Epoch: 77, Batch: 171/842, Loss: 0.4610, Time: 0.76s\n",
      "Epoch: 77, Batch: 181/842, Loss: 0.5567, Time: 0.78s\n",
      "Epoch: 77, Batch: 191/842, Loss: 0.5469, Time: 0.77s\n",
      "Epoch: 77, Batch: 201/842, Loss: 0.5314, Time: 0.76s\n",
      "Epoch: 77, Batch: 211/842, Loss: 0.4057, Time: 0.75s\n",
      "Epoch: 77, Batch: 221/842, Loss: 0.3839, Time: 0.78s\n",
      "Epoch: 77, Batch: 231/842, Loss: 0.3390, Time: 0.82s\n",
      "Epoch: 77, Batch: 241/842, Loss: 0.4505, Time: 0.76s\n",
      "Epoch: 77, Batch: 251/842, Loss: 0.4755, Time: 0.76s\n",
      "Epoch: 77, Batch: 261/842, Loss: 0.4272, Time: 0.84s\n",
      "Epoch: 77, Batch: 271/842, Loss: 0.4914, Time: 0.77s\n",
      "Epoch: 77, Batch: 281/842, Loss: 0.5398, Time: 0.74s\n",
      "Epoch: 77, Batch: 291/842, Loss: 0.4222, Time: 0.77s\n",
      "Epoch: 77, Batch: 301/842, Loss: 0.3452, Time: 0.76s\n",
      "Epoch: 77, Batch: 311/842, Loss: 0.4472, Time: 0.78s\n",
      "Epoch: 77, Batch: 321/842, Loss: 0.3043, Time: 0.80s\n",
      "Epoch: 77, Batch: 331/842, Loss: 0.4355, Time: 0.76s\n",
      "Epoch: 77, Batch: 341/842, Loss: 0.5182, Time: 0.76s\n",
      "Epoch: 77, Batch: 351/842, Loss: 0.5041, Time: 0.78s\n",
      "Epoch: 77, Batch: 361/842, Loss: 0.4045, Time: 0.76s\n",
      "Epoch: 77, Batch: 371/842, Loss: 0.3658, Time: 0.78s\n",
      "Epoch: 77, Batch: 381/842, Loss: 0.4803, Time: 0.77s\n",
      "Epoch: 77, Batch: 391/842, Loss: 0.4679, Time: 0.79s\n",
      "Epoch: 77, Batch: 401/842, Loss: 0.4163, Time: 0.76s\n",
      "Epoch: 77, Batch: 411/842, Loss: 0.4590, Time: 0.77s\n",
      "Epoch: 77, Batch: 421/842, Loss: 0.4357, Time: 0.78s\n",
      "Epoch: 77, Batch: 431/842, Loss: 0.5479, Time: 0.75s\n",
      "Epoch: 77, Batch: 441/842, Loss: 0.5303, Time: 0.72s\n",
      "Epoch: 77, Batch: 451/842, Loss: 0.4897, Time: 0.75s\n",
      "Epoch: 77, Batch: 461/842, Loss: 0.4679, Time: 0.75s\n",
      "Epoch: 77, Batch: 471/842, Loss: 0.3301, Time: 0.80s\n",
      "Epoch: 77, Batch: 481/842, Loss: 0.5008, Time: 0.77s\n",
      "Epoch: 77, Batch: 491/842, Loss: 0.4298, Time: 0.75s\n",
      "Epoch: 77, Batch: 501/842, Loss: 0.3534, Time: 0.79s\n",
      "Epoch: 77, Batch: 511/842, Loss: 0.3900, Time: 0.85s\n",
      "Epoch: 77, Batch: 521/842, Loss: 0.4800, Time: 0.76s\n",
      "Epoch: 77, Batch: 531/842, Loss: 0.5050, Time: 0.78s\n",
      "Epoch: 77, Batch: 541/842, Loss: 0.4229, Time: 0.74s\n",
      "Epoch: 77, Batch: 551/842, Loss: 0.4754, Time: 0.75s\n",
      "Epoch: 77, Batch: 561/842, Loss: 0.4118, Time: 0.83s\n",
      "Epoch: 77, Batch: 571/842, Loss: 0.4316, Time: 0.81s\n",
      "Epoch: 77, Batch: 581/842, Loss: 0.5467, Time: 0.78s\n",
      "Epoch: 77, Batch: 591/842, Loss: 0.4977, Time: 0.78s\n",
      "Epoch: 77, Batch: 601/842, Loss: 0.5156, Time: 0.77s\n",
      "Epoch: 77, Batch: 611/842, Loss: 0.4843, Time: 0.78s\n",
      "Epoch: 77, Batch: 621/842, Loss: 0.4235, Time: 0.77s\n",
      "Epoch: 77, Batch: 631/842, Loss: 0.6313, Time: 0.76s\n",
      "Epoch: 77, Batch: 641/842, Loss: 0.3300, Time: 0.77s\n",
      "Epoch: 77, Batch: 651/842, Loss: 0.6136, Time: 0.78s\n",
      "Epoch: 77, Batch: 661/842, Loss: 0.4293, Time: 0.77s\n",
      "Epoch: 77, Batch: 671/842, Loss: 0.4151, Time: 0.81s\n",
      "Epoch: 77, Batch: 681/842, Loss: 0.3963, Time: 0.76s\n",
      "Epoch: 77, Batch: 691/842, Loss: 0.3753, Time: 0.75s\n",
      "Epoch: 77, Batch: 701/842, Loss: 0.4396, Time: 0.77s\n",
      "Epoch: 77, Batch: 711/842, Loss: 0.3503, Time: 0.83s\n",
      "Epoch: 77, Batch: 721/842, Loss: 0.3639, Time: 0.75s\n",
      "Epoch: 77, Batch: 731/842, Loss: 0.5986, Time: 0.77s\n",
      "Epoch: 77, Batch: 741/842, Loss: 0.5057, Time: 0.75s\n",
      "Epoch: 77, Batch: 751/842, Loss: 0.4340, Time: 0.78s\n",
      "Epoch: 77, Batch: 761/842, Loss: 0.3815, Time: 0.77s\n",
      "Epoch: 77, Batch: 771/842, Loss: 0.3549, Time: 0.78s\n",
      "Epoch: 77, Batch: 781/842, Loss: 0.4339, Time: 0.77s\n",
      "Epoch: 77, Batch: 791/842, Loss: 0.4700, Time: 0.81s\n",
      "Epoch: 77, Batch: 801/842, Loss: 0.4399, Time: 0.79s\n",
      "Epoch: 77, Batch: 811/842, Loss: 0.3534, Time: 0.76s\n",
      "Epoch: 77, Batch: 821/842, Loss: 0.4010, Time: 0.79s\n",
      "Epoch: 77, Batch: 831/842, Loss: 0.5996, Time: 0.76s\n",
      "Epoch: 77, Batch: 841/842, Loss: 0.5152, Time: 0.74s\n",
      "Epoch 77/100: Train Loss: 0.4555, Val Loss: 0.4190, mIoU: 0.7001, F1: 0.8177, OA: 0.9660\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7848, F1=0.8794, OA=0.8778, Precision=0.8383, Recall=0.9248\n",
      "    Class 1: IoU=0.7192, F1=0.8366, OA=0.9966, Precision=0.8638, Recall=0.8112\n",
      "    Class 2: IoU=0.9133, F1=0.9547, OA=0.9816, Precision=0.9550, Recall=0.9544\n",
      "    Class 3: IoU=0.4932, F1=0.6606, OA=0.9648, Precision=0.7973, Recall=0.5639\n",
      "    Class 4: IoU=0.7608, F1=0.8641, OA=0.9819, Precision=0.8843, Recall=0.8448\n",
      "    Class 5: IoU=0.6872, F1=0.8146, OA=0.9967, Precision=0.8173, Recall=0.8119\n",
      "    Class 6: IoU=0.5822, F1=0.7359, OA=0.9587, Precision=0.8260, Recall=0.6636\n",
      "    Class 7: IoU=0.6604, F1=0.7955, OA=0.9695, Precision=0.8589, Recall=0.7408\n",
      "Saving best model with mIoU: 0.7001\n",
      "Epoch: 78, Batch: 1/842, Loss: 0.4240, Time: 2.00s\n",
      "Epoch: 78, Batch: 11/842, Loss: 0.4237, Time: 0.80s\n",
      "Epoch: 78, Batch: 21/842, Loss: 0.4860, Time: 0.80s\n",
      "Epoch: 78, Batch: 31/842, Loss: 0.5177, Time: 0.78s\n",
      "Epoch: 78, Batch: 41/842, Loss: 0.4371, Time: 0.78s\n",
      "Epoch: 78, Batch: 51/842, Loss: 0.4591, Time: 0.79s\n",
      "Epoch: 78, Batch: 61/842, Loss: 0.5270, Time: 0.77s\n",
      "Epoch: 78, Batch: 71/842, Loss: 0.3956, Time: 0.78s\n",
      "Epoch: 78, Batch: 81/842, Loss: 0.4685, Time: 0.76s\n",
      "Epoch: 78, Batch: 91/842, Loss: 0.4842, Time: 0.77s\n",
      "Epoch: 78, Batch: 101/842, Loss: 0.3941, Time: 0.77s\n",
      "Epoch: 78, Batch: 111/842, Loss: 0.5323, Time: 0.74s\n",
      "Epoch: 78, Batch: 121/842, Loss: 0.3472, Time: 0.78s\n",
      "Epoch: 78, Batch: 131/842, Loss: 0.4661, Time: 0.77s\n",
      "Epoch: 78, Batch: 141/842, Loss: 0.4603, Time: 0.78s\n",
      "Epoch: 78, Batch: 151/842, Loss: 0.5017, Time: 0.80s\n",
      "Epoch: 78, Batch: 161/842, Loss: 0.4652, Time: 0.82s\n",
      "Epoch: 78, Batch: 171/842, Loss: 0.3177, Time: 0.81s\n",
      "Epoch: 78, Batch: 181/842, Loss: 0.3880, Time: 0.78s\n",
      "Epoch: 78, Batch: 191/842, Loss: 0.4170, Time: 0.80s\n",
      "Epoch: 78, Batch: 201/842, Loss: 0.4079, Time: 0.90s\n",
      "Epoch: 78, Batch: 211/842, Loss: 0.5438, Time: 0.75s\n",
      "Epoch: 78, Batch: 221/842, Loss: 0.4176, Time: 0.74s\n",
      "Epoch: 78, Batch: 231/842, Loss: 0.4401, Time: 0.74s\n",
      "Epoch: 78, Batch: 241/842, Loss: 0.4844, Time: 0.74s\n",
      "Epoch: 78, Batch: 251/842, Loss: 0.4649, Time: 0.79s\n",
      "Epoch: 78, Batch: 261/842, Loss: 0.4664, Time: 0.73s\n",
      "Epoch: 78, Batch: 271/842, Loss: 0.4706, Time: 0.70s\n",
      "Epoch: 78, Batch: 281/842, Loss: 0.5254, Time: 0.74s\n",
      "Epoch: 78, Batch: 291/842, Loss: 0.4766, Time: 0.80s\n",
      "Epoch: 78, Batch: 301/842, Loss: 0.6115, Time: 0.78s\n",
      "Epoch: 78, Batch: 311/842, Loss: 0.4610, Time: 0.84s\n",
      "Epoch: 78, Batch: 321/842, Loss: 0.5209, Time: 0.79s\n",
      "Epoch: 78, Batch: 331/842, Loss: 0.3353, Time: 0.76s\n",
      "Epoch: 78, Batch: 341/842, Loss: 0.5248, Time: 0.77s\n",
      "Epoch: 78, Batch: 351/842, Loss: 0.4872, Time: 0.78s\n",
      "Epoch: 78, Batch: 361/842, Loss: 0.4394, Time: 0.81s\n",
      "Epoch: 78, Batch: 371/842, Loss: 0.5259, Time: 0.78s\n",
      "Epoch: 78, Batch: 381/842, Loss: 0.5113, Time: 0.78s\n",
      "Epoch: 78, Batch: 391/842, Loss: 0.4537, Time: 0.74s\n",
      "Epoch: 78, Batch: 401/842, Loss: 0.4289, Time: 0.76s\n",
      "Epoch: 78, Batch: 411/842, Loss: 0.3443, Time: 0.75s\n",
      "Epoch: 78, Batch: 421/842, Loss: 0.3473, Time: 0.76s\n",
      "Epoch: 78, Batch: 431/842, Loss: 0.3608, Time: 0.74s\n",
      "Epoch: 78, Batch: 441/842, Loss: 0.4860, Time: 0.76s\n",
      "Epoch: 78, Batch: 451/842, Loss: 0.4086, Time: 0.76s\n",
      "Epoch: 78, Batch: 461/842, Loss: 0.4324, Time: 0.82s\n",
      "Epoch: 78, Batch: 471/842, Loss: 0.4479, Time: 0.77s\n",
      "Epoch: 78, Batch: 481/842, Loss: 0.4868, Time: 0.82s\n",
      "Epoch: 78, Batch: 491/842, Loss: 0.4360, Time: 0.77s\n",
      "Epoch: 78, Batch: 501/842, Loss: 0.5613, Time: 0.76s\n",
      "Epoch: 78, Batch: 511/842, Loss: 0.4280, Time: 0.77s\n",
      "Epoch: 78, Batch: 521/842, Loss: 0.4502, Time: 0.81s\n",
      "Epoch: 78, Batch: 531/842, Loss: 0.3467, Time: 0.71s\n",
      "Epoch: 78, Batch: 541/842, Loss: 0.4626, Time: 0.79s\n",
      "Epoch: 78, Batch: 551/842, Loss: 0.5451, Time: 0.77s\n",
      "Epoch: 78, Batch: 561/842, Loss: 0.4671, Time: 0.78s\n",
      "Epoch: 78, Batch: 571/842, Loss: 0.3415, Time: 0.73s\n",
      "Epoch: 78, Batch: 581/842, Loss: 0.4315, Time: 0.76s\n",
      "Epoch: 78, Batch: 591/842, Loss: 0.4120, Time: 0.78s\n",
      "Epoch: 78, Batch: 601/842, Loss: 0.4955, Time: 0.75s\n",
      "Epoch: 78, Batch: 611/842, Loss: 0.5332, Time: 0.77s\n",
      "Epoch: 78, Batch: 621/842, Loss: 0.5515, Time: 0.80s\n",
      "Epoch: 78, Batch: 631/842, Loss: 0.3345, Time: 0.76s\n",
      "Epoch: 78, Batch: 641/842, Loss: 0.3244, Time: 0.74s\n",
      "Epoch: 78, Batch: 651/842, Loss: 0.4704, Time: 0.76s\n",
      "Epoch: 78, Batch: 661/842, Loss: 0.4254, Time: 0.77s\n",
      "Epoch: 78, Batch: 671/842, Loss: 0.4136, Time: 0.82s\n",
      "Epoch: 78, Batch: 681/842, Loss: 0.3686, Time: 0.78s\n",
      "Epoch: 78, Batch: 691/842, Loss: 0.4668, Time: 0.79s\n",
      "Epoch: 78, Batch: 701/842, Loss: 0.4952, Time: 0.80s\n",
      "Epoch: 78, Batch: 711/842, Loss: 0.5375, Time: 0.75s\n",
      "Epoch: 78, Batch: 721/842, Loss: 0.3180, Time: 0.75s\n",
      "Epoch: 78, Batch: 731/842, Loss: 0.5362, Time: 0.77s\n",
      "Epoch: 78, Batch: 741/842, Loss: 0.3364, Time: 0.77s\n",
      "Epoch: 78, Batch: 751/842, Loss: 0.7248, Time: 0.77s\n",
      "Epoch: 78, Batch: 761/842, Loss: 0.3893, Time: 0.77s\n",
      "Epoch: 78, Batch: 771/842, Loss: 0.4435, Time: 0.76s\n",
      "Epoch: 78, Batch: 781/842, Loss: 0.3506, Time: 0.75s\n",
      "Epoch: 78, Batch: 791/842, Loss: 0.4711, Time: 0.76s\n",
      "Epoch: 78, Batch: 801/842, Loss: 0.4262, Time: 0.80s\n",
      "Epoch: 78, Batch: 811/842, Loss: 0.4036, Time: 0.79s\n",
      "Epoch: 78, Batch: 821/842, Loss: 0.4443, Time: 0.79s\n",
      "Epoch: 78, Batch: 831/842, Loss: 0.5214, Time: 0.76s\n",
      "Epoch: 78, Batch: 841/842, Loss: 0.5121, Time: 0.72s\n",
      "Epoch 78/100: Train Loss: 0.4480, Val Loss: 0.4203, mIoU: 0.6977, F1: 0.8158, OA: 0.9658\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7843, F1=0.8791, OA=0.8757, Precision=0.8278, Recall=0.9371\n",
      "    Class 1: IoU=0.7174, F1=0.8355, OA=0.9967, Precision=0.8756, Recall=0.7989\n",
      "    Class 2: IoU=0.9135, F1=0.9548, OA=0.9818, Precision=0.9607, Recall=0.9489\n",
      "    Class 3: IoU=0.4878, F1=0.6557, OA=0.9649, Precision=0.8128, Recall=0.5495\n",
      "    Class 4: IoU=0.7599, F1=0.8636, OA=0.9821, Precision=0.8976, Recall=0.8320\n",
      "    Class 5: IoU=0.6870, F1=0.8145, OA=0.9967, Precision=0.8167, Recall=0.8123\n",
      "    Class 6: IoU=0.5773, F1=0.7320, OA=0.9593, Precision=0.8532, Recall=0.6409\n",
      "    Class 7: IoU=0.6548, F1=0.7914, OA=0.9695, Precision=0.8750, Recall=0.7224\n",
      "Epoch: 79, Batch: 1/842, Loss: 0.4969, Time: 1.83s\n",
      "Epoch: 79, Batch: 11/842, Loss: 0.4302, Time: 0.75s\n",
      "Epoch: 79, Batch: 21/842, Loss: 0.4116, Time: 0.78s\n",
      "Epoch: 79, Batch: 31/842, Loss: 0.4617, Time: 0.78s\n",
      "Epoch: 79, Batch: 41/842, Loss: 0.4527, Time: 0.79s\n",
      "Epoch: 79, Batch: 51/842, Loss: 0.5248, Time: 0.76s\n",
      "Epoch: 79, Batch: 61/842, Loss: 0.4594, Time: 0.78s\n",
      "Epoch: 79, Batch: 71/842, Loss: 0.4001, Time: 0.77s\n",
      "Epoch: 79, Batch: 81/842, Loss: 0.4969, Time: 0.77s\n",
      "Epoch: 79, Batch: 91/842, Loss: 0.5382, Time: 0.77s\n",
      "Epoch: 79, Batch: 101/842, Loss: 0.5086, Time: 0.77s\n",
      "Epoch: 79, Batch: 111/842, Loss: 0.3520, Time: 0.77s\n",
      "Epoch: 79, Batch: 121/842, Loss: 0.5682, Time: 0.80s\n",
      "Epoch: 79, Batch: 131/842, Loss: 0.5065, Time: 0.80s\n",
      "Epoch: 79, Batch: 141/842, Loss: 0.4679, Time: 0.79s\n",
      "Epoch: 79, Batch: 151/842, Loss: 0.4794, Time: 0.80s\n",
      "Epoch: 79, Batch: 161/842, Loss: 0.4687, Time: 0.84s\n",
      "Epoch: 79, Batch: 171/842, Loss: 0.5257, Time: 0.83s\n",
      "Epoch: 79, Batch: 181/842, Loss: 0.5088, Time: 0.82s\n",
      "Epoch: 79, Batch: 191/842, Loss: 0.4236, Time: 0.79s\n",
      "Epoch: 79, Batch: 201/842, Loss: 0.5882, Time: 0.85s\n",
      "Epoch: 79, Batch: 211/842, Loss: 0.4266, Time: 0.81s\n",
      "Epoch: 79, Batch: 221/842, Loss: 0.2870, Time: 0.79s\n",
      "Epoch: 79, Batch: 231/842, Loss: 0.3942, Time: 0.77s\n",
      "Epoch: 79, Batch: 241/842, Loss: 0.3960, Time: 0.86s\n",
      "Epoch: 79, Batch: 251/842, Loss: 0.3749, Time: 0.80s\n",
      "Epoch: 79, Batch: 261/842, Loss: 0.5024, Time: 0.78s\n",
      "Epoch: 79, Batch: 271/842, Loss: 0.4053, Time: 0.77s\n",
      "Epoch: 79, Batch: 281/842, Loss: 0.4422, Time: 0.79s\n",
      "Epoch: 79, Batch: 291/842, Loss: 0.3558, Time: 0.74s\n",
      "Epoch: 79, Batch: 301/842, Loss: 0.3835, Time: 0.76s\n",
      "Epoch: 79, Batch: 311/842, Loss: 0.4360, Time: 0.86s\n",
      "Epoch: 79, Batch: 321/842, Loss: 0.5218, Time: 0.76s\n",
      "Epoch: 79, Batch: 331/842, Loss: 0.4458, Time: 0.75s\n",
      "Epoch: 79, Batch: 341/842, Loss: 0.3201, Time: 0.75s\n",
      "Epoch: 79, Batch: 351/842, Loss: 0.4080, Time: 0.78s\n",
      "Epoch: 79, Batch: 361/842, Loss: 0.4119, Time: 0.76s\n",
      "Epoch: 79, Batch: 371/842, Loss: 0.5063, Time: 0.77s\n",
      "Epoch: 79, Batch: 381/842, Loss: 0.4661, Time: 0.74s\n",
      "Epoch: 79, Batch: 391/842, Loss: 0.5210, Time: 0.77s\n",
      "Epoch: 79, Batch: 401/842, Loss: 0.3741, Time: 0.79s\n",
      "Epoch: 79, Batch: 411/842, Loss: 0.3222, Time: 0.74s\n",
      "Epoch: 79, Batch: 421/842, Loss: 0.4357, Time: 0.77s\n",
      "Epoch: 79, Batch: 431/842, Loss: 0.3996, Time: 0.81s\n",
      "Epoch: 79, Batch: 441/842, Loss: 0.4971, Time: 0.75s\n",
      "Epoch: 79, Batch: 451/842, Loss: 0.4392, Time: 0.74s\n",
      "Epoch: 79, Batch: 461/842, Loss: 0.4171, Time: 0.78s\n",
      "Epoch: 79, Batch: 471/842, Loss: 0.5048, Time: 0.80s\n",
      "Epoch: 79, Batch: 481/842, Loss: 0.3953, Time: 0.78s\n",
      "Epoch: 79, Batch: 491/842, Loss: 0.4194, Time: 0.78s\n",
      "Epoch: 79, Batch: 501/842, Loss: 0.4450, Time: 0.78s\n",
      "Epoch: 79, Batch: 511/842, Loss: 0.3382, Time: 0.76s\n",
      "Epoch: 79, Batch: 521/842, Loss: 0.4719, Time: 0.76s\n",
      "Epoch: 79, Batch: 531/842, Loss: 0.5191, Time: 0.78s\n",
      "Epoch: 79, Batch: 541/842, Loss: 0.4656, Time: 0.80s\n",
      "Epoch: 79, Batch: 551/842, Loss: 0.4704, Time: 0.79s\n",
      "Epoch: 79, Batch: 561/842, Loss: 0.4823, Time: 0.77s\n",
      "Epoch: 79, Batch: 571/842, Loss: 0.5182, Time: 0.75s\n",
      "Epoch: 79, Batch: 581/842, Loss: 0.5418, Time: 0.72s\n",
      "Epoch: 79, Batch: 591/842, Loss: 0.6100, Time: 0.75s\n",
      "Epoch: 79, Batch: 601/842, Loss: 0.5342, Time: 0.76s\n",
      "Epoch: 79, Batch: 611/842, Loss: 0.4298, Time: 0.78s\n",
      "Epoch: 79, Batch: 621/842, Loss: 0.4218, Time: 0.79s\n",
      "Epoch: 79, Batch: 631/842, Loss: 0.4421, Time: 0.78s\n",
      "Epoch: 79, Batch: 641/842, Loss: 0.4330, Time: 0.78s\n",
      "Epoch: 79, Batch: 651/842, Loss: 0.5583, Time: 0.76s\n",
      "Epoch: 79, Batch: 661/842, Loss: 0.4531, Time: 0.77s\n",
      "Epoch: 79, Batch: 671/842, Loss: 0.4549, Time: 0.79s\n",
      "Epoch: 79, Batch: 681/842, Loss: 0.3801, Time: 0.73s\n",
      "Epoch: 79, Batch: 691/842, Loss: 0.3652, Time: 0.81s\n",
      "Epoch: 79, Batch: 701/842, Loss: 0.5504, Time: 0.79s\n",
      "Epoch: 79, Batch: 711/842, Loss: 0.5030, Time: 0.77s\n",
      "Epoch: 79, Batch: 721/842, Loss: 0.4749, Time: 0.77s\n",
      "Epoch: 79, Batch: 731/842, Loss: 0.3508, Time: 0.78s\n",
      "Epoch: 79, Batch: 741/842, Loss: 0.4246, Time: 0.81s\n",
      "Epoch: 79, Batch: 751/842, Loss: 0.4838, Time: 0.80s\n",
      "Epoch: 79, Batch: 761/842, Loss: 0.4811, Time: 0.80s\n",
      "Epoch: 79, Batch: 771/842, Loss: 0.5476, Time: 0.78s\n",
      "Epoch: 79, Batch: 781/842, Loss: 0.3575, Time: 0.83s\n",
      "Epoch: 79, Batch: 791/842, Loss: 0.5460, Time: 0.80s\n",
      "Epoch: 79, Batch: 801/842, Loss: 0.4264, Time: 0.91s\n",
      "Epoch: 79, Batch: 811/842, Loss: 0.4208, Time: 0.80s\n",
      "Epoch: 79, Batch: 821/842, Loss: 0.4227, Time: 0.81s\n",
      "Epoch: 79, Batch: 831/842, Loss: 0.5576, Time: 0.78s\n",
      "Epoch: 79, Batch: 841/842, Loss: 0.4301, Time: 0.73s\n",
      "Epoch 79/100: Train Loss: 0.4529, Val Loss: 0.4215, mIoU: 0.6967, F1: 0.8152, OA: 0.9657\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7837, F1=0.8787, OA=0.8758, Precision=0.8304, Recall=0.9331\n",
      "    Class 1: IoU=0.7173, F1=0.8354, OA=0.9966, Precision=0.8645, Recall=0.8082\n",
      "    Class 2: IoU=0.9131, F1=0.9546, OA=0.9817, Precision=0.9582, Recall=0.9511\n",
      "    Class 3: IoU=0.4907, F1=0.6583, OA=0.9648, Precision=0.8029, Recall=0.5579\n",
      "    Class 4: IoU=0.7594, F1=0.8633, OA=0.9819, Precision=0.8903, Recall=0.8379\n",
      "    Class 5: IoU=0.6801, F1=0.8096, OA=0.9966, Precision=0.8017, Recall=0.8177\n",
      "    Class 6: IoU=0.5791, F1=0.7335, OA=0.9590, Precision=0.8405, Recall=0.6506\n",
      "    Class 7: IoU=0.6501, F1=0.7879, OA=0.9693, Precision=0.8822, Recall=0.7119\n",
      "Epoch: 80, Batch: 1/842, Loss: 0.4533, Time: 1.91s\n",
      "Epoch: 80, Batch: 11/842, Loss: 0.6295, Time: 0.79s\n",
      "Epoch: 80, Batch: 21/842, Loss: 0.6189, Time: 1.54s\n",
      "Epoch: 80, Batch: 31/842, Loss: 0.2589, Time: 0.80s\n",
      "Epoch: 80, Batch: 41/842, Loss: 0.4737, Time: 0.79s\n",
      "Epoch: 80, Batch: 51/842, Loss: 0.5438, Time: 0.74s\n",
      "Epoch: 80, Batch: 61/842, Loss: 0.4367, Time: 0.76s\n",
      "Epoch: 80, Batch: 71/842, Loss: 0.5871, Time: 0.77s\n",
      "Epoch: 80, Batch: 81/842, Loss: 0.4685, Time: 0.81s\n",
      "Epoch: 80, Batch: 91/842, Loss: 0.4610, Time: 0.78s\n",
      "Epoch: 80, Batch: 101/842, Loss: 0.6227, Time: 0.78s\n",
      "Epoch: 80, Batch: 111/842, Loss: 0.4168, Time: 0.78s\n",
      "Epoch: 80, Batch: 121/842, Loss: 0.4941, Time: 0.75s\n",
      "Epoch: 80, Batch: 131/842, Loss: 0.2691, Time: 0.77s\n",
      "Epoch: 80, Batch: 141/842, Loss: 0.5517, Time: 0.76s\n",
      "Epoch: 80, Batch: 151/842, Loss: 0.4804, Time: 0.72s\n",
      "Epoch: 80, Batch: 161/842, Loss: 0.4295, Time: 0.81s\n",
      "Epoch: 80, Batch: 171/842, Loss: 0.5096, Time: 0.80s\n",
      "Epoch: 80, Batch: 181/842, Loss: 0.5772, Time: 0.77s\n",
      "Epoch: 80, Batch: 191/842, Loss: 0.3950, Time: 0.81s\n",
      "Epoch: 80, Batch: 201/842, Loss: 0.5013, Time: 0.88s\n",
      "Epoch: 80, Batch: 211/842, Loss: 0.6319, Time: 0.80s\n",
      "Epoch: 80, Batch: 221/842, Loss: 0.4262, Time: 0.77s\n",
      "Epoch: 80, Batch: 231/842, Loss: 0.4076, Time: 0.70s\n",
      "Epoch: 80, Batch: 241/842, Loss: 0.3842, Time: 0.79s\n",
      "Epoch: 80, Batch: 251/842, Loss: 0.4650, Time: 0.80s\n",
      "Epoch: 80, Batch: 261/842, Loss: 0.3816, Time: 0.80s\n",
      "Epoch: 80, Batch: 271/842, Loss: 0.4229, Time: 0.71s\n",
      "Epoch: 80, Batch: 281/842, Loss: 0.3608, Time: 0.76s\n",
      "Epoch: 80, Batch: 291/842, Loss: 0.4714, Time: 0.78s\n",
      "Epoch: 80, Batch: 301/842, Loss: 0.4452, Time: 0.75s\n",
      "Epoch: 80, Batch: 311/842, Loss: 0.3687, Time: 0.87s\n",
      "Epoch: 80, Batch: 321/842, Loss: 0.4049, Time: 0.83s\n",
      "Epoch: 80, Batch: 331/842, Loss: 0.4656, Time: 0.76s\n",
      "Epoch: 80, Batch: 341/842, Loss: 0.4842, Time: 0.82s\n",
      "Epoch: 80, Batch: 351/842, Loss: 0.5031, Time: 0.79s\n",
      "Epoch: 80, Batch: 361/842, Loss: 0.4556, Time: 0.78s\n",
      "Epoch: 80, Batch: 371/842, Loss: 0.4649, Time: 0.77s\n",
      "Epoch: 80, Batch: 381/842, Loss: 0.3668, Time: 0.79s\n",
      "Epoch: 80, Batch: 391/842, Loss: 0.4164, Time: 0.80s\n",
      "Epoch: 80, Batch: 401/842, Loss: 0.5200, Time: 0.79s\n",
      "Epoch: 80, Batch: 411/842, Loss: 0.4569, Time: 0.81s\n",
      "Epoch: 80, Batch: 421/842, Loss: 0.4249, Time: 0.94s\n",
      "Epoch: 80, Batch: 431/842, Loss: 0.4806, Time: 0.82s\n",
      "Epoch: 80, Batch: 441/842, Loss: 0.5090, Time: 0.79s\n",
      "Epoch: 80, Batch: 451/842, Loss: 0.5111, Time: 0.79s\n",
      "Epoch: 80, Batch: 461/842, Loss: 0.4453, Time: 0.82s\n",
      "Epoch: 80, Batch: 471/842, Loss: 0.4621, Time: 0.82s\n",
      "Epoch: 80, Batch: 481/842, Loss: 0.3400, Time: 0.80s\n",
      "Epoch: 80, Batch: 491/842, Loss: 0.5340, Time: 0.80s\n",
      "Epoch: 80, Batch: 501/842, Loss: 0.5441, Time: 0.79s\n",
      "Epoch: 80, Batch: 511/842, Loss: 0.4408, Time: 0.78s\n",
      "Epoch: 80, Batch: 521/842, Loss: 0.6407, Time: 0.78s\n",
      "Epoch: 80, Batch: 531/842, Loss: 0.3611, Time: 0.82s\n",
      "Epoch: 80, Batch: 541/842, Loss: 0.3943, Time: 0.82s\n",
      "Epoch: 80, Batch: 551/842, Loss: 0.5338, Time: 0.75s\n",
      "Epoch: 80, Batch: 561/842, Loss: 0.4818, Time: 0.77s\n",
      "Epoch: 80, Batch: 571/842, Loss: 0.3877, Time: 0.76s\n",
      "Epoch: 80, Batch: 581/842, Loss: 0.5246, Time: 0.77s\n",
      "Epoch: 80, Batch: 591/842, Loss: 0.4352, Time: 0.74s\n",
      "Epoch: 80, Batch: 601/842, Loss: 0.4303, Time: 0.76s\n",
      "Epoch: 80, Batch: 611/842, Loss: 0.5754, Time: 0.77s\n",
      "Epoch: 80, Batch: 621/842, Loss: 0.3733, Time: 0.76s\n",
      "Epoch: 80, Batch: 631/842, Loss: 0.3576, Time: 0.75s\n",
      "Epoch: 80, Batch: 641/842, Loss: 0.4284, Time: 0.81s\n",
      "Epoch: 80, Batch: 651/842, Loss: 0.4564, Time: 0.79s\n",
      "Epoch: 80, Batch: 661/842, Loss: 0.4586, Time: 0.79s\n",
      "Epoch: 80, Batch: 671/842, Loss: 0.4702, Time: 0.78s\n",
      "Epoch: 80, Batch: 681/842, Loss: 0.3755, Time: 0.75s\n",
      "Epoch: 80, Batch: 691/842, Loss: 0.4895, Time: 0.82s\n",
      "Epoch: 80, Batch: 701/842, Loss: 0.5056, Time: 0.91s\n",
      "Epoch: 80, Batch: 711/842, Loss: 0.5034, Time: 0.88s\n",
      "Epoch: 80, Batch: 721/842, Loss: 0.3716, Time: 0.79s\n",
      "Epoch: 80, Batch: 731/842, Loss: 0.5178, Time: 0.76s\n",
      "Epoch: 80, Batch: 741/842, Loss: 0.3738, Time: 0.73s\n",
      "Epoch: 80, Batch: 751/842, Loss: 0.4691, Time: 0.73s\n",
      "Epoch: 80, Batch: 761/842, Loss: 0.3285, Time: 0.75s\n",
      "Epoch: 80, Batch: 771/842, Loss: 0.4078, Time: 0.83s\n",
      "Epoch: 80, Batch: 781/842, Loss: 0.3860, Time: 0.77s\n",
      "Epoch: 80, Batch: 791/842, Loss: 0.4329, Time: 0.82s\n",
      "Epoch: 80, Batch: 801/842, Loss: 0.4697, Time: 0.79s\n",
      "Epoch: 80, Batch: 811/842, Loss: 0.5557, Time: 0.80s\n",
      "Epoch: 80, Batch: 821/842, Loss: 0.3709, Time: 0.85s\n",
      "Epoch: 80, Batch: 831/842, Loss: 0.4235, Time: 0.75s\n",
      "Epoch: 80, Batch: 841/842, Loss: 0.4816, Time: 0.72s\n",
      "Epoch 80/100: Train Loss: 0.4506, Val Loss: 0.4238, mIoU: 0.6942, F1: 0.8134, OA: 0.9655\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7830, F1=0.8783, OA=0.8750, Precision=0.8278, Recall=0.9353\n",
      "    Class 1: IoU=0.7086, F1=0.8295, OA=0.9965, Precision=0.8611, Recall=0.8001\n",
      "    Class 2: IoU=0.9110, F1=0.9534, OA=0.9813, Precision=0.9637, Recall=0.9434\n",
      "    Class 3: IoU=0.4871, F1=0.6551, OA=0.9647, Precision=0.8071, Recall=0.5513\n",
      "    Class 4: IoU=0.7588, F1=0.8629, OA=0.9820, Precision=0.8953, Recall=0.8327\n",
      "    Class 5: IoU=0.6776, F1=0.8078, OA=0.9965, Precision=0.7944, Recall=0.8218\n",
      "    Class 6: IoU=0.5779, F1=0.7325, OA=0.9588, Precision=0.8370, Recall=0.6512\n",
      "    Class 7: IoU=0.6493, F1=0.7874, OA=0.9691, Precision=0.8753, Recall=0.7155\n",
      "Epoch: 81, Batch: 1/842, Loss: 0.5098, Time: 1.76s\n",
      "Epoch: 81, Batch: 11/842, Loss: 0.4582, Time: 0.81s\n",
      "Epoch: 81, Batch: 21/842, Loss: 0.4663, Time: 0.83s\n",
      "Epoch: 81, Batch: 31/842, Loss: 0.4434, Time: 0.79s\n",
      "Epoch: 81, Batch: 41/842, Loss: 0.4769, Time: 0.81s\n",
      "Epoch: 81, Batch: 51/842, Loss: 0.5887, Time: 0.87s\n",
      "Epoch: 81, Batch: 61/842, Loss: 0.4189, Time: 0.92s\n",
      "Epoch: 81, Batch: 71/842, Loss: 0.5961, Time: 0.87s\n",
      "Epoch: 81, Batch: 81/842, Loss: 0.3779, Time: 0.83s\n",
      "Epoch: 81, Batch: 91/842, Loss: 0.5197, Time: 0.82s\n",
      "Epoch: 81, Batch: 101/842, Loss: 0.4959, Time: 0.82s\n",
      "Epoch: 81, Batch: 111/842, Loss: 0.4704, Time: 0.76s\n",
      "Epoch: 81, Batch: 121/842, Loss: 0.5456, Time: 0.74s\n",
      "Epoch: 81, Batch: 131/842, Loss: 0.5048, Time: 0.78s\n",
      "Epoch: 81, Batch: 141/842, Loss: 0.4412, Time: 0.77s\n",
      "Epoch: 81, Batch: 151/842, Loss: 0.4225, Time: 0.79s\n",
      "Epoch: 81, Batch: 161/842, Loss: 0.4645, Time: 0.76s\n",
      "Epoch: 81, Batch: 171/842, Loss: 0.4885, Time: 0.77s\n",
      "Epoch: 81, Batch: 181/842, Loss: 0.3522, Time: 0.77s\n",
      "Epoch: 81, Batch: 191/842, Loss: 0.5600, Time: 0.78s\n",
      "Epoch: 81, Batch: 201/842, Loss: 0.3431, Time: 0.80s\n",
      "Epoch: 81, Batch: 211/842, Loss: 0.4763, Time: 0.75s\n",
      "Epoch: 81, Batch: 221/842, Loss: 0.4546, Time: 0.80s\n",
      "Epoch: 81, Batch: 231/842, Loss: 0.3619, Time: 0.81s\n",
      "Epoch: 81, Batch: 241/842, Loss: 0.4293, Time: 0.75s\n",
      "Epoch: 81, Batch: 251/842, Loss: 0.4572, Time: 0.73s\n",
      "Epoch: 81, Batch: 261/842, Loss: 0.4320, Time: 0.76s\n",
      "Epoch: 81, Batch: 271/842, Loss: 0.4560, Time: 0.75s\n",
      "Epoch: 81, Batch: 281/842, Loss: 0.4153, Time: 0.84s\n",
      "Epoch: 81, Batch: 291/842, Loss: 0.3745, Time: 0.81s\n",
      "Epoch: 81, Batch: 301/842, Loss: 0.3559, Time: 0.85s\n",
      "Epoch: 81, Batch: 311/842, Loss: 0.4888, Time: 0.82s\n",
      "Epoch: 81, Batch: 321/842, Loss: 0.5567, Time: 0.82s\n",
      "Epoch: 81, Batch: 331/842, Loss: 0.4750, Time: 0.77s\n",
      "Epoch: 81, Batch: 341/842, Loss: 0.4427, Time: 0.81s\n",
      "Epoch: 81, Batch: 351/842, Loss: 0.3330, Time: 0.80s\n",
      "Epoch: 81, Batch: 361/842, Loss: 0.4596, Time: 0.80s\n",
      "Epoch: 81, Batch: 371/842, Loss: 0.5196, Time: 0.84s\n",
      "Epoch: 81, Batch: 381/842, Loss: 0.4891, Time: 0.86s\n",
      "Epoch: 81, Batch: 391/842, Loss: 0.3642, Time: 0.85s\n",
      "Epoch: 81, Batch: 401/842, Loss: 0.4413, Time: 0.83s\n",
      "Epoch: 81, Batch: 411/842, Loss: 0.5417, Time: 0.82s\n",
      "Epoch: 81, Batch: 421/842, Loss: 0.5253, Time: 0.82s\n",
      "Epoch: 81, Batch: 431/842, Loss: 0.5150, Time: 0.86s\n",
      "Epoch: 81, Batch: 441/842, Loss: 0.5151, Time: 0.83s\n",
      "Epoch: 81, Batch: 451/842, Loss: 0.4103, Time: 0.79s\n",
      "Epoch: 81, Batch: 461/842, Loss: 0.3528, Time: 0.78s\n",
      "Epoch: 81, Batch: 471/842, Loss: 0.4412, Time: 0.79s\n",
      "Epoch: 81, Batch: 481/842, Loss: 0.5254, Time: 0.79s\n",
      "Epoch: 81, Batch: 491/842, Loss: 0.4864, Time: 0.80s\n",
      "Epoch: 81, Batch: 501/842, Loss: 0.3034, Time: 0.82s\n",
      "Epoch: 81, Batch: 511/842, Loss: 0.4692, Time: 0.79s\n",
      "Epoch: 81, Batch: 521/842, Loss: 0.5453, Time: 0.79s\n",
      "Epoch: 81, Batch: 531/842, Loss: 0.3743, Time: 0.80s\n",
      "Epoch: 81, Batch: 541/842, Loss: 0.5473, Time: 0.74s\n",
      "Epoch: 81, Batch: 551/842, Loss: 0.3777, Time: 0.77s\n",
      "Epoch: 81, Batch: 561/842, Loss: 0.3381, Time: 0.83s\n",
      "Epoch: 81, Batch: 571/842, Loss: 0.5360, Time: 0.79s\n",
      "Epoch: 81, Batch: 581/842, Loss: 0.4190, Time: 0.71s\n",
      "Epoch: 81, Batch: 591/842, Loss: 0.3427, Time: 0.77s\n",
      "Epoch: 81, Batch: 601/842, Loss: 0.4592, Time: 0.83s\n",
      "Epoch: 81, Batch: 611/842, Loss: 0.5471, Time: 0.81s\n",
      "Epoch: 81, Batch: 621/842, Loss: 0.3405, Time: 0.86s\n",
      "Epoch: 81, Batch: 631/842, Loss: 0.3851, Time: 0.84s\n",
      "Epoch: 81, Batch: 641/842, Loss: 0.4395, Time: 0.92s\n",
      "Epoch: 81, Batch: 651/842, Loss: 0.4763, Time: 0.95s\n",
      "Epoch: 81, Batch: 661/842, Loss: 0.3115, Time: 0.88s\n",
      "Epoch: 81, Batch: 671/842, Loss: 0.4249, Time: 0.80s\n",
      "Epoch: 81, Batch: 681/842, Loss: 0.4650, Time: 0.83s\n",
      "Epoch: 81, Batch: 691/842, Loss: 0.5039, Time: 0.82s\n",
      "Epoch: 81, Batch: 701/842, Loss: 0.6010, Time: 0.88s\n",
      "Epoch: 81, Batch: 711/842, Loss: 0.4729, Time: 0.82s\n",
      "Epoch: 81, Batch: 721/842, Loss: 0.4857, Time: 0.80s\n",
      "Epoch: 81, Batch: 731/842, Loss: 0.5428, Time: 0.79s\n",
      "Epoch: 81, Batch: 741/842, Loss: 0.5364, Time: 0.87s\n",
      "Epoch: 81, Batch: 751/842, Loss: 0.3599, Time: 0.77s\n",
      "Epoch: 81, Batch: 761/842, Loss: 0.3822, Time: 0.80s\n",
      "Epoch: 81, Batch: 771/842, Loss: 0.5382, Time: 0.86s\n",
      "Epoch: 81, Batch: 781/842, Loss: 0.4829, Time: 0.82s\n",
      "Epoch: 81, Batch: 791/842, Loss: 0.4177, Time: 0.83s\n",
      "Epoch: 81, Batch: 801/842, Loss: 0.5134, Time: 0.83s\n",
      "Epoch: 81, Batch: 811/842, Loss: 0.4655, Time: 0.83s\n",
      "Epoch: 81, Batch: 821/842, Loss: 0.5773, Time: 0.84s\n",
      "Epoch: 81, Batch: 831/842, Loss: 0.4011, Time: 0.77s\n",
      "Epoch: 81, Batch: 841/842, Loss: 0.4188, Time: 0.75s\n",
      "Epoch 81/100: Train Loss: 0.4530, Val Loss: 0.4231, mIoU: 0.6968, F1: 0.8151, OA: 0.9655\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7832, F1=0.8784, OA=0.8754, Precision=0.8296, Recall=0.9333\n",
      "    Class 1: IoU=0.7155, F1=0.8341, OA=0.9966, Precision=0.8670, Recall=0.8037\n",
      "    Class 2: IoU=0.9121, F1=0.9540, OA=0.9815, Precision=0.9603, Recall=0.9479\n",
      "    Class 3: IoU=0.4841, F1=0.6523, OA=0.9641, Precision=0.7923, Recall=0.5544\n",
      "    Class 4: IoU=0.7593, F1=0.8632, OA=0.9819, Precision=0.8889, Recall=0.8389\n",
      "    Class 5: IoU=0.6934, F1=0.8190, OA=0.9968, Precision=0.8325, Recall=0.8058\n",
      "    Class 6: IoU=0.5772, F1=0.7319, OA=0.9588, Precision=0.8405, Recall=0.6482\n",
      "    Class 7: IoU=0.6494, F1=0.7875, OA=0.9691, Precision=0.8758, Recall=0.7153\n",
      "Epoch: 82, Batch: 1/842, Loss: 0.4699, Time: 2.02s\n",
      "Epoch: 82, Batch: 11/842, Loss: 0.5922, Time: 0.77s\n",
      "Epoch: 82, Batch: 21/842, Loss: 0.5418, Time: 0.78s\n",
      "Epoch: 82, Batch: 31/842, Loss: 0.5197, Time: 0.78s\n",
      "Epoch: 82, Batch: 41/842, Loss: 0.4121, Time: 0.76s\n",
      "Epoch: 82, Batch: 51/842, Loss: 0.4248, Time: 0.76s\n",
      "Epoch: 82, Batch: 61/842, Loss: 0.3742, Time: 0.78s\n",
      "Epoch: 82, Batch: 71/842, Loss: 0.4985, Time: 0.74s\n",
      "Epoch: 82, Batch: 81/842, Loss: 0.3152, Time: 0.75s\n",
      "Epoch: 82, Batch: 91/842, Loss: 0.5301, Time: 0.77s\n",
      "Epoch: 82, Batch: 101/842, Loss: 0.5134, Time: 0.82s\n",
      "Epoch: 82, Batch: 111/842, Loss: 0.5024, Time: 0.80s\n",
      "Epoch: 82, Batch: 121/842, Loss: 0.4315, Time: 0.81s\n",
      "Epoch: 82, Batch: 131/842, Loss: 0.4196, Time: 0.83s\n",
      "Epoch: 82, Batch: 141/842, Loss: 0.5217, Time: 0.84s\n",
      "Epoch: 82, Batch: 151/842, Loss: 0.5099, Time: 0.77s\n",
      "Epoch: 82, Batch: 161/842, Loss: 0.4197, Time: 0.83s\n",
      "Epoch: 82, Batch: 171/842, Loss: 0.3834, Time: 0.81s\n",
      "Epoch: 82, Batch: 181/842, Loss: 0.4982, Time: 0.89s\n",
      "Epoch: 82, Batch: 191/842, Loss: 0.4094, Time: 0.88s\n",
      "Epoch: 82, Batch: 201/842, Loss: 0.5368, Time: 0.85s\n",
      "Epoch: 82, Batch: 211/842, Loss: 0.3822, Time: 0.78s\n",
      "Epoch: 82, Batch: 221/842, Loss: 0.3071, Time: 0.79s\n",
      "Epoch: 82, Batch: 231/842, Loss: 0.4342, Time: 0.83s\n",
      "Epoch: 82, Batch: 241/842, Loss: 0.3432, Time: 0.83s\n",
      "Epoch: 82, Batch: 251/842, Loss: 0.5020, Time: 0.86s\n",
      "Epoch: 82, Batch: 261/842, Loss: 0.4215, Time: 0.82s\n",
      "Epoch: 82, Batch: 271/842, Loss: 0.3796, Time: 0.83s\n",
      "Epoch: 82, Batch: 281/842, Loss: 0.4018, Time: 0.81s\n",
      "Epoch: 82, Batch: 291/842, Loss: 0.2825, Time: 0.84s\n",
      "Epoch: 82, Batch: 301/842, Loss: 0.4939, Time: 0.83s\n",
      "Epoch: 82, Batch: 311/842, Loss: 0.4402, Time: 0.85s\n",
      "Epoch: 82, Batch: 321/842, Loss: 0.4412, Time: 0.84s\n",
      "Epoch: 82, Batch: 331/842, Loss: 0.3548, Time: 0.90s\n",
      "Epoch: 82, Batch: 341/842, Loss: 0.4089, Time: 0.82s\n",
      "Epoch: 82, Batch: 351/842, Loss: 0.4418, Time: 0.82s\n",
      "Epoch: 82, Batch: 361/842, Loss: 0.5406, Time: 0.80s\n",
      "Epoch: 82, Batch: 371/842, Loss: 0.5595, Time: 0.81s\n",
      "Epoch: 82, Batch: 381/842, Loss: 0.5147, Time: 0.89s\n",
      "Epoch: 82, Batch: 391/842, Loss: 0.4481, Time: 0.80s\n",
      "Epoch: 82, Batch: 401/842, Loss: 0.4336, Time: 0.80s\n",
      "Epoch: 82, Batch: 411/842, Loss: 0.4895, Time: 0.80s\n",
      "Epoch: 82, Batch: 421/842, Loss: 0.5159, Time: 0.81s\n",
      "Epoch: 82, Batch: 431/842, Loss: 0.3986, Time: 0.83s\n",
      "Epoch: 82, Batch: 441/842, Loss: 0.5224, Time: 0.82s\n",
      "Epoch: 82, Batch: 451/842, Loss: 0.4150, Time: 0.80s\n",
      "Epoch: 82, Batch: 461/842, Loss: 0.4135, Time: 0.83s\n",
      "Epoch: 82, Batch: 471/842, Loss: 0.4406, Time: 0.82s\n",
      "Epoch: 82, Batch: 481/842, Loss: 0.5048, Time: 0.87s\n",
      "Epoch: 82, Batch: 491/842, Loss: 0.3740, Time: 0.87s\n",
      "Epoch: 82, Batch: 501/842, Loss: 0.5373, Time: 0.81s\n",
      "Epoch: 82, Batch: 511/842, Loss: 0.2978, Time: 0.80s\n",
      "Epoch: 82, Batch: 521/842, Loss: 0.3881, Time: 0.78s\n",
      "Epoch: 82, Batch: 531/842, Loss: 0.3680, Time: 0.78s\n",
      "Epoch: 82, Batch: 541/842, Loss: 0.3916, Time: 0.77s\n",
      "Epoch: 82, Batch: 551/842, Loss: 0.4674, Time: 0.76s\n",
      "Epoch: 82, Batch: 561/842, Loss: 0.4708, Time: 0.78s\n",
      "Epoch: 82, Batch: 571/842, Loss: 0.5184, Time: 0.74s\n",
      "Epoch: 82, Batch: 581/842, Loss: 0.3402, Time: 0.83s\n",
      "Epoch: 82, Batch: 591/842, Loss: 0.4559, Time: 0.82s\n",
      "Epoch: 82, Batch: 601/842, Loss: 0.4885, Time: 0.84s\n",
      "Epoch: 82, Batch: 611/842, Loss: 0.4299, Time: 0.79s\n",
      "Epoch: 82, Batch: 621/842, Loss: 0.4663, Time: 0.75s\n",
      "Epoch: 82, Batch: 631/842, Loss: 0.5264, Time: 0.77s\n",
      "Epoch: 82, Batch: 641/842, Loss: 0.4916, Time: 0.78s\n",
      "Epoch: 82, Batch: 651/842, Loss: 0.5248, Time: 0.77s\n",
      "Epoch: 82, Batch: 661/842, Loss: 0.4077, Time: 0.79s\n",
      "Epoch: 82, Batch: 671/842, Loss: 0.4497, Time: 0.84s\n",
      "Epoch: 82, Batch: 681/842, Loss: 0.5161, Time: 0.86s\n",
      "Epoch: 82, Batch: 691/842, Loss: 0.6192, Time: 0.82s\n",
      "Epoch: 82, Batch: 701/842, Loss: 0.5153, Time: 0.81s\n",
      "Epoch: 82, Batch: 711/842, Loss: 0.4895, Time: 0.79s\n",
      "Epoch: 82, Batch: 721/842, Loss: 0.5972, Time: 0.76s\n",
      "Epoch: 82, Batch: 731/842, Loss: 0.4365, Time: 0.82s\n",
      "Epoch: 82, Batch: 741/842, Loss: 0.5538, Time: 0.87s\n",
      "Epoch: 82, Batch: 751/842, Loss: 0.5276, Time: 0.78s\n",
      "Epoch: 82, Batch: 761/842, Loss: 0.5708, Time: 0.81s\n",
      "Epoch: 82, Batch: 771/842, Loss: 0.2937, Time: 0.83s\n",
      "Epoch: 82, Batch: 781/842, Loss: 0.5061, Time: 0.83s\n",
      "Epoch: 82, Batch: 791/842, Loss: 0.5724, Time: 0.85s\n",
      "Epoch: 82, Batch: 801/842, Loss: 0.5095, Time: 0.82s\n",
      "Epoch: 82, Batch: 811/842, Loss: 0.3746, Time: 0.79s\n",
      "Epoch: 82, Batch: 821/842, Loss: 0.4660, Time: 0.79s\n",
      "Epoch: 82, Batch: 831/842, Loss: 0.4820, Time: 0.77s\n",
      "Epoch: 82, Batch: 841/842, Loss: 0.5400, Time: 0.70s\n",
      "Epoch 82/100: Train Loss: 0.4491, Val Loss: 0.4206, mIoU: 0.6990, F1: 0.8168, OA: 0.9658\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7844, F1=0.8792, OA=0.8765, Precision=0.8320, Recall=0.9321\n",
      "    Class 1: IoU=0.7187, F1=0.8363, OA=0.9967, Precision=0.8735, Recall=0.8022\n",
      "    Class 2: IoU=0.9124, F1=0.9542, OA=0.9816, Precision=0.9612, Recall=0.9472\n",
      "    Class 3: IoU=0.4898, F1=0.6576, OA=0.9649, Precision=0.8066, Recall=0.5550\n",
      "    Class 4: IoU=0.7610, F1=0.8643, OA=0.9820, Precision=0.8875, Recall=0.8423\n",
      "    Class 5: IoU=0.6929, F1=0.8186, OA=0.9968, Precision=0.8268, Recall=0.8105\n",
      "    Class 6: IoU=0.5806, F1=0.7346, OA=0.9588, Precision=0.8313, Recall=0.6581\n",
      "    Class 7: IoU=0.6522, F1=0.7895, OA=0.9692, Precision=0.8699, Recall=0.7227\n",
      "Epoch: 83, Batch: 1/842, Loss: 0.3226, Time: 1.73s\n",
      "Epoch: 83, Batch: 11/842, Loss: 0.4713, Time: 0.76s\n",
      "Epoch: 83, Batch: 21/842, Loss: 0.4422, Time: 0.73s\n",
      "Epoch: 83, Batch: 31/842, Loss: 0.4071, Time: 0.75s\n",
      "Epoch: 83, Batch: 41/842, Loss: 0.4893, Time: 0.76s\n",
      "Epoch: 83, Batch: 51/842, Loss: 0.4654, Time: 0.78s\n",
      "Epoch: 83, Batch: 61/842, Loss: 0.4066, Time: 0.82s\n",
      "Epoch: 83, Batch: 71/842, Loss: 0.3460, Time: 0.80s\n",
      "Epoch: 83, Batch: 81/842, Loss: 0.5647, Time: 0.84s\n",
      "Epoch: 83, Batch: 91/842, Loss: 0.5208, Time: 0.78s\n",
      "Epoch: 83, Batch: 101/842, Loss: 0.4796, Time: 0.80s\n",
      "Epoch: 83, Batch: 111/842, Loss: 0.4856, Time: 0.81s\n",
      "Epoch: 83, Batch: 121/842, Loss: 0.4119, Time: 0.80s\n",
      "Epoch: 83, Batch: 131/842, Loss: 0.4602, Time: 0.80s\n",
      "Epoch: 83, Batch: 141/842, Loss: 0.3936, Time: 0.85s\n",
      "Epoch: 83, Batch: 151/842, Loss: 0.5082, Time: 0.81s\n",
      "Epoch: 83, Batch: 161/842, Loss: 0.3344, Time: 0.79s\n",
      "Epoch: 83, Batch: 171/842, Loss: 0.5894, Time: 0.87s\n",
      "Epoch: 83, Batch: 181/842, Loss: 0.4452, Time: 0.80s\n",
      "Epoch: 83, Batch: 191/842, Loss: 0.5101, Time: 0.79s\n",
      "Epoch: 83, Batch: 201/842, Loss: 0.5684, Time: 0.76s\n",
      "Epoch: 83, Batch: 211/842, Loss: 0.3521, Time: 0.80s\n",
      "Epoch: 83, Batch: 221/842, Loss: 0.4342, Time: 0.80s\n",
      "Epoch: 83, Batch: 231/842, Loss: 0.4242, Time: 0.80s\n",
      "Epoch: 83, Batch: 241/842, Loss: 0.4683, Time: 0.77s\n",
      "Epoch: 83, Batch: 251/842, Loss: 0.3145, Time: 0.75s\n",
      "Epoch: 83, Batch: 261/842, Loss: 0.3857, Time: 0.78s\n",
      "Epoch: 83, Batch: 271/842, Loss: 0.3901, Time: 0.79s\n",
      "Epoch: 83, Batch: 281/842, Loss: 0.4135, Time: 0.78s\n",
      "Epoch: 83, Batch: 291/842, Loss: 0.5569, Time: 0.85s\n",
      "Epoch: 83, Batch: 301/842, Loss: 0.5412, Time: 0.84s\n",
      "Epoch: 83, Batch: 311/842, Loss: 0.5358, Time: 0.78s\n",
      "Epoch: 83, Batch: 321/842, Loss: 0.4405, Time: 0.78s\n",
      "Epoch: 83, Batch: 331/842, Loss: 0.2802, Time: 0.77s\n",
      "Epoch: 83, Batch: 341/842, Loss: 0.4269, Time: 0.80s\n",
      "Epoch: 83, Batch: 351/842, Loss: 0.3866, Time: 0.77s\n",
      "Epoch: 83, Batch: 361/842, Loss: 0.4271, Time: 0.78s\n",
      "Epoch: 83, Batch: 371/842, Loss: 0.4140, Time: 0.78s\n",
      "Epoch: 83, Batch: 381/842, Loss: 0.4724, Time: 1.42s\n",
      "Epoch: 83, Batch: 391/842, Loss: 0.4862, Time: 0.82s\n",
      "Epoch: 83, Batch: 401/842, Loss: 0.4337, Time: 0.80s\n",
      "Epoch: 83, Batch: 411/842, Loss: 0.4315, Time: 0.87s\n",
      "Epoch: 83, Batch: 421/842, Loss: 0.4249, Time: 0.84s\n",
      "Epoch: 83, Batch: 431/842, Loss: 0.5028, Time: 0.85s\n",
      "Epoch: 83, Batch: 441/842, Loss: 0.5422, Time: 0.81s\n",
      "Epoch: 83, Batch: 451/842, Loss: 0.4139, Time: 0.90s\n",
      "Epoch: 83, Batch: 461/842, Loss: 0.4522, Time: 0.83s\n",
      "Epoch: 83, Batch: 471/842, Loss: 0.3833, Time: 0.81s\n",
      "Epoch: 83, Batch: 481/842, Loss: 0.4104, Time: 0.89s\n",
      "Epoch: 83, Batch: 491/842, Loss: 0.3470, Time: 0.79s\n",
      "Epoch: 83, Batch: 501/842, Loss: 0.4437, Time: 0.80s\n",
      "Epoch: 83, Batch: 511/842, Loss: 0.3809, Time: 0.77s\n",
      "Epoch: 83, Batch: 521/842, Loss: 0.3471, Time: 0.75s\n",
      "Epoch: 83, Batch: 531/842, Loss: 0.4021, Time: 0.83s\n",
      "Epoch: 83, Batch: 541/842, Loss: 0.3921, Time: 0.84s\n",
      "Epoch: 83, Batch: 551/842, Loss: 0.4879, Time: 0.81s\n",
      "Epoch: 83, Batch: 561/842, Loss: 0.6191, Time: 0.80s\n",
      "Epoch: 83, Batch: 571/842, Loss: 0.4089, Time: 0.78s\n",
      "Epoch: 83, Batch: 581/842, Loss: 0.3403, Time: 0.80s\n",
      "Epoch: 83, Batch: 591/842, Loss: 0.3153, Time: 0.83s\n",
      "Epoch: 83, Batch: 601/842, Loss: 0.4405, Time: 0.82s\n",
      "Epoch: 83, Batch: 611/842, Loss: 0.3156, Time: 0.80s\n",
      "Epoch: 83, Batch: 621/842, Loss: 0.4484, Time: 0.84s\n",
      "Epoch: 83, Batch: 631/842, Loss: 0.5497, Time: 0.80s\n",
      "Epoch: 83, Batch: 641/842, Loss: 0.4679, Time: 0.79s\n",
      "Epoch: 83, Batch: 651/842, Loss: 0.3291, Time: 0.80s\n",
      "Epoch: 83, Batch: 661/842, Loss: 0.3914, Time: 0.81s\n",
      "Epoch: 83, Batch: 671/842, Loss: 0.3249, Time: 0.85s\n",
      "Epoch: 83, Batch: 681/842, Loss: 0.5306, Time: 0.80s\n",
      "Epoch: 83, Batch: 691/842, Loss: 0.4002, Time: 0.85s\n",
      "Epoch: 83, Batch: 701/842, Loss: 0.5383, Time: 0.84s\n",
      "Epoch: 83, Batch: 711/842, Loss: 0.4067, Time: 0.91s\n",
      "Epoch: 83, Batch: 721/842, Loss: 0.5438, Time: 0.86s\n",
      "Epoch: 83, Batch: 731/842, Loss: 0.4438, Time: 0.82s\n",
      "Epoch: 83, Batch: 741/842, Loss: 0.4729, Time: 0.78s\n",
      "Epoch: 83, Batch: 751/842, Loss: 0.2901, Time: 0.87s\n",
      "Epoch: 83, Batch: 761/842, Loss: 0.4552, Time: 0.81s\n",
      "Epoch: 83, Batch: 771/842, Loss: 0.5302, Time: 0.86s\n",
      "Epoch: 83, Batch: 781/842, Loss: 0.3529, Time: 0.83s\n",
      "Epoch: 83, Batch: 791/842, Loss: 0.3984, Time: 0.79s\n",
      "Epoch: 83, Batch: 801/842, Loss: 0.4037, Time: 0.78s\n",
      "Epoch: 83, Batch: 811/842, Loss: 0.4133, Time: 0.84s\n",
      "Epoch: 83, Batch: 821/842, Loss: 0.3359, Time: 0.82s\n",
      "Epoch: 83, Batch: 831/842, Loss: 0.5009, Time: 0.80s\n",
      "Epoch: 83, Batch: 841/842, Loss: 0.3822, Time: 0.76s\n",
      "Epoch 83/100: Train Loss: 0.4499, Val Loss: 0.4222, mIoU: 0.6963, F1: 0.8150, OA: 0.9656\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7839, F1=0.8789, OA=0.8765, Precision=0.8335, Recall=0.9294\n",
      "    Class 1: IoU=0.7164, F1=0.8348, OA=0.9966, Precision=0.8658, Recall=0.8059\n",
      "    Class 2: IoU=0.9123, F1=0.9542, OA=0.9815, Precision=0.9588, Recall=0.9495\n",
      "    Class 3: IoU=0.4920, F1=0.6596, OA=0.9646, Precision=0.7933, Recall=0.5644\n",
      "    Class 4: IoU=0.7595, F1=0.8633, OA=0.9819, Precision=0.8876, Recall=0.8404\n",
      "    Class 5: IoU=0.6776, F1=0.8078, OA=0.9965, Precision=0.7963, Recall=0.8197\n",
      "    Class 6: IoU=0.5790, F1=0.7334, OA=0.9587, Precision=0.8319, Recall=0.6558\n",
      "    Class 7: IoU=0.6498, F1=0.7877, OA=0.9689, Precision=0.8677, Recall=0.7213\n",
      "Epoch: 84, Batch: 1/842, Loss: 0.5123, Time: 1.86s\n",
      "Epoch: 84, Batch: 11/842, Loss: 0.4332, Time: 0.82s\n",
      "Epoch: 84, Batch: 21/842, Loss: 0.4548, Time: 0.78s\n",
      "Epoch: 84, Batch: 31/842, Loss: 0.4686, Time: 0.77s\n",
      "Epoch: 84, Batch: 41/842, Loss: 0.4217, Time: 0.79s\n",
      "Epoch: 84, Batch: 51/842, Loss: 0.4960, Time: 0.78s\n",
      "Epoch: 84, Batch: 61/842, Loss: 0.4147, Time: 0.81s\n",
      "Epoch: 84, Batch: 71/842, Loss: 0.4470, Time: 0.77s\n",
      "Epoch: 84, Batch: 81/842, Loss: 0.5303, Time: 0.75s\n",
      "Epoch: 84, Batch: 91/842, Loss: 0.4662, Time: 0.79s\n",
      "Epoch: 84, Batch: 101/842, Loss: 0.4574, Time: 0.79s\n",
      "Epoch: 84, Batch: 111/842, Loss: 0.4999, Time: 0.75s\n",
      "Epoch: 84, Batch: 121/842, Loss: 0.4624, Time: 0.87s\n",
      "Epoch: 84, Batch: 131/842, Loss: 0.4848, Time: 0.81s\n",
      "Epoch: 84, Batch: 141/842, Loss: 0.4629, Time: 0.78s\n",
      "Epoch: 84, Batch: 151/842, Loss: 0.4619, Time: 0.77s\n",
      "Epoch: 84, Batch: 161/842, Loss: 0.4355, Time: 0.77s\n",
      "Epoch: 84, Batch: 171/842, Loss: 0.5490, Time: 0.80s\n",
      "Epoch: 84, Batch: 181/842, Loss: 0.4077, Time: 0.78s\n",
      "Epoch: 84, Batch: 191/842, Loss: 0.4079, Time: 0.80s\n",
      "Epoch: 84, Batch: 201/842, Loss: 0.3962, Time: 0.78s\n",
      "Epoch: 84, Batch: 211/842, Loss: 0.4767, Time: 0.77s\n",
      "Epoch: 84, Batch: 221/842, Loss: 0.3710, Time: 0.79s\n",
      "Epoch: 84, Batch: 231/842, Loss: 0.4336, Time: 0.76s\n",
      "Epoch: 84, Batch: 241/842, Loss: 0.3465, Time: 0.80s\n",
      "Epoch: 84, Batch: 251/842, Loss: 0.4972, Time: 0.77s\n",
      "Epoch: 84, Batch: 261/842, Loss: 0.4233, Time: 0.75s\n",
      "Epoch: 84, Batch: 271/842, Loss: 0.3900, Time: 0.75s\n",
      "Epoch: 84, Batch: 281/842, Loss: 0.3744, Time: 0.77s\n",
      "Epoch: 84, Batch: 291/842, Loss: 0.4660, Time: 0.78s\n",
      "Epoch: 84, Batch: 301/842, Loss: 0.4780, Time: 0.78s\n",
      "Epoch: 84, Batch: 311/842, Loss: 0.3890, Time: 0.75s\n",
      "Epoch: 84, Batch: 321/842, Loss: 0.3897, Time: 0.76s\n",
      "Epoch: 84, Batch: 331/842, Loss: 0.4126, Time: 0.75s\n",
      "Epoch: 84, Batch: 341/842, Loss: 0.4404, Time: 0.78s\n",
      "Epoch: 84, Batch: 351/842, Loss: 0.5423, Time: 0.80s\n",
      "Epoch: 84, Batch: 361/842, Loss: 0.5457, Time: 0.78s\n",
      "Epoch: 84, Batch: 371/842, Loss: 0.4524, Time: 0.76s\n",
      "Epoch: 84, Batch: 381/842, Loss: 0.3878, Time: 0.73s\n",
      "Epoch: 84, Batch: 391/842, Loss: 0.4215, Time: 0.74s\n",
      "Epoch: 84, Batch: 401/842, Loss: 0.3805, Time: 0.76s\n",
      "Epoch: 84, Batch: 411/842, Loss: 0.5254, Time: 0.74s\n",
      "Epoch: 84, Batch: 421/842, Loss: 0.4644, Time: 0.76s\n",
      "Epoch: 84, Batch: 431/842, Loss: 0.3981, Time: 0.71s\n",
      "Epoch: 84, Batch: 441/842, Loss: 0.5698, Time: 0.78s\n",
      "Epoch: 84, Batch: 451/842, Loss: 0.4040, Time: 0.76s\n",
      "Epoch: 84, Batch: 461/842, Loss: 0.4210, Time: 0.76s\n",
      "Epoch: 84, Batch: 471/842, Loss: 0.4431, Time: 0.78s\n",
      "Epoch: 84, Batch: 481/842, Loss: 0.3247, Time: 0.82s\n",
      "Epoch: 84, Batch: 491/842, Loss: 0.5389, Time: 0.79s\n",
      "Epoch: 84, Batch: 501/842, Loss: 0.4620, Time: 0.87s\n",
      "Epoch: 84, Batch: 511/842, Loss: 0.4364, Time: 0.87s\n",
      "Epoch: 84, Batch: 521/842, Loss: 0.5194, Time: 0.82s\n",
      "Epoch: 84, Batch: 531/842, Loss: 0.3935, Time: 0.79s\n",
      "Epoch: 84, Batch: 541/842, Loss: 0.4277, Time: 0.82s\n",
      "Epoch: 84, Batch: 551/842, Loss: 0.4872, Time: 0.83s\n",
      "Epoch: 84, Batch: 561/842, Loss: 0.5179, Time: 0.81s\n",
      "Epoch: 84, Batch: 571/842, Loss: 0.4781, Time: 0.77s\n",
      "Epoch: 84, Batch: 581/842, Loss: 0.4731, Time: 0.79s\n",
      "Epoch: 84, Batch: 591/842, Loss: 0.4212, Time: 0.78s\n",
      "Epoch: 84, Batch: 601/842, Loss: 0.4918, Time: 0.76s\n",
      "Epoch: 84, Batch: 611/842, Loss: 0.4533, Time: 0.75s\n",
      "Epoch: 84, Batch: 621/842, Loss: 0.5145, Time: 0.77s\n",
      "Epoch: 84, Batch: 631/842, Loss: 0.5213, Time: 0.76s\n",
      "Epoch: 84, Batch: 641/842, Loss: 0.4362, Time: 0.73s\n",
      "Epoch: 84, Batch: 651/842, Loss: 0.4165, Time: 0.77s\n",
      "Epoch: 84, Batch: 661/842, Loss: 0.3525, Time: 0.78s\n",
      "Epoch: 84, Batch: 671/842, Loss: 0.4563, Time: 0.80s\n",
      "Epoch: 84, Batch: 681/842, Loss: 0.5156, Time: 0.82s\n",
      "Epoch: 84, Batch: 691/842, Loss: 0.3509, Time: 0.82s\n",
      "Epoch: 84, Batch: 701/842, Loss: 0.5047, Time: 0.81s\n",
      "Epoch: 84, Batch: 711/842, Loss: 0.4347, Time: 0.81s\n",
      "Epoch: 84, Batch: 721/842, Loss: 0.6103, Time: 0.80s\n",
      "Epoch: 84, Batch: 731/842, Loss: 0.5113, Time: 0.77s\n",
      "Epoch: 84, Batch: 741/842, Loss: 0.4719, Time: 0.82s\n",
      "Epoch: 84, Batch: 751/842, Loss: 0.4328, Time: 0.79s\n",
      "Epoch: 84, Batch: 761/842, Loss: 0.4725, Time: 0.79s\n",
      "Epoch: 84, Batch: 771/842, Loss: 0.3946, Time: 0.79s\n",
      "Epoch: 84, Batch: 781/842, Loss: 0.4519, Time: 0.82s\n",
      "Epoch: 84, Batch: 791/842, Loss: 0.3860, Time: 0.84s\n",
      "Epoch: 84, Batch: 801/842, Loss: 0.5214, Time: 0.79s\n",
      "Epoch: 84, Batch: 811/842, Loss: 0.4341, Time: 0.77s\n",
      "Epoch: 84, Batch: 821/842, Loss: 0.4436, Time: 0.75s\n",
      "Epoch: 84, Batch: 831/842, Loss: 0.4591, Time: 0.77s\n",
      "Epoch: 84, Batch: 841/842, Loss: 0.4283, Time: 0.82s\n",
      "Epoch 84/100: Train Loss: 0.4504, Val Loss: 0.4221, mIoU: 0.6960, F1: 0.8148, OA: 0.9657\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7840, F1=0.8789, OA=0.8761, Precision=0.8307, Recall=0.9332\n",
      "    Class 1: IoU=0.7107, F1=0.8309, OA=0.9965, Precision=0.8564, Recall=0.8069\n",
      "    Class 2: IoU=0.9114, F1=0.9537, OA=0.9814, Precision=0.9631, Recall=0.9444\n",
      "    Class 3: IoU=0.4926, F1=0.6601, OA=0.9647, Precision=0.7960, Recall=0.5638\n",
      "    Class 4: IoU=0.7604, F1=0.8639, OA=0.9821, Precision=0.8968, Recall=0.8333\n",
      "    Class 5: IoU=0.6760, F1=0.8067, OA=0.9965, Precision=0.7897, Recall=0.8245\n",
      "    Class 6: IoU=0.5797, F1=0.7339, OA=0.9589, Precision=0.8361, Recall=0.6540\n",
      "    Class 7: IoU=0.6532, F1=0.7902, OA=0.9693, Precision=0.8735, Recall=0.7214\n",
      "Epoch: 85, Batch: 1/842, Loss: 0.4316, Time: 1.77s\n",
      "Epoch: 85, Batch: 11/842, Loss: 0.4895, Time: 0.86s\n",
      "Epoch: 85, Batch: 21/842, Loss: 0.3470, Time: 0.85s\n",
      "Epoch: 85, Batch: 31/842, Loss: 0.4247, Time: 0.85s\n",
      "Epoch: 85, Batch: 41/842, Loss: 0.4046, Time: 0.84s\n",
      "Epoch: 85, Batch: 51/842, Loss: 0.4121, Time: 0.86s\n",
      "Epoch: 85, Batch: 61/842, Loss: 0.4291, Time: 0.85s\n",
      "Epoch: 85, Batch: 71/842, Loss: 0.4637, Time: 0.83s\n",
      "Epoch: 85, Batch: 81/842, Loss: 0.4073, Time: 0.84s\n",
      "Epoch: 85, Batch: 91/842, Loss: 0.3598, Time: 0.87s\n",
      "Epoch: 85, Batch: 101/842, Loss: 0.5296, Time: 0.86s\n",
      "Epoch: 85, Batch: 111/842, Loss: 0.3971, Time: 0.79s\n",
      "Epoch: 85, Batch: 121/842, Loss: 0.3382, Time: 0.84s\n",
      "Epoch: 85, Batch: 131/842, Loss: 0.4915, Time: 0.87s\n",
      "Epoch: 85, Batch: 141/842, Loss: 0.2672, Time: 0.83s\n",
      "Epoch: 85, Batch: 151/842, Loss: 0.3421, Time: 0.84s\n",
      "Epoch: 85, Batch: 161/842, Loss: 0.4099, Time: 0.83s\n",
      "Epoch: 85, Batch: 171/842, Loss: 0.4113, Time: 0.83s\n",
      "Epoch: 85, Batch: 181/842, Loss: 0.4278, Time: 0.80s\n",
      "Epoch: 85, Batch: 191/842, Loss: 0.4747, Time: 0.82s\n",
      "Epoch: 85, Batch: 201/842, Loss: 0.4907, Time: 0.80s\n",
      "Epoch: 85, Batch: 211/842, Loss: 0.3634, Time: 0.89s\n",
      "Epoch: 85, Batch: 221/842, Loss: 0.4503, Time: 0.83s\n",
      "Epoch: 85, Batch: 231/842, Loss: 0.4804, Time: 0.87s\n",
      "Epoch: 85, Batch: 241/842, Loss: 0.3284, Time: 0.81s\n",
      "Epoch: 85, Batch: 251/842, Loss: 0.4254, Time: 0.81s\n",
      "Epoch: 85, Batch: 261/842, Loss: 0.4806, Time: 0.83s\n",
      "Epoch: 85, Batch: 271/842, Loss: 0.4237, Time: 0.77s\n",
      "Epoch: 85, Batch: 281/842, Loss: 0.3793, Time: 0.80s\n",
      "Epoch: 85, Batch: 291/842, Loss: 0.5308, Time: 0.79s\n",
      "Epoch: 85, Batch: 301/842, Loss: 0.4788, Time: 0.83s\n",
      "Epoch: 85, Batch: 311/842, Loss: 0.4490, Time: 0.84s\n",
      "Epoch: 85, Batch: 321/842, Loss: 0.4736, Time: 0.76s\n",
      "Epoch: 85, Batch: 331/842, Loss: 0.5109, Time: 0.87s\n",
      "Epoch: 85, Batch: 341/842, Loss: 0.4739, Time: 0.87s\n",
      "Epoch: 85, Batch: 351/842, Loss: 0.5195, Time: 0.85s\n",
      "Epoch: 85, Batch: 361/842, Loss: 0.4028, Time: 0.83s\n",
      "Epoch: 85, Batch: 371/842, Loss: 0.5952, Time: 0.83s\n",
      "Epoch: 85, Batch: 381/842, Loss: 0.4889, Time: 0.83s\n",
      "Epoch: 85, Batch: 391/842, Loss: 0.4737, Time: 0.86s\n",
      "Epoch: 85, Batch: 401/842, Loss: 0.5219, Time: 0.83s\n",
      "Epoch: 85, Batch: 411/842, Loss: 0.5501, Time: 0.87s\n",
      "Epoch: 85, Batch: 421/842, Loss: 0.4604, Time: 0.79s\n",
      "Epoch: 85, Batch: 431/842, Loss: 0.5704, Time: 0.89s\n",
      "Epoch: 85, Batch: 441/842, Loss: 0.4496, Time: 0.88s\n",
      "Epoch: 85, Batch: 451/842, Loss: 0.5850, Time: 0.86s\n",
      "Epoch: 85, Batch: 461/842, Loss: 0.4630, Time: 0.86s\n",
      "Epoch: 85, Batch: 471/842, Loss: 0.5609, Time: 0.83s\n",
      "Epoch: 85, Batch: 481/842, Loss: 0.4087, Time: 0.82s\n",
      "Epoch: 85, Batch: 491/842, Loss: 0.3695, Time: 0.85s\n",
      "Epoch: 85, Batch: 501/842, Loss: 0.4430, Time: 0.87s\n",
      "Epoch: 85, Batch: 511/842, Loss: 0.2691, Time: 0.88s\n",
      "Epoch: 85, Batch: 521/842, Loss: 0.3228, Time: 0.86s\n",
      "Epoch: 85, Batch: 531/842, Loss: 0.4536, Time: 0.87s\n",
      "Epoch: 85, Batch: 541/842, Loss: 0.5542, Time: 0.84s\n",
      "Epoch: 85, Batch: 551/842, Loss: 0.5241, Time: 0.82s\n",
      "Epoch: 85, Batch: 561/842, Loss: 0.4891, Time: 0.89s\n",
      "Epoch: 85, Batch: 571/842, Loss: 0.4737, Time: 0.91s\n",
      "Epoch: 85, Batch: 581/842, Loss: 0.3981, Time: 0.87s\n",
      "Epoch: 85, Batch: 591/842, Loss: 0.3440, Time: 0.86s\n",
      "Epoch: 85, Batch: 601/842, Loss: 0.4657, Time: 0.84s\n",
      "Epoch: 85, Batch: 611/842, Loss: 0.4700, Time: 0.84s\n",
      "Epoch: 85, Batch: 621/842, Loss: 0.3947, Time: 0.90s\n",
      "Epoch: 85, Batch: 631/842, Loss: 0.4115, Time: 0.85s\n",
      "Epoch: 85, Batch: 641/842, Loss: 0.4002, Time: 0.85s\n",
      "Epoch: 85, Batch: 651/842, Loss: 0.5391, Time: 0.84s\n",
      "Epoch: 85, Batch: 661/842, Loss: 0.3130, Time: 0.90s\n",
      "Epoch: 85, Batch: 671/842, Loss: 0.4896, Time: 0.87s\n",
      "Epoch: 85, Batch: 681/842, Loss: 0.6579, Time: 0.86s\n",
      "Epoch: 85, Batch: 691/842, Loss: 0.3877, Time: 0.86s\n",
      "Epoch: 85, Batch: 701/842, Loss: 0.4864, Time: 0.82s\n",
      "Epoch: 85, Batch: 711/842, Loss: 0.3487, Time: 0.83s\n",
      "Epoch: 85, Batch: 721/842, Loss: 0.5057, Time: 0.85s\n",
      "Epoch: 85, Batch: 731/842, Loss: 0.4703, Time: 0.88s\n",
      "Epoch: 85, Batch: 741/842, Loss: 0.4334, Time: 0.85s\n",
      "Epoch: 85, Batch: 751/842, Loss: 0.4187, Time: 0.85s\n",
      "Epoch: 85, Batch: 761/842, Loss: 0.4545, Time: 0.90s\n",
      "Epoch: 85, Batch: 771/842, Loss: 0.4343, Time: 0.83s\n",
      "Epoch: 85, Batch: 781/842, Loss: 0.4720, Time: 0.79s\n",
      "Epoch: 85, Batch: 791/842, Loss: 0.3873, Time: 0.80s\n",
      "Epoch: 85, Batch: 801/842, Loss: 0.4180, Time: 0.79s\n",
      "Epoch: 85, Batch: 811/842, Loss: 0.5156, Time: 0.81s\n",
      "Epoch: 85, Batch: 821/842, Loss: 0.4711, Time: 0.79s\n",
      "Epoch: 85, Batch: 831/842, Loss: 0.3863, Time: 0.82s\n",
      "Epoch: 85, Batch: 841/842, Loss: 0.4325, Time: 0.81s\n",
      "Epoch 85/100: Train Loss: 0.4517, Val Loss: 0.4222, mIoU: 0.6958, F1: 0.8146, OA: 0.9657\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7837, F1=0.8787, OA=0.8756, Precision=0.8290, Recall=0.9347\n",
      "    Class 1: IoU=0.7045, F1=0.8266, OA=0.9964, Precision=0.8523, Recall=0.8024\n",
      "    Class 2: IoU=0.9107, F1=0.9533, OA=0.9812, Precision=0.9629, Recall=0.9439\n",
      "    Class 3: IoU=0.4946, F1=0.6618, OA=0.9647, Precision=0.7938, Recall=0.5674\n",
      "    Class 4: IoU=0.7610, F1=0.8643, OA=0.9821, Precision=0.8932, Recall=0.8372\n",
      "    Class 5: IoU=0.6808, F1=0.8101, OA=0.9966, Precision=0.7992, Recall=0.8213\n",
      "    Class 6: IoU=0.5763, F1=0.7312, OA=0.9592, Precision=0.8537, Recall=0.6394\n",
      "    Class 7: IoU=0.6546, F1=0.7912, OA=0.9695, Precision=0.8731, Recall=0.7234\n",
      "Epoch: 86, Batch: 1/842, Loss: 0.2964, Time: 1.37s\n",
      "Epoch: 86, Batch: 11/842, Loss: 0.5039, Time: 0.84s\n",
      "Epoch: 86, Batch: 21/842, Loss: 0.4658, Time: 0.83s\n",
      "Epoch: 86, Batch: 31/842, Loss: 0.4435, Time: 0.82s\n",
      "Epoch: 86, Batch: 41/842, Loss: 0.4621, Time: 0.78s\n",
      "Epoch: 86, Batch: 51/842, Loss: 0.3833, Time: 0.73s\n",
      "Epoch: 86, Batch: 61/842, Loss: 0.4106, Time: 0.74s\n",
      "Epoch: 86, Batch: 71/842, Loss: 0.5933, Time: 0.77s\n",
      "Epoch: 86, Batch: 81/842, Loss: 0.4415, Time: 0.79s\n",
      "Epoch: 86, Batch: 91/842, Loss: 0.4872, Time: 0.78s\n",
      "Epoch: 86, Batch: 101/842, Loss: 0.4154, Time: 0.77s\n",
      "Epoch: 86, Batch: 111/842, Loss: 0.4802, Time: 0.76s\n",
      "Epoch: 86, Batch: 121/842, Loss: 0.2777, Time: 0.79s\n",
      "Epoch: 86, Batch: 131/842, Loss: 0.4793, Time: 0.85s\n",
      "Epoch: 86, Batch: 141/842, Loss: 0.4427, Time: 0.78s\n",
      "Epoch: 86, Batch: 151/842, Loss: 0.4532, Time: 0.76s\n",
      "Epoch: 86, Batch: 161/842, Loss: 0.4364, Time: 0.79s\n",
      "Epoch: 86, Batch: 171/842, Loss: 0.4586, Time: 0.76s\n",
      "Epoch: 86, Batch: 181/842, Loss: 0.4410, Time: 0.73s\n",
      "Epoch: 86, Batch: 191/842, Loss: 0.3787, Time: 0.81s\n",
      "Epoch: 86, Batch: 201/842, Loss: 0.4517, Time: 0.80s\n",
      "Epoch: 86, Batch: 211/842, Loss: 0.4615, Time: 0.75s\n",
      "Epoch: 86, Batch: 221/842, Loss: 0.4837, Time: 0.82s\n",
      "Epoch: 86, Batch: 231/842, Loss: 0.4389, Time: 0.80s\n",
      "Epoch: 86, Batch: 241/842, Loss: 0.4407, Time: 0.83s\n",
      "Epoch: 86, Batch: 251/842, Loss: 0.3006, Time: 0.83s\n",
      "Epoch: 86, Batch: 261/842, Loss: 0.5234, Time: 0.88s\n",
      "Epoch: 86, Batch: 271/842, Loss: 0.4603, Time: 0.84s\n",
      "Epoch: 86, Batch: 281/842, Loss: 0.3681, Time: 0.80s\n",
      "Epoch: 86, Batch: 291/842, Loss: 0.4692, Time: 0.87s\n",
      "Epoch: 86, Batch: 301/842, Loss: 0.4543, Time: 0.82s\n",
      "Epoch: 86, Batch: 311/842, Loss: 0.4981, Time: 0.81s\n",
      "Epoch: 86, Batch: 321/842, Loss: 0.4113, Time: 0.83s\n",
      "Epoch: 86, Batch: 331/842, Loss: 0.6435, Time: 0.82s\n",
      "Epoch: 86, Batch: 341/842, Loss: 0.4003, Time: 0.87s\n",
      "Epoch: 86, Batch: 351/842, Loss: 0.4639, Time: 0.87s\n",
      "Epoch: 86, Batch: 361/842, Loss: 0.4984, Time: 0.89s\n",
      "Epoch: 86, Batch: 371/842, Loss: 0.5936, Time: 0.89s\n",
      "Epoch: 86, Batch: 381/842, Loss: 0.5143, Time: 0.87s\n",
      "Epoch: 86, Batch: 391/842, Loss: 0.4915, Time: 0.84s\n",
      "Epoch: 86, Batch: 401/842, Loss: 0.4730, Time: 0.85s\n",
      "Epoch: 86, Batch: 411/842, Loss: 0.4342, Time: 0.82s\n",
      "Epoch: 86, Batch: 421/842, Loss: 0.5102, Time: 0.87s\n",
      "Epoch: 86, Batch: 431/842, Loss: 0.4753, Time: 0.83s\n",
      "Epoch: 86, Batch: 441/842, Loss: 0.4725, Time: 0.84s\n",
      "Epoch: 86, Batch: 451/842, Loss: 0.4537, Time: 0.91s\n",
      "Epoch: 86, Batch: 461/842, Loss: 0.5606, Time: 0.81s\n",
      "Epoch: 86, Batch: 471/842, Loss: 0.4763, Time: 0.79s\n",
      "Epoch: 86, Batch: 481/842, Loss: 0.4897, Time: 0.89s\n",
      "Epoch: 86, Batch: 491/842, Loss: 0.4436, Time: 0.87s\n",
      "Epoch: 86, Batch: 501/842, Loss: 0.4658, Time: 0.83s\n",
      "Epoch: 86, Batch: 511/842, Loss: 0.4441, Time: 0.81s\n",
      "Epoch: 86, Batch: 521/842, Loss: 0.4255, Time: 0.83s\n",
      "Epoch: 86, Batch: 531/842, Loss: 0.3855, Time: 0.82s\n",
      "Epoch: 86, Batch: 541/842, Loss: 0.3531, Time: 0.90s\n",
      "Epoch: 86, Batch: 551/842, Loss: 0.4821, Time: 0.88s\n",
      "Epoch: 86, Batch: 561/842, Loss: 0.2292, Time: 0.86s\n",
      "Epoch: 86, Batch: 571/842, Loss: 0.3620, Time: 0.78s\n",
      "Epoch: 86, Batch: 581/842, Loss: 0.4111, Time: 0.78s\n",
      "Epoch: 86, Batch: 591/842, Loss: 0.3643, Time: 0.78s\n",
      "Epoch: 86, Batch: 601/842, Loss: 0.4039, Time: 0.75s\n",
      "Epoch: 86, Batch: 611/842, Loss: 0.3738, Time: 0.79s\n",
      "Epoch: 86, Batch: 621/842, Loss: 0.4736, Time: 0.76s\n",
      "Epoch: 86, Batch: 631/842, Loss: 0.4271, Time: 0.75s\n",
      "Epoch: 86, Batch: 641/842, Loss: 0.5317, Time: 0.81s\n",
      "Epoch: 86, Batch: 651/842, Loss: 0.4533, Time: 0.81s\n",
      "Epoch: 86, Batch: 661/842, Loss: 0.4459, Time: 0.79s\n",
      "Epoch: 86, Batch: 671/842, Loss: 0.3643, Time: 0.79s\n",
      "Epoch: 86, Batch: 681/842, Loss: 0.6603, Time: 0.84s\n",
      "Epoch: 86, Batch: 691/842, Loss: 0.5772, Time: 0.84s\n",
      "Epoch: 86, Batch: 701/842, Loss: 0.4417, Time: 0.81s\n",
      "Epoch: 86, Batch: 711/842, Loss: 0.3544, Time: 0.87s\n",
      "Epoch: 86, Batch: 721/842, Loss: 0.5615, Time: 0.89s\n",
      "Epoch: 86, Batch: 731/842, Loss: 0.3869, Time: 0.86s\n",
      "Epoch: 86, Batch: 741/842, Loss: 0.4298, Time: 0.82s\n",
      "Epoch: 86, Batch: 751/842, Loss: 0.6123, Time: 1.52s\n",
      "Epoch: 86, Batch: 761/842, Loss: 0.4772, Time: 0.80s\n",
      "Epoch: 86, Batch: 771/842, Loss: 0.4339, Time: 0.77s\n",
      "Epoch: 86, Batch: 781/842, Loss: 0.4862, Time: 0.77s\n",
      "Epoch: 86, Batch: 791/842, Loss: 0.5057, Time: 0.82s\n",
      "Epoch: 86, Batch: 801/842, Loss: 0.3543, Time: 0.82s\n",
      "Epoch: 86, Batch: 811/842, Loss: 0.4236, Time: 0.83s\n",
      "Epoch: 86, Batch: 821/842, Loss: 0.4361, Time: 0.80s\n",
      "Epoch: 86, Batch: 831/842, Loss: 0.5199, Time: 0.77s\n",
      "Epoch: 86, Batch: 841/842, Loss: 0.4551, Time: 0.75s\n",
      "Epoch 86/100: Train Loss: 0.4509, Val Loss: 0.4192, mIoU: 0.6984, F1: 0.8165, OA: 0.9659\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7850, F1=0.8795, OA=0.8769, Precision=0.8325, Recall=0.9323\n",
      "    Class 1: IoU=0.7164, F1=0.8348, OA=0.9966, Precision=0.8675, Recall=0.8044\n",
      "    Class 2: IoU=0.9129, F1=0.9545, OA=0.9817, Precision=0.9610, Recall=0.9480\n",
      "    Class 3: IoU=0.4922, F1=0.6597, OA=0.9649, Precision=0.8037, Recall=0.5595\n",
      "    Class 4: IoU=0.7595, F1=0.8633, OA=0.9820, Precision=0.8930, Recall=0.8356\n",
      "    Class 5: IoU=0.6819, F1=0.8109, OA=0.9966, Precision=0.7994, Recall=0.8227\n",
      "    Class 6: IoU=0.5805, F1=0.7346, OA=0.9591, Precision=0.8407, Recall=0.6522\n",
      "    Class 7: IoU=0.6591, F1=0.7945, OA=0.9696, Precision=0.8663, Recall=0.7338\n",
      "Epoch: 87, Batch: 1/842, Loss: 0.4365, Time: 1.68s\n",
      "Epoch: 87, Batch: 11/842, Loss: 0.5925, Time: 0.78s\n",
      "Epoch: 87, Batch: 21/842, Loss: 0.4774, Time: 0.78s\n",
      "Epoch: 87, Batch: 31/842, Loss: 0.5064, Time: 0.78s\n",
      "Epoch: 87, Batch: 41/842, Loss: 0.4364, Time: 0.76s\n",
      "Epoch: 87, Batch: 51/842, Loss: 0.5086, Time: 0.80s\n",
      "Epoch: 87, Batch: 61/842, Loss: 0.5134, Time: 0.80s\n",
      "Epoch: 87, Batch: 71/842, Loss: 0.4176, Time: 0.82s\n",
      "Epoch: 87, Batch: 81/842, Loss: 0.4271, Time: 0.85s\n",
      "Epoch: 87, Batch: 91/842, Loss: 0.4105, Time: 0.83s\n",
      "Epoch: 87, Batch: 101/842, Loss: 0.3768, Time: 0.83s\n",
      "Epoch: 87, Batch: 111/842, Loss: 0.4522, Time: 0.82s\n",
      "Epoch: 87, Batch: 121/842, Loss: 0.4421, Time: 0.81s\n",
      "Epoch: 87, Batch: 131/842, Loss: 0.4217, Time: 0.78s\n",
      "Epoch: 87, Batch: 141/842, Loss: 0.4842, Time: 0.83s\n",
      "Epoch: 87, Batch: 151/842, Loss: 0.3779, Time: 0.81s\n",
      "Epoch: 87, Batch: 161/842, Loss: 0.4956, Time: 0.84s\n",
      "Epoch: 87, Batch: 171/842, Loss: 0.4167, Time: 0.83s\n",
      "Epoch: 87, Batch: 181/842, Loss: 0.3721, Time: 0.80s\n",
      "Epoch: 87, Batch: 191/842, Loss: 0.4122, Time: 0.84s\n",
      "Epoch: 87, Batch: 201/842, Loss: 0.4281, Time: 0.79s\n",
      "Epoch: 87, Batch: 211/842, Loss: 0.4741, Time: 0.84s\n",
      "Epoch: 87, Batch: 221/842, Loss: 0.4690, Time: 0.81s\n",
      "Epoch: 87, Batch: 231/842, Loss: 0.4059, Time: 0.77s\n",
      "Epoch: 87, Batch: 241/842, Loss: 0.3925, Time: 0.81s\n",
      "Epoch: 87, Batch: 251/842, Loss: 0.5151, Time: 0.80s\n",
      "Epoch: 87, Batch: 261/842, Loss: 0.5275, Time: 0.79s\n",
      "Epoch: 87, Batch: 271/842, Loss: 0.4032, Time: 0.82s\n",
      "Epoch: 87, Batch: 281/842, Loss: 0.4625, Time: 0.76s\n",
      "Epoch: 87, Batch: 291/842, Loss: 0.5842, Time: 0.83s\n",
      "Epoch: 87, Batch: 301/842, Loss: 0.6127, Time: 0.82s\n",
      "Epoch: 87, Batch: 311/842, Loss: 0.3415, Time: 0.81s\n",
      "Epoch: 87, Batch: 321/842, Loss: 0.4796, Time: 0.77s\n",
      "Epoch: 87, Batch: 331/842, Loss: 0.4170, Time: 0.76s\n",
      "Epoch: 87, Batch: 341/842, Loss: 0.4266, Time: 0.82s\n",
      "Epoch: 87, Batch: 351/842, Loss: 0.4790, Time: 0.76s\n",
      "Epoch: 87, Batch: 361/842, Loss: 0.5709, Time: 0.78s\n",
      "Epoch: 87, Batch: 371/842, Loss: 0.5262, Time: 0.80s\n",
      "Epoch: 87, Batch: 381/842, Loss: 0.5353, Time: 0.80s\n",
      "Epoch: 87, Batch: 391/842, Loss: 0.4893, Time: 0.75s\n",
      "Epoch: 87, Batch: 401/842, Loss: 0.4779, Time: 0.79s\n",
      "Epoch: 87, Batch: 411/842, Loss: 0.4421, Time: 0.79s\n",
      "Epoch: 87, Batch: 421/842, Loss: 0.3779, Time: 0.81s\n",
      "Epoch: 87, Batch: 431/842, Loss: 0.3983, Time: 0.81s\n",
      "Epoch: 87, Batch: 441/842, Loss: 0.4595, Time: 0.81s\n",
      "Epoch: 87, Batch: 451/842, Loss: 0.4778, Time: 0.86s\n",
      "Epoch: 87, Batch: 461/842, Loss: 0.3422, Time: 0.78s\n",
      "Epoch: 87, Batch: 471/842, Loss: 0.4928, Time: 0.77s\n",
      "Epoch: 87, Batch: 481/842, Loss: 0.4447, Time: 0.77s\n",
      "Epoch: 87, Batch: 491/842, Loss: 0.2907, Time: 0.80s\n",
      "Epoch: 87, Batch: 501/842, Loss: 0.5278, Time: 0.78s\n",
      "Epoch: 87, Batch: 511/842, Loss: 0.4313, Time: 0.78s\n",
      "Epoch: 87, Batch: 521/842, Loss: 0.4871, Time: 0.79s\n",
      "Epoch: 87, Batch: 531/842, Loss: 0.3828, Time: 0.83s\n",
      "Epoch: 87, Batch: 541/842, Loss: 0.4417, Time: 0.77s\n",
      "Epoch: 87, Batch: 551/842, Loss: 0.4129, Time: 0.78s\n",
      "Epoch: 87, Batch: 561/842, Loss: 0.4126, Time: 0.78s\n",
      "Epoch: 87, Batch: 571/842, Loss: 0.5130, Time: 0.82s\n",
      "Epoch: 87, Batch: 581/842, Loss: 0.3365, Time: 0.80s\n",
      "Epoch: 87, Batch: 591/842, Loss: 0.3896, Time: 0.79s\n",
      "Epoch: 87, Batch: 601/842, Loss: 0.3717, Time: 0.82s\n",
      "Epoch: 87, Batch: 611/842, Loss: 0.5399, Time: 0.79s\n",
      "Epoch: 87, Batch: 621/842, Loss: 0.3696, Time: 0.76s\n",
      "Epoch: 87, Batch: 631/842, Loss: 0.4355, Time: 0.76s\n",
      "Epoch: 87, Batch: 641/842, Loss: 0.4070, Time: 0.74s\n",
      "Epoch: 87, Batch: 651/842, Loss: 0.5187, Time: 0.79s\n",
      "Epoch: 87, Batch: 661/842, Loss: 0.3925, Time: 0.79s\n",
      "Epoch: 87, Batch: 671/842, Loss: 0.4473, Time: 0.78s\n",
      "Epoch: 87, Batch: 681/842, Loss: 0.4152, Time: 0.82s\n",
      "Epoch: 87, Batch: 691/842, Loss: 0.4222, Time: 0.78s\n",
      "Epoch: 87, Batch: 701/842, Loss: 0.5547, Time: 0.80s\n",
      "Epoch: 87, Batch: 711/842, Loss: 0.4676, Time: 0.76s\n",
      "Epoch: 87, Batch: 721/842, Loss: 0.4268, Time: 0.76s\n",
      "Epoch: 87, Batch: 731/842, Loss: 0.2970, Time: 0.77s\n",
      "Epoch: 87, Batch: 741/842, Loss: 0.5487, Time: 0.76s\n",
      "Epoch: 87, Batch: 751/842, Loss: 0.3873, Time: 0.78s\n",
      "Epoch: 87, Batch: 761/842, Loss: 0.4760, Time: 0.77s\n",
      "Epoch: 87, Batch: 771/842, Loss: 0.5205, Time: 0.74s\n",
      "Epoch: 87, Batch: 781/842, Loss: 0.5056, Time: 0.81s\n",
      "Epoch: 87, Batch: 791/842, Loss: 0.4266, Time: 0.76s\n",
      "Epoch: 87, Batch: 801/842, Loss: 0.4542, Time: 0.81s\n",
      "Epoch: 87, Batch: 811/842, Loss: 0.4679, Time: 0.72s\n",
      "Epoch: 87, Batch: 821/842, Loss: 0.6370, Time: 0.86s\n",
      "Epoch: 87, Batch: 831/842, Loss: 0.3389, Time: 0.80s\n",
      "Epoch: 87, Batch: 841/842, Loss: 0.4379, Time: 0.74s\n",
      "Epoch 87/100: Train Loss: 0.4513, Val Loss: 0.4207, mIoU: 0.6979, F1: 0.8160, OA: 0.9657\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7843, F1=0.8791, OA=0.8762, Precision=0.8307, Recall=0.9335\n",
      "    Class 1: IoU=0.7152, F1=0.8339, OA=0.9966, Precision=0.8695, Recall=0.8012\n",
      "    Class 2: IoU=0.9125, F1=0.9542, OA=0.9816, Precision=0.9609, Recall=0.9477\n",
      "    Class 3: IoU=0.4907, F1=0.6584, OA=0.9650, Precision=0.8089, Recall=0.5551\n",
      "    Class 4: IoU=0.7609, F1=0.8642, OA=0.9821, Precision=0.8922, Recall=0.8380\n",
      "    Class 5: IoU=0.6900, F1=0.8166, OA=0.9967, Precision=0.8195, Recall=0.8137\n",
      "    Class 6: IoU=0.5788, F1=0.7332, OA=0.9586, Precision=0.8312, Recall=0.6559\n",
      "    Class 7: IoU=0.6506, F1=0.7883, OA=0.9692, Precision=0.8754, Recall=0.7170\n",
      "Epoch: 88, Batch: 1/842, Loss: 0.4348, Time: 1.47s\n",
      "Epoch: 88, Batch: 11/842, Loss: 0.5624, Time: 0.79s\n",
      "Epoch: 88, Batch: 21/842, Loss: 0.3685, Time: 0.81s\n",
      "Epoch: 88, Batch: 31/842, Loss: 0.3698, Time: 0.77s\n",
      "Epoch: 88, Batch: 41/842, Loss: 0.5161, Time: 0.74s\n",
      "Epoch: 88, Batch: 51/842, Loss: 0.5010, Time: 0.75s\n",
      "Epoch: 88, Batch: 61/842, Loss: 0.5274, Time: 0.81s\n",
      "Epoch: 88, Batch: 71/842, Loss: 0.3942, Time: 0.78s\n",
      "Epoch: 88, Batch: 81/842, Loss: 0.3445, Time: 0.80s\n",
      "Epoch: 88, Batch: 91/842, Loss: 0.5760, Time: 0.87s\n",
      "Epoch: 88, Batch: 101/842, Loss: 0.5390, Time: 0.76s\n",
      "Epoch: 88, Batch: 111/842, Loss: 0.3831, Time: 0.84s\n",
      "Epoch: 88, Batch: 121/842, Loss: 0.5569, Time: 0.83s\n",
      "Epoch: 88, Batch: 131/842, Loss: 0.5328, Time: 0.85s\n",
      "Epoch: 88, Batch: 141/842, Loss: 0.4385, Time: 0.86s\n",
      "Epoch: 88, Batch: 151/842, Loss: 0.4110, Time: 0.82s\n",
      "Epoch: 88, Batch: 161/842, Loss: 0.4046, Time: 0.81s\n",
      "Epoch: 88, Batch: 171/842, Loss: 0.4127, Time: 0.81s\n",
      "Epoch: 88, Batch: 181/842, Loss: 0.5924, Time: 0.79s\n",
      "Epoch: 88, Batch: 191/842, Loss: 0.5478, Time: 0.84s\n",
      "Epoch: 88, Batch: 201/842, Loss: 0.3899, Time: 0.76s\n",
      "Epoch: 88, Batch: 211/842, Loss: 0.4298, Time: 0.75s\n",
      "Epoch: 88, Batch: 221/842, Loss: 0.4927, Time: 0.66s\n",
      "Epoch: 88, Batch: 231/842, Loss: 0.3879, Time: 0.75s\n",
      "Epoch: 88, Batch: 241/842, Loss: 0.3720, Time: 0.78s\n",
      "Epoch: 88, Batch: 251/842, Loss: 0.5398, Time: 0.78s\n",
      "Epoch: 88, Batch: 261/842, Loss: 0.4597, Time: 0.76s\n",
      "Epoch: 88, Batch: 271/842, Loss: 0.3503, Time: 0.78s\n",
      "Epoch: 88, Batch: 281/842, Loss: 0.3977, Time: 0.75s\n",
      "Epoch: 88, Batch: 291/842, Loss: 0.4971, Time: 0.76s\n",
      "Epoch: 88, Batch: 301/842, Loss: 0.3696, Time: 0.76s\n",
      "Epoch: 88, Batch: 311/842, Loss: 0.5051, Time: 0.77s\n",
      "Epoch: 88, Batch: 321/842, Loss: 0.4734, Time: 0.76s\n",
      "Epoch: 88, Batch: 331/842, Loss: 0.4882, Time: 0.80s\n",
      "Epoch: 88, Batch: 341/842, Loss: 0.4507, Time: 0.77s\n",
      "Epoch: 88, Batch: 351/842, Loss: 0.5503, Time: 0.77s\n",
      "Epoch: 88, Batch: 361/842, Loss: 0.4661, Time: 0.80s\n",
      "Epoch: 88, Batch: 371/842, Loss: 0.3354, Time: 0.75s\n",
      "Epoch: 88, Batch: 381/842, Loss: 0.3686, Time: 0.75s\n",
      "Epoch: 88, Batch: 391/842, Loss: 0.4072, Time: 0.78s\n",
      "Epoch: 88, Batch: 401/842, Loss: 0.4936, Time: 0.76s\n",
      "Epoch: 88, Batch: 411/842, Loss: 0.4034, Time: 0.76s\n",
      "Epoch: 88, Batch: 421/842, Loss: 0.4844, Time: 0.74s\n",
      "Epoch: 88, Batch: 431/842, Loss: 0.3951, Time: 0.78s\n",
      "Epoch: 88, Batch: 441/842, Loss: 0.3427, Time: 0.73s\n",
      "Epoch: 88, Batch: 451/842, Loss: 0.5206, Time: 0.73s\n",
      "Epoch: 88, Batch: 461/842, Loss: 0.6615, Time: 0.76s\n",
      "Epoch: 88, Batch: 471/842, Loss: 0.4769, Time: 0.73s\n",
      "Epoch: 88, Batch: 481/842, Loss: 0.4759, Time: 0.79s\n",
      "Epoch: 88, Batch: 491/842, Loss: 0.4039, Time: 0.78s\n",
      "Epoch: 88, Batch: 501/842, Loss: 0.5052, Time: 0.78s\n",
      "Epoch: 88, Batch: 511/842, Loss: 0.3717, Time: 0.79s\n",
      "Epoch: 88, Batch: 521/842, Loss: 0.4743, Time: 0.78s\n",
      "Epoch: 88, Batch: 531/842, Loss: 0.4594, Time: 0.79s\n",
      "Epoch: 88, Batch: 541/842, Loss: 0.5345, Time: 0.75s\n",
      "Epoch: 88, Batch: 551/842, Loss: 0.3272, Time: 0.77s\n",
      "Epoch: 88, Batch: 561/842, Loss: 0.4545, Time: 0.75s\n",
      "Epoch: 88, Batch: 571/842, Loss: 0.4482, Time: 0.76s\n",
      "Epoch: 88, Batch: 581/842, Loss: 0.3484, Time: 0.78s\n",
      "Epoch: 88, Batch: 591/842, Loss: 0.5460, Time: 0.78s\n",
      "Epoch: 88, Batch: 601/842, Loss: 0.4119, Time: 0.79s\n",
      "Epoch: 88, Batch: 611/842, Loss: 0.4441, Time: 0.79s\n",
      "Epoch: 88, Batch: 621/842, Loss: 0.5464, Time: 0.76s\n",
      "Epoch: 88, Batch: 631/842, Loss: 0.5333, Time: 0.83s\n",
      "Epoch: 88, Batch: 641/842, Loss: 0.3632, Time: 0.77s\n",
      "Epoch: 88, Batch: 651/842, Loss: 0.4553, Time: 0.83s\n",
      "Epoch: 88, Batch: 661/842, Loss: 0.3255, Time: 0.87s\n",
      "Epoch: 88, Batch: 671/842, Loss: 0.4432, Time: 0.87s\n",
      "Epoch: 88, Batch: 681/842, Loss: 0.6102, Time: 0.80s\n",
      "Epoch: 88, Batch: 691/842, Loss: 0.5851, Time: 0.72s\n",
      "Epoch: 88, Batch: 701/842, Loss: 0.3804, Time: 0.78s\n",
      "Epoch: 88, Batch: 711/842, Loss: 0.4550, Time: 0.76s\n",
      "Epoch: 88, Batch: 721/842, Loss: 0.5739, Time: 0.75s\n",
      "Epoch: 88, Batch: 731/842, Loss: 0.4114, Time: 0.75s\n",
      "Epoch: 88, Batch: 741/842, Loss: 0.4286, Time: 0.77s\n",
      "Epoch: 88, Batch: 751/842, Loss: 0.4555, Time: 0.75s\n",
      "Epoch: 88, Batch: 761/842, Loss: 0.5032, Time: 0.78s\n",
      "Epoch: 88, Batch: 771/842, Loss: 0.5184, Time: 0.75s\n",
      "Epoch: 88, Batch: 781/842, Loss: 0.4420, Time: 0.76s\n",
      "Epoch: 88, Batch: 791/842, Loss: 0.4557, Time: 0.75s\n",
      "Epoch: 88, Batch: 801/842, Loss: 0.4512, Time: 0.77s\n",
      "Epoch: 88, Batch: 811/842, Loss: 0.4948, Time: 0.72s\n",
      "Epoch: 88, Batch: 821/842, Loss: 0.4109, Time: 0.74s\n",
      "Epoch: 88, Batch: 831/842, Loss: 0.5640, Time: 0.73s\n",
      "Epoch: 88, Batch: 841/842, Loss: 0.4221, Time: 0.70s\n",
      "Epoch 88/100: Train Loss: 0.4527, Val Loss: 0.4176, mIoU: 0.7012, F1: 0.8184, OA: 0.9660\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7854, F1=0.8798, OA=0.8779, Precision=0.8370, Recall=0.9272\n",
      "    Class 1: IoU=0.7199, F1=0.8371, OA=0.9967, Precision=0.8710, Recall=0.8059\n",
      "    Class 2: IoU=0.9137, F1=0.9549, OA=0.9818, Precision=0.9566, Recall=0.9532\n",
      "    Class 3: IoU=0.4915, F1=0.6591, OA=0.9647, Precision=0.7993, Recall=0.5607\n",
      "    Class 4: IoU=0.7614, F1=0.8646, OA=0.9819, Precision=0.8848, Recall=0.8453\n",
      "    Class 5: IoU=0.6948, F1=0.8199, OA=0.9968, Precision=0.8261, Recall=0.8139\n",
      "    Class 6: IoU=0.5817, F1=0.7355, OA=0.9588, Precision=0.8295, Recall=0.6607\n",
      "    Class 7: IoU=0.6615, F1=0.7962, OA=0.9697, Precision=0.8629, Recall=0.7392\n",
      "Saving best model with mIoU: 0.7012\n",
      "Epoch: 89, Batch: 1/842, Loss: 0.5161, Time: 1.45s\n",
      "Epoch: 89, Batch: 11/842, Loss: 0.4350, Time: 0.81s\n",
      "Epoch: 89, Batch: 21/842, Loss: 0.5194, Time: 0.81s\n",
      "Epoch: 89, Batch: 31/842, Loss: 0.5437, Time: 0.84s\n",
      "Epoch: 89, Batch: 41/842, Loss: 0.4403, Time: 0.84s\n",
      "Epoch: 89, Batch: 51/842, Loss: 0.3957, Time: 0.78s\n",
      "Epoch: 89, Batch: 61/842, Loss: 0.4624, Time: 0.75s\n",
      "Epoch: 89, Batch: 71/842, Loss: 0.3615, Time: 0.77s\n",
      "Epoch: 89, Batch: 81/842, Loss: 0.6425, Time: 0.77s\n",
      "Epoch: 89, Batch: 91/842, Loss: 0.4893, Time: 0.78s\n",
      "Epoch: 89, Batch: 101/842, Loss: 0.3122, Time: 0.81s\n",
      "Epoch: 89, Batch: 111/842, Loss: 0.4003, Time: 0.79s\n",
      "Epoch: 89, Batch: 121/842, Loss: 0.4117, Time: 0.81s\n",
      "Epoch: 89, Batch: 131/842, Loss: 0.4469, Time: 0.79s\n",
      "Epoch: 89, Batch: 141/842, Loss: 0.4402, Time: 0.77s\n",
      "Epoch: 89, Batch: 151/842, Loss: 0.5184, Time: 0.82s\n",
      "Epoch: 89, Batch: 161/842, Loss: 0.3919, Time: 0.78s\n",
      "Epoch: 89, Batch: 171/842, Loss: 0.5414, Time: 0.76s\n",
      "Epoch: 89, Batch: 181/842, Loss: 0.4975, Time: 0.75s\n",
      "Epoch: 89, Batch: 191/842, Loss: 0.4520, Time: 0.80s\n",
      "Epoch: 89, Batch: 201/842, Loss: 0.4877, Time: 0.74s\n",
      "Epoch: 89, Batch: 211/842, Loss: 0.4658, Time: 0.76s\n",
      "Epoch: 89, Batch: 221/842, Loss: 0.4616, Time: 0.74s\n",
      "Epoch: 89, Batch: 231/842, Loss: 0.6138, Time: 0.83s\n",
      "Epoch: 89, Batch: 241/842, Loss: 0.5248, Time: 0.80s\n",
      "Epoch: 89, Batch: 251/842, Loss: 0.4403, Time: 0.77s\n",
      "Epoch: 89, Batch: 261/842, Loss: 0.3167, Time: 0.79s\n",
      "Epoch: 89, Batch: 271/842, Loss: 0.4097, Time: 0.78s\n",
      "Epoch: 89, Batch: 281/842, Loss: 0.4381, Time: 0.78s\n",
      "Epoch: 89, Batch: 291/842, Loss: 0.4979, Time: 0.78s\n",
      "Epoch: 89, Batch: 301/842, Loss: 0.5220, Time: 0.76s\n",
      "Epoch: 89, Batch: 311/842, Loss: 0.4259, Time: 0.80s\n",
      "Epoch: 89, Batch: 321/842, Loss: 0.6122, Time: 0.74s\n",
      "Epoch: 89, Batch: 331/842, Loss: 0.3656, Time: 0.74s\n",
      "Epoch: 89, Batch: 341/842, Loss: 0.5523, Time: 0.75s\n",
      "Epoch: 89, Batch: 351/842, Loss: 0.4950, Time: 0.77s\n",
      "Epoch: 89, Batch: 361/842, Loss: 0.4962, Time: 0.75s\n",
      "Epoch: 89, Batch: 371/842, Loss: 0.4678, Time: 0.78s\n",
      "Epoch: 89, Batch: 381/842, Loss: 0.4784, Time: 0.81s\n",
      "Epoch: 89, Batch: 391/842, Loss: 0.4902, Time: 0.80s\n",
      "Epoch: 89, Batch: 401/842, Loss: 0.4579, Time: 0.82s\n",
      "Epoch: 89, Batch: 411/842, Loss: 0.5085, Time: 0.83s\n",
      "Epoch: 89, Batch: 421/842, Loss: 0.5053, Time: 0.77s\n",
      "Epoch: 89, Batch: 431/842, Loss: 0.5162, Time: 0.81s\n",
      "Epoch: 89, Batch: 441/842, Loss: 0.6369, Time: 0.87s\n",
      "Epoch: 89, Batch: 451/842, Loss: 0.4200, Time: 0.85s\n",
      "Epoch: 89, Batch: 461/842, Loss: 0.4389, Time: 0.79s\n",
      "Epoch: 89, Batch: 471/842, Loss: 0.5452, Time: 0.86s\n",
      "Epoch: 89, Batch: 481/842, Loss: 0.4442, Time: 0.84s\n",
      "Epoch: 89, Batch: 491/842, Loss: 0.4527, Time: 0.81s\n",
      "Epoch: 89, Batch: 501/842, Loss: 0.4392, Time: 0.84s\n",
      "Epoch: 89, Batch: 511/842, Loss: 0.4731, Time: 0.78s\n",
      "Epoch: 89, Batch: 521/842, Loss: 0.4116, Time: 0.81s\n",
      "Epoch: 89, Batch: 531/842, Loss: 0.4278, Time: 0.75s\n",
      "Epoch: 89, Batch: 541/842, Loss: 0.3475, Time: 0.80s\n",
      "Epoch: 89, Batch: 551/842, Loss: 0.5558, Time: 0.86s\n",
      "Epoch: 89, Batch: 561/842, Loss: 0.4919, Time: 0.87s\n",
      "Epoch: 89, Batch: 571/842, Loss: 0.3538, Time: 0.87s\n",
      "Epoch: 89, Batch: 581/842, Loss: 0.5233, Time: 0.83s\n",
      "Epoch: 89, Batch: 591/842, Loss: 0.4641, Time: 0.84s\n",
      "Epoch: 89, Batch: 601/842, Loss: 0.4440, Time: 0.78s\n",
      "Epoch: 89, Batch: 611/842, Loss: 0.4585, Time: 0.79s\n",
      "Epoch: 89, Batch: 621/842, Loss: 0.4485, Time: 0.77s\n",
      "Epoch: 89, Batch: 631/842, Loss: 0.5451, Time: 0.79s\n",
      "Epoch: 89, Batch: 641/842, Loss: 0.4565, Time: 0.84s\n",
      "Epoch: 89, Batch: 651/842, Loss: 0.3946, Time: 0.84s\n",
      "Epoch: 89, Batch: 661/842, Loss: 0.5315, Time: 0.79s\n",
      "Epoch: 89, Batch: 671/842, Loss: 0.4508, Time: 0.75s\n",
      "Epoch: 89, Batch: 681/842, Loss: 0.5651, Time: 0.81s\n",
      "Epoch: 89, Batch: 691/842, Loss: 0.4099, Time: 0.86s\n",
      "Epoch: 89, Batch: 701/842, Loss: 0.5378, Time: 0.82s\n",
      "Epoch: 89, Batch: 711/842, Loss: 0.3913, Time: 0.79s\n",
      "Epoch: 89, Batch: 721/842, Loss: 0.4842, Time: 0.78s\n",
      "Epoch: 89, Batch: 731/842, Loss: 0.5144, Time: 0.81s\n",
      "Epoch: 89, Batch: 741/842, Loss: 0.6097, Time: 0.80s\n",
      "Epoch: 89, Batch: 751/842, Loss: 0.4977, Time: 0.77s\n",
      "Epoch: 89, Batch: 761/842, Loss: 0.4647, Time: 0.80s\n",
      "Epoch: 89, Batch: 771/842, Loss: 0.4422, Time: 0.77s\n",
      "Epoch: 89, Batch: 781/842, Loss: 0.3752, Time: 0.75s\n",
      "Epoch: 89, Batch: 791/842, Loss: 0.4116, Time: 0.76s\n",
      "Epoch: 89, Batch: 801/842, Loss: 0.3570, Time: 0.76s\n",
      "Epoch: 89, Batch: 811/842, Loss: 0.2514, Time: 0.76s\n",
      "Epoch: 89, Batch: 821/842, Loss: 0.5186, Time: 0.75s\n",
      "Epoch: 89, Batch: 831/842, Loss: 0.3929, Time: 0.74s\n",
      "Epoch: 89, Batch: 841/842, Loss: 0.4364, Time: 0.71s\n",
      "Epoch 89/100: Train Loss: 0.4560, Val Loss: 0.4180, mIoU: 0.6995, F1: 0.8172, OA: 0.9660\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7855, F1=0.8799, OA=0.8777, Precision=0.8358, Recall=0.9289\n",
      "    Class 1: IoU=0.7174, F1=0.8355, OA=0.9966, Precision=0.8709, Recall=0.8028\n",
      "    Class 2: IoU=0.9134, F1=0.9547, OA=0.9817, Precision=0.9596, Recall=0.9499\n",
      "    Class 3: IoU=0.4945, F1=0.6618, OA=0.9648, Precision=0.7964, Recall=0.5661\n",
      "    Class 4: IoU=0.7617, F1=0.8647, OA=0.9821, Precision=0.8906, Recall=0.8403\n",
      "    Class 5: IoU=0.6818, F1=0.8108, OA=0.9966, Precision=0.7979, Recall=0.8242\n",
      "    Class 6: IoU=0.5814, F1=0.7353, OA=0.9590, Precision=0.8354, Recall=0.6566\n",
      "    Class 7: IoU=0.6600, F1=0.7952, OA=0.9695, Precision=0.8596, Recall=0.7398\n",
      "Epoch: 90, Batch: 1/842, Loss: 0.3575, Time: 1.87s\n",
      "Epoch: 90, Batch: 11/842, Loss: 0.4487, Time: 0.80s\n",
      "Epoch: 90, Batch: 21/842, Loss: 0.4821, Time: 0.76s\n",
      "Epoch: 90, Batch: 31/842, Loss: 0.5423, Time: 0.78s\n",
      "Epoch: 90, Batch: 41/842, Loss: 0.4608, Time: 0.81s\n",
      "Epoch: 90, Batch: 51/842, Loss: 0.3051, Time: 0.80s\n",
      "Epoch: 90, Batch: 61/842, Loss: 0.4447, Time: 0.80s\n",
      "Epoch: 90, Batch: 71/842, Loss: 0.3821, Time: 0.76s\n",
      "Epoch: 90, Batch: 81/842, Loss: 0.4680, Time: 0.77s\n",
      "Epoch: 90, Batch: 91/842, Loss: 0.5272, Time: 0.80s\n",
      "Epoch: 90, Batch: 101/842, Loss: 0.4675, Time: 0.75s\n",
      "Epoch: 90, Batch: 111/842, Loss: 0.4650, Time: 0.76s\n",
      "Epoch: 90, Batch: 121/842, Loss: 0.4753, Time: 0.78s\n",
      "Epoch: 90, Batch: 131/842, Loss: 0.5309, Time: 0.76s\n",
      "Epoch: 90, Batch: 141/842, Loss: 0.5165, Time: 0.76s\n",
      "Epoch: 90, Batch: 151/842, Loss: 0.4533, Time: 0.75s\n",
      "Epoch: 90, Batch: 161/842, Loss: 0.4869, Time: 0.76s\n",
      "Epoch: 90, Batch: 171/842, Loss: 0.5137, Time: 0.79s\n",
      "Epoch: 90, Batch: 181/842, Loss: 0.3256, Time: 0.77s\n",
      "Epoch: 90, Batch: 191/842, Loss: 0.3915, Time: 0.74s\n",
      "Epoch: 90, Batch: 201/842, Loss: 0.5952, Time: 0.77s\n",
      "Epoch: 90, Batch: 211/842, Loss: 0.4931, Time: 0.79s\n",
      "Epoch: 90, Batch: 221/842, Loss: 0.5654, Time: 0.78s\n",
      "Epoch: 90, Batch: 231/842, Loss: 0.3914, Time: 0.78s\n",
      "Epoch: 90, Batch: 241/842, Loss: 0.4053, Time: 0.74s\n",
      "Epoch: 90, Batch: 251/842, Loss: 0.3696, Time: 0.76s\n",
      "Epoch: 90, Batch: 261/842, Loss: 0.4470, Time: 0.76s\n",
      "Epoch: 90, Batch: 271/842, Loss: 0.4246, Time: 0.80s\n",
      "Epoch: 90, Batch: 281/842, Loss: 0.4903, Time: 0.71s\n",
      "Epoch: 90, Batch: 291/842, Loss: 0.3561, Time: 0.75s\n",
      "Epoch: 90, Batch: 301/842, Loss: 0.4892, Time: 0.76s\n",
      "Epoch: 90, Batch: 311/842, Loss: 0.4229, Time: 0.77s\n",
      "Epoch: 90, Batch: 321/842, Loss: 0.3759, Time: 1.34s\n",
      "Epoch: 90, Batch: 331/842, Loss: 0.3900, Time: 0.80s\n",
      "Epoch: 90, Batch: 341/842, Loss: 0.5211, Time: 0.76s\n",
      "Epoch: 90, Batch: 351/842, Loss: 0.4038, Time: 0.77s\n",
      "Epoch: 90, Batch: 361/842, Loss: 0.5106, Time: 0.79s\n",
      "Epoch: 90, Batch: 371/842, Loss: 0.4714, Time: 0.78s\n",
      "Epoch: 90, Batch: 381/842, Loss: 0.5234, Time: 0.83s\n",
      "Epoch: 90, Batch: 391/842, Loss: 0.3648, Time: 0.79s\n",
      "Epoch: 90, Batch: 401/842, Loss: 0.4056, Time: 0.79s\n",
      "Epoch: 90, Batch: 411/842, Loss: 0.4553, Time: 0.81s\n",
      "Epoch: 90, Batch: 421/842, Loss: 0.3876, Time: 0.76s\n",
      "Epoch: 90, Batch: 431/842, Loss: 0.3169, Time: 0.76s\n",
      "Epoch: 90, Batch: 441/842, Loss: 0.5515, Time: 0.79s\n",
      "Epoch: 90, Batch: 451/842, Loss: 0.4001, Time: 0.74s\n",
      "Epoch: 90, Batch: 461/842, Loss: 0.6571, Time: 0.80s\n",
      "Epoch: 90, Batch: 471/842, Loss: 0.4352, Time: 0.83s\n",
      "Epoch: 90, Batch: 481/842, Loss: 0.4751, Time: 0.83s\n",
      "Epoch: 90, Batch: 491/842, Loss: 0.4414, Time: 0.80s\n",
      "Epoch: 90, Batch: 501/842, Loss: 0.4615, Time: 0.82s\n",
      "Epoch: 90, Batch: 511/842, Loss: 0.4788, Time: 0.75s\n",
      "Epoch: 90, Batch: 521/842, Loss: 0.4258, Time: 0.79s\n",
      "Epoch: 90, Batch: 531/842, Loss: 0.3995, Time: 0.77s\n",
      "Epoch: 90, Batch: 541/842, Loss: 0.5105, Time: 0.76s\n",
      "Epoch: 90, Batch: 551/842, Loss: 0.4802, Time: 0.76s\n",
      "Epoch: 90, Batch: 561/842, Loss: 0.4711, Time: 0.78s\n",
      "Epoch: 90, Batch: 571/842, Loss: 0.4109, Time: 0.78s\n",
      "Epoch: 90, Batch: 581/842, Loss: 0.3257, Time: 0.76s\n",
      "Epoch: 90, Batch: 591/842, Loss: 0.5017, Time: 0.84s\n",
      "Epoch: 90, Batch: 601/842, Loss: 0.4829, Time: 0.81s\n",
      "Epoch: 90, Batch: 611/842, Loss: 0.4452, Time: 0.76s\n",
      "Epoch: 90, Batch: 621/842, Loss: 0.5426, Time: 0.76s\n",
      "Epoch: 90, Batch: 631/842, Loss: 0.4959, Time: 0.77s\n",
      "Epoch: 90, Batch: 641/842, Loss: 0.4000, Time: 0.74s\n",
      "Epoch: 90, Batch: 651/842, Loss: 0.4640, Time: 0.75s\n",
      "Epoch: 90, Batch: 661/842, Loss: 0.4113, Time: 0.78s\n",
      "Epoch: 90, Batch: 671/842, Loss: 0.4552, Time: 0.81s\n",
      "Epoch: 90, Batch: 681/842, Loss: 0.3854, Time: 0.78s\n",
      "Epoch: 90, Batch: 691/842, Loss: 0.4104, Time: 0.75s\n",
      "Epoch: 90, Batch: 701/842, Loss: 0.3601, Time: 0.76s\n",
      "Epoch: 90, Batch: 711/842, Loss: 0.4327, Time: 0.77s\n",
      "Epoch: 90, Batch: 721/842, Loss: 0.3696, Time: 0.77s\n",
      "Epoch: 90, Batch: 731/842, Loss: 0.4397, Time: 0.77s\n",
      "Epoch: 90, Batch: 741/842, Loss: 0.4806, Time: 0.80s\n",
      "Epoch: 90, Batch: 751/842, Loss: 0.3568, Time: 0.77s\n",
      "Epoch: 90, Batch: 761/842, Loss: 0.5810, Time: 0.77s\n",
      "Epoch: 90, Batch: 771/842, Loss: 0.3259, Time: 0.73s\n",
      "Epoch: 90, Batch: 781/842, Loss: 0.3195, Time: 0.76s\n",
      "Epoch: 90, Batch: 791/842, Loss: 0.4588, Time: 0.77s\n",
      "Epoch: 90, Batch: 801/842, Loss: 0.5098, Time: 0.81s\n",
      "Epoch: 90, Batch: 811/842, Loss: 0.4643, Time: 0.85s\n",
      "Epoch: 90, Batch: 821/842, Loss: 0.5212, Time: 0.79s\n",
      "Epoch: 90, Batch: 831/842, Loss: 0.5986, Time: 0.76s\n",
      "Epoch: 90, Batch: 841/842, Loss: 0.3541, Time: 0.75s\n",
      "Epoch 90/100: Train Loss: 0.4498, Val Loss: 0.4215, mIoU: 0.6976, F1: 0.8159, OA: 0.9658\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7846, F1=0.8793, OA=0.8769, Precision=0.8339, Recall=0.9300\n",
      "    Class 1: IoU=0.7054, F1=0.8273, OA=0.9964, Precision=0.8534, Recall=0.8027\n",
      "    Class 2: IoU=0.9110, F1=0.9534, OA=0.9812, Precision=0.9588, Recall=0.9482\n",
      "    Class 3: IoU=0.4916, F1=0.6592, OA=0.9650, Precision=0.8063, Recall=0.5574\n",
      "    Class 4: IoU=0.7615, F1=0.8646, OA=0.9821, Precision=0.8918, Recall=0.8389\n",
      "    Class 5: IoU=0.6915, F1=0.8176, OA=0.9968, Precision=0.8220, Recall=0.8132\n",
      "    Class 6: IoU=0.5803, F1=0.7344, OA=0.9585, Precision=0.8252, Recall=0.6616\n",
      "    Class 7: IoU=0.6550, F1=0.7915, OA=0.9693, Precision=0.8651, Recall=0.7295\n",
      "Epoch: 91, Batch: 1/842, Loss: 0.5080, Time: 1.44s\n",
      "Epoch: 91, Batch: 11/842, Loss: 0.4357, Time: 0.84s\n",
      "Epoch: 91, Batch: 21/842, Loss: 0.4984, Time: 0.80s\n",
      "Epoch: 91, Batch: 31/842, Loss: 0.4006, Time: 0.79s\n",
      "Epoch: 91, Batch: 41/842, Loss: 0.4040, Time: 0.77s\n",
      "Epoch: 91, Batch: 51/842, Loss: 0.4499, Time: 0.76s\n",
      "Epoch: 91, Batch: 61/842, Loss: 0.5025, Time: 0.76s\n",
      "Epoch: 91, Batch: 71/842, Loss: 0.3988, Time: 0.76s\n",
      "Epoch: 91, Batch: 81/842, Loss: 0.5167, Time: 0.77s\n",
      "Epoch: 91, Batch: 91/842, Loss: 0.2257, Time: 0.78s\n",
      "Epoch: 91, Batch: 101/842, Loss: 0.4382, Time: 0.78s\n",
      "Epoch: 91, Batch: 111/842, Loss: 0.5009, Time: 0.76s\n",
      "Epoch: 91, Batch: 121/842, Loss: 0.4604, Time: 0.74s\n",
      "Epoch: 91, Batch: 131/842, Loss: 0.3584, Time: 0.79s\n",
      "Epoch: 91, Batch: 141/842, Loss: 0.4959, Time: 0.72s\n",
      "Epoch: 91, Batch: 151/842, Loss: 0.4659, Time: 0.74s\n",
      "Epoch: 91, Batch: 161/842, Loss: 0.6552, Time: 0.80s\n",
      "Epoch: 91, Batch: 171/842, Loss: 0.4455, Time: 0.80s\n",
      "Epoch: 91, Batch: 181/842, Loss: 0.4246, Time: 0.71s\n",
      "Epoch: 91, Batch: 191/842, Loss: 0.4870, Time: 0.74s\n",
      "Epoch: 91, Batch: 201/842, Loss: 0.4805, Time: 0.77s\n",
      "Epoch: 91, Batch: 211/842, Loss: 0.4882, Time: 0.75s\n",
      "Epoch: 91, Batch: 221/842, Loss: 0.4168, Time: 0.78s\n",
      "Epoch: 91, Batch: 231/842, Loss: 0.4363, Time: 0.81s\n",
      "Epoch: 91, Batch: 241/842, Loss: 0.4965, Time: 0.80s\n",
      "Epoch: 91, Batch: 251/842, Loss: 0.5355, Time: 0.79s\n",
      "Epoch: 91, Batch: 261/842, Loss: 0.4962, Time: 0.76s\n",
      "Epoch: 91, Batch: 271/842, Loss: 0.5491, Time: 0.77s\n",
      "Epoch: 91, Batch: 281/842, Loss: 0.4196, Time: 0.78s\n",
      "Epoch: 91, Batch: 291/842, Loss: 0.4424, Time: 0.79s\n",
      "Epoch: 91, Batch: 301/842, Loss: 0.4868, Time: 0.78s\n",
      "Epoch: 91, Batch: 311/842, Loss: 0.4017, Time: 0.77s\n",
      "Epoch: 91, Batch: 321/842, Loss: 0.5427, Time: 0.72s\n",
      "Epoch: 91, Batch: 331/842, Loss: 0.6042, Time: 0.76s\n",
      "Epoch: 91, Batch: 341/842, Loss: 0.5069, Time: 0.80s\n",
      "Epoch: 91, Batch: 351/842, Loss: 0.4968, Time: 0.78s\n",
      "Epoch: 91, Batch: 361/842, Loss: 0.3602, Time: 0.77s\n",
      "Epoch: 91, Batch: 371/842, Loss: 0.4601, Time: 0.75s\n",
      "Epoch: 91, Batch: 381/842, Loss: 0.4188, Time: 0.76s\n",
      "Epoch: 91, Batch: 391/842, Loss: 0.3594, Time: 0.73s\n",
      "Epoch: 91, Batch: 401/842, Loss: 0.5137, Time: 0.74s\n",
      "Epoch: 91, Batch: 411/842, Loss: 0.5672, Time: 0.82s\n",
      "Epoch: 91, Batch: 421/842, Loss: 0.4818, Time: 0.77s\n",
      "Epoch: 91, Batch: 431/842, Loss: 0.4150, Time: 0.82s\n",
      "Epoch: 91, Batch: 441/842, Loss: 0.3703, Time: 0.86s\n",
      "Epoch: 91, Batch: 451/842, Loss: 0.5317, Time: 0.82s\n",
      "Epoch: 91, Batch: 461/842, Loss: 0.3516, Time: 0.74s\n",
      "Epoch: 91, Batch: 471/842, Loss: 0.3906, Time: 0.76s\n",
      "Epoch: 91, Batch: 481/842, Loss: 0.3659, Time: 0.79s\n",
      "Epoch: 91, Batch: 491/842, Loss: 0.4940, Time: 0.76s\n",
      "Epoch: 91, Batch: 501/842, Loss: 0.3013, Time: 0.76s\n",
      "Epoch: 91, Batch: 511/842, Loss: 0.4903, Time: 0.75s\n",
      "Epoch: 91, Batch: 521/842, Loss: 0.4124, Time: 0.78s\n",
      "Epoch: 91, Batch: 531/842, Loss: 0.4377, Time: 0.77s\n",
      "Epoch: 91, Batch: 541/842, Loss: 0.4287, Time: 0.77s\n",
      "Epoch: 91, Batch: 551/842, Loss: 0.4180, Time: 0.77s\n",
      "Epoch: 91, Batch: 561/842, Loss: 0.4220, Time: 0.75s\n",
      "Epoch: 91, Batch: 571/842, Loss: 0.3385, Time: 0.75s\n",
      "Epoch: 91, Batch: 581/842, Loss: 0.4824, Time: 0.78s\n",
      "Epoch: 91, Batch: 591/842, Loss: 0.4727, Time: 0.77s\n",
      "Epoch: 91, Batch: 601/842, Loss: 0.6085, Time: 0.77s\n",
      "Epoch: 91, Batch: 611/842, Loss: 0.4166, Time: 0.77s\n",
      "Epoch: 91, Batch: 621/842, Loss: 0.3035, Time: 0.78s\n",
      "Epoch: 91, Batch: 631/842, Loss: 0.4151, Time: 0.78s\n",
      "Epoch: 91, Batch: 641/842, Loss: 0.3747, Time: 0.76s\n",
      "Epoch: 91, Batch: 651/842, Loss: 0.3347, Time: 0.77s\n",
      "Epoch: 91, Batch: 661/842, Loss: 0.4485, Time: 0.79s\n",
      "Epoch: 91, Batch: 671/842, Loss: 0.4599, Time: 0.92s\n",
      "Epoch: 91, Batch: 681/842, Loss: 0.3563, Time: 0.84s\n",
      "Epoch: 91, Batch: 691/842, Loss: 0.5726, Time: 0.82s\n",
      "Epoch: 91, Batch: 701/842, Loss: 0.6144, Time: 0.79s\n",
      "Epoch: 91, Batch: 711/842, Loss: 0.3443, Time: 0.79s\n",
      "Epoch: 91, Batch: 721/842, Loss: 0.4793, Time: 0.76s\n",
      "Epoch: 91, Batch: 731/842, Loss: 0.4509, Time: 0.74s\n",
      "Epoch: 91, Batch: 741/842, Loss: 0.4748, Time: 0.73s\n",
      "Epoch: 91, Batch: 751/842, Loss: 0.4347, Time: 0.80s\n",
      "Epoch: 91, Batch: 761/842, Loss: 0.4013, Time: 0.76s\n",
      "Epoch: 91, Batch: 771/842, Loss: 0.3743, Time: 0.74s\n",
      "Epoch: 91, Batch: 781/842, Loss: 0.5259, Time: 0.72s\n",
      "Epoch: 91, Batch: 791/842, Loss: 0.6089, Time: 0.80s\n",
      "Epoch: 91, Batch: 801/842, Loss: 0.2911, Time: 0.82s\n",
      "Epoch: 91, Batch: 811/842, Loss: 0.3631, Time: 0.76s\n",
      "Epoch: 91, Batch: 821/842, Loss: 0.3703, Time: 0.77s\n",
      "Epoch: 91, Batch: 831/842, Loss: 0.4906, Time: 0.72s\n",
      "Epoch: 91, Batch: 841/842, Loss: 0.4815, Time: 0.69s\n",
      "Epoch 91/100: Train Loss: 0.4457, Val Loss: 0.4202, mIoU: 0.6979, F1: 0.8162, OA: 0.9659\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7849, F1=0.8795, OA=0.8774, Precision=0.8357, Recall=0.9282\n",
      "    Class 1: IoU=0.7083, F1=0.8292, OA=0.9965, Precision=0.8518, Recall=0.8079\n",
      "    Class 2: IoU=0.9115, F1=0.9537, OA=0.9813, Precision=0.9588, Recall=0.9486\n",
      "    Class 3: IoU=0.4940, F1=0.6613, OA=0.9652, Precision=0.8108, Recall=0.5584\n",
      "    Class 4: IoU=0.7613, F1=0.8645, OA=0.9819, Precision=0.8820, Recall=0.8477\n",
      "    Class 5: IoU=0.6858, F1=0.8136, OA=0.9966, Precision=0.8056, Recall=0.8218\n",
      "    Class 6: IoU=0.5809, F1=0.7349, OA=0.9587, Precision=0.8285, Recall=0.6603\n",
      "    Class 7: IoU=0.6565, F1=0.7926, OA=0.9692, Precision=0.8604, Recall=0.7348\n",
      "Epoch: 92, Batch: 1/842, Loss: 0.4940, Time: 1.51s\n",
      "Epoch: 92, Batch: 11/842, Loss: 0.4335, Time: 0.75s\n",
      "Epoch: 92, Batch: 21/842, Loss: 0.4201, Time: 0.74s\n",
      "Epoch: 92, Batch: 31/842, Loss: 0.4993, Time: 0.78s\n",
      "Epoch: 92, Batch: 41/842, Loss: 0.5164, Time: 0.77s\n",
      "Epoch: 92, Batch: 51/842, Loss: 0.4239, Time: 0.74s\n",
      "Epoch: 92, Batch: 61/842, Loss: 0.4090, Time: 0.75s\n",
      "Epoch: 92, Batch: 71/842, Loss: 0.4903, Time: 0.76s\n",
      "Epoch: 92, Batch: 81/842, Loss: 0.5355, Time: 0.74s\n",
      "Epoch: 92, Batch: 91/842, Loss: 0.4283, Time: 0.79s\n",
      "Epoch: 92, Batch: 101/842, Loss: 0.4003, Time: 0.81s\n",
      "Epoch: 92, Batch: 111/842, Loss: 0.3732, Time: 0.79s\n",
      "Epoch: 92, Batch: 121/842, Loss: 0.5000, Time: 0.81s\n",
      "Epoch: 92, Batch: 131/842, Loss: 0.4585, Time: 0.80s\n",
      "Epoch: 92, Batch: 141/842, Loss: 0.5125, Time: 0.82s\n",
      "Epoch: 92, Batch: 151/842, Loss: 0.3378, Time: 0.81s\n",
      "Epoch: 92, Batch: 161/842, Loss: 0.4005, Time: 0.76s\n",
      "Epoch: 92, Batch: 171/842, Loss: 0.4150, Time: 0.77s\n",
      "Epoch: 92, Batch: 181/842, Loss: 0.4988, Time: 0.76s\n",
      "Epoch: 92, Batch: 191/842, Loss: 0.5406, Time: 0.83s\n",
      "Epoch: 92, Batch: 201/842, Loss: 0.3864, Time: 0.77s\n",
      "Epoch: 92, Batch: 211/842, Loss: 0.4923, Time: 0.77s\n",
      "Epoch: 92, Batch: 221/842, Loss: 0.4426, Time: 0.76s\n",
      "Epoch: 92, Batch: 231/842, Loss: 0.4391, Time: 0.77s\n",
      "Epoch: 92, Batch: 241/842, Loss: 0.4571, Time: 0.76s\n",
      "Epoch: 92, Batch: 251/842, Loss: 0.3794, Time: 0.92s\n",
      "Epoch: 92, Batch: 261/842, Loss: 0.4000, Time: 0.83s\n",
      "Epoch: 92, Batch: 271/842, Loss: 0.5495, Time: 0.83s\n",
      "Epoch: 92, Batch: 281/842, Loss: 0.4761, Time: 0.80s\n",
      "Epoch: 92, Batch: 291/842, Loss: 0.4927, Time: 0.78s\n",
      "Epoch: 92, Batch: 301/842, Loss: 0.3707, Time: 0.78s\n",
      "Epoch: 92, Batch: 311/842, Loss: 0.4114, Time: 0.76s\n",
      "Epoch: 92, Batch: 321/842, Loss: 0.3854, Time: 0.78s\n",
      "Epoch: 92, Batch: 331/842, Loss: 0.5379, Time: 0.80s\n",
      "Epoch: 92, Batch: 341/842, Loss: 0.6129, Time: 0.77s\n",
      "Epoch: 92, Batch: 351/842, Loss: 0.5449, Time: 0.77s\n",
      "Epoch: 92, Batch: 361/842, Loss: 0.3349, Time: 0.77s\n",
      "Epoch: 92, Batch: 371/842, Loss: 0.5221, Time: 0.75s\n",
      "Epoch: 92, Batch: 381/842, Loss: 0.4923, Time: 0.74s\n",
      "Epoch: 92, Batch: 391/842, Loss: 0.5897, Time: 0.75s\n",
      "Epoch: 92, Batch: 401/842, Loss: 0.5033, Time: 0.78s\n",
      "Epoch: 92, Batch: 411/842, Loss: 0.4459, Time: 0.77s\n",
      "Epoch: 92, Batch: 421/842, Loss: 0.6203, Time: 0.76s\n",
      "Epoch: 92, Batch: 431/842, Loss: 0.3913, Time: 0.81s\n",
      "Epoch: 92, Batch: 441/842, Loss: 0.5971, Time: 0.79s\n",
      "Epoch: 92, Batch: 451/842, Loss: 0.4512, Time: 0.80s\n",
      "Epoch: 92, Batch: 461/842, Loss: 0.4428, Time: 0.77s\n",
      "Epoch: 92, Batch: 471/842, Loss: 0.4558, Time: 0.76s\n",
      "Epoch: 92, Batch: 481/842, Loss: 0.3994, Time: 0.76s\n",
      "Epoch: 92, Batch: 491/842, Loss: 0.4198, Time: 0.77s\n",
      "Epoch: 92, Batch: 501/842, Loss: 0.4061, Time: 0.70s\n",
      "Epoch: 92, Batch: 511/842, Loss: 0.3904, Time: 0.73s\n",
      "Epoch: 92, Batch: 521/842, Loss: 0.4615, Time: 0.75s\n",
      "Epoch: 92, Batch: 531/842, Loss: 0.3089, Time: 0.81s\n",
      "Epoch: 92, Batch: 541/842, Loss: 0.4800, Time: 0.92s\n",
      "Epoch: 92, Batch: 551/842, Loss: 0.5354, Time: 0.82s\n",
      "Epoch: 92, Batch: 561/842, Loss: 0.3477, Time: 0.75s\n",
      "Epoch: 92, Batch: 571/842, Loss: 0.4319, Time: 0.79s\n",
      "Epoch: 92, Batch: 581/842, Loss: 0.3519, Time: 0.79s\n",
      "Epoch: 92, Batch: 591/842, Loss: 0.3117, Time: 0.79s\n",
      "Epoch: 92, Batch: 601/842, Loss: 0.4652, Time: 0.79s\n",
      "Epoch: 92, Batch: 611/842, Loss: 0.4038, Time: 0.78s\n",
      "Epoch: 92, Batch: 621/842, Loss: 0.5643, Time: 0.78s\n",
      "Epoch: 92, Batch: 631/842, Loss: 0.4062, Time: 0.79s\n",
      "Epoch: 92, Batch: 641/842, Loss: 0.3185, Time: 0.80s\n",
      "Epoch: 92, Batch: 651/842, Loss: 0.3356, Time: 0.82s\n",
      "Epoch: 92, Batch: 661/842, Loss: 0.5164, Time: 0.71s\n",
      "Epoch: 92, Batch: 671/842, Loss: 0.4973, Time: 0.75s\n",
      "Epoch: 92, Batch: 681/842, Loss: 0.5296, Time: 0.74s\n",
      "Epoch: 92, Batch: 691/842, Loss: 0.4135, Time: 0.76s\n",
      "Epoch: 92, Batch: 701/842, Loss: 0.5630, Time: 0.78s\n",
      "Epoch: 92, Batch: 711/842, Loss: 0.4073, Time: 0.82s\n",
      "Epoch: 92, Batch: 721/842, Loss: 0.3517, Time: 0.81s\n",
      "Epoch: 92, Batch: 731/842, Loss: 0.3686, Time: 0.81s\n",
      "Epoch: 92, Batch: 741/842, Loss: 0.4354, Time: 0.81s\n",
      "Epoch: 92, Batch: 751/842, Loss: 0.3553, Time: 0.84s\n",
      "Epoch: 92, Batch: 761/842, Loss: 0.2785, Time: 0.81s\n",
      "Epoch: 92, Batch: 771/842, Loss: 0.4352, Time: 0.80s\n",
      "Epoch: 92, Batch: 781/842, Loss: 0.4741, Time: 0.77s\n",
      "Epoch: 92, Batch: 791/842, Loss: 0.4895, Time: 0.84s\n",
      "Epoch: 92, Batch: 801/842, Loss: 0.3532, Time: 0.81s\n",
      "Epoch: 92, Batch: 811/842, Loss: 0.5062, Time: 0.77s\n",
      "Epoch: 92, Batch: 821/842, Loss: 0.3923, Time: 0.74s\n",
      "Epoch: 92, Batch: 831/842, Loss: 0.4241, Time: 0.74s\n",
      "Epoch: 92, Batch: 841/842, Loss: 0.3778, Time: 0.74s\n",
      "Epoch 92/100: Train Loss: 0.4494, Val Loss: 0.4201, mIoU: 0.6963, F1: 0.8149, OA: 0.9658\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7845, F1=0.8793, OA=0.8766, Precision=0.8322, Recall=0.9319\n",
      "    Class 1: IoU=0.7161, F1=0.8345, OA=0.9966, Precision=0.8656, Recall=0.8057\n",
      "    Class 2: IoU=0.9129, F1=0.9544, OA=0.9816, Precision=0.9576, Recall=0.9513\n",
      "    Class 3: IoU=0.4914, F1=0.6590, OA=0.9649, Precision=0.8034, Recall=0.5586\n",
      "    Class 4: IoU=0.7609, F1=0.8642, OA=0.9820, Precision=0.8909, Recall=0.8391\n",
      "    Class 5: IoU=0.6730, F1=0.8046, OA=0.9964, Precision=0.7859, Recall=0.8241\n",
      "    Class 6: IoU=0.5796, F1=0.7339, OA=0.9590, Precision=0.8389, Recall=0.6522\n",
      "    Class 7: IoU=0.6523, F1=0.7896, OA=0.9693, Precision=0.8757, Recall=0.7189\n",
      "Epoch: 93, Batch: 1/842, Loss: 0.3414, Time: 1.67s\n",
      "Epoch: 93, Batch: 11/842, Loss: 0.3225, Time: 0.74s\n",
      "Epoch: 93, Batch: 21/842, Loss: 0.3771, Time: 0.81s\n",
      "Epoch: 93, Batch: 31/842, Loss: 0.4482, Time: 0.76s\n",
      "Epoch: 93, Batch: 41/842, Loss: 0.5350, Time: 0.74s\n",
      "Epoch: 93, Batch: 51/842, Loss: 0.4020, Time: 0.72s\n",
      "Epoch: 93, Batch: 61/842, Loss: 0.4234, Time: 0.74s\n",
      "Epoch: 93, Batch: 71/842, Loss: 0.4356, Time: 0.78s\n",
      "Epoch: 93, Batch: 81/842, Loss: 0.4955, Time: 0.73s\n",
      "Epoch: 93, Batch: 91/842, Loss: 0.4230, Time: 0.75s\n",
      "Epoch: 93, Batch: 101/842, Loss: 0.4491, Time: 0.77s\n",
      "Epoch: 93, Batch: 111/842, Loss: 0.4476, Time: 0.84s\n",
      "Epoch: 93, Batch: 121/842, Loss: 0.4284, Time: 0.80s\n",
      "Epoch: 93, Batch: 131/842, Loss: 0.5024, Time: 0.78s\n",
      "Epoch: 93, Batch: 141/842, Loss: 0.5121, Time: 0.79s\n",
      "Epoch: 93, Batch: 151/842, Loss: 0.5217, Time: 0.78s\n",
      "Epoch: 93, Batch: 161/842, Loss: 0.4203, Time: 0.79s\n",
      "Epoch: 93, Batch: 171/842, Loss: 0.5145, Time: 0.84s\n",
      "Epoch: 93, Batch: 181/842, Loss: 0.4658, Time: 0.77s\n",
      "Epoch: 93, Batch: 191/842, Loss: 0.2834, Time: 0.76s\n",
      "Epoch: 93, Batch: 201/842, Loss: 0.5210, Time: 0.79s\n",
      "Epoch: 93, Batch: 211/842, Loss: 0.4706, Time: 0.74s\n",
      "Epoch: 93, Batch: 221/842, Loss: 0.4989, Time: 0.79s\n",
      "Epoch: 93, Batch: 231/842, Loss: 0.5006, Time: 0.80s\n",
      "Epoch: 93, Batch: 241/842, Loss: 0.4456, Time: 0.80s\n",
      "Epoch: 93, Batch: 251/842, Loss: 0.4278, Time: 0.84s\n",
      "Epoch: 93, Batch: 261/842, Loss: 0.3744, Time: 0.80s\n",
      "Epoch: 93, Batch: 271/842, Loss: 0.4149, Time: 0.79s\n",
      "Epoch: 93, Batch: 281/842, Loss: 0.4925, Time: 0.80s\n",
      "Epoch: 93, Batch: 291/842, Loss: 0.5044, Time: 0.83s\n",
      "Epoch: 93, Batch: 301/842, Loss: 0.4710, Time: 0.81s\n",
      "Epoch: 93, Batch: 311/842, Loss: 0.4225, Time: 0.79s\n",
      "Epoch: 93, Batch: 321/842, Loss: 0.4766, Time: 0.85s\n",
      "Epoch: 93, Batch: 331/842, Loss: 0.4157, Time: 0.80s\n",
      "Epoch: 93, Batch: 341/842, Loss: 0.4823, Time: 0.85s\n",
      "Epoch: 93, Batch: 351/842, Loss: 0.4826, Time: 0.77s\n",
      "Epoch: 93, Batch: 361/842, Loss: 0.5180, Time: 0.79s\n",
      "Epoch: 93, Batch: 371/842, Loss: 0.3638, Time: 0.84s\n",
      "Epoch: 93, Batch: 381/842, Loss: 0.5161, Time: 0.72s\n",
      "Epoch: 93, Batch: 391/842, Loss: 0.6339, Time: 0.77s\n",
      "Epoch: 93, Batch: 401/842, Loss: 0.4677, Time: 0.78s\n",
      "Epoch: 93, Batch: 411/842, Loss: 0.4351, Time: 0.83s\n",
      "Epoch: 93, Batch: 421/842, Loss: 0.4287, Time: 0.77s\n",
      "Epoch: 93, Batch: 431/842, Loss: 0.4917, Time: 0.75s\n",
      "Epoch: 93, Batch: 441/842, Loss: 0.4553, Time: 0.77s\n",
      "Epoch: 93, Batch: 451/842, Loss: 0.5485, Time: 0.76s\n",
      "Epoch: 93, Batch: 461/842, Loss: 0.5529, Time: 0.76s\n",
      "Epoch: 93, Batch: 471/842, Loss: 0.4154, Time: 0.78s\n",
      "Epoch: 93, Batch: 481/842, Loss: 0.4870, Time: 0.78s\n",
      "Epoch: 93, Batch: 491/842, Loss: 0.4918, Time: 0.75s\n",
      "Epoch: 93, Batch: 501/842, Loss: 0.5250, Time: 0.78s\n",
      "Epoch: 93, Batch: 511/842, Loss: 0.4366, Time: 0.81s\n",
      "Epoch: 93, Batch: 521/842, Loss: 0.3676, Time: 0.79s\n",
      "Epoch: 93, Batch: 531/842, Loss: 0.3834, Time: 0.77s\n",
      "Epoch: 93, Batch: 541/842, Loss: 0.4930, Time: 0.78s\n",
      "Epoch: 93, Batch: 551/842, Loss: 0.4709, Time: 0.76s\n",
      "Epoch: 93, Batch: 561/842, Loss: 0.4217, Time: 0.78s\n",
      "Epoch: 93, Batch: 571/842, Loss: 0.5211, Time: 0.81s\n",
      "Epoch: 93, Batch: 581/842, Loss: 0.4449, Time: 0.73s\n",
      "Epoch: 93, Batch: 591/842, Loss: 0.5249, Time: 0.74s\n",
      "Epoch: 93, Batch: 601/842, Loss: 0.3963, Time: 0.75s\n",
      "Epoch: 93, Batch: 611/842, Loss: 0.4296, Time: 0.76s\n",
      "Epoch: 93, Batch: 621/842, Loss: 0.3507, Time: 0.84s\n",
      "Epoch: 93, Batch: 631/842, Loss: 0.4477, Time: 0.86s\n",
      "Epoch: 93, Batch: 641/842, Loss: 0.4595, Time: 0.81s\n",
      "Epoch: 93, Batch: 651/842, Loss: 0.4050, Time: 0.75s\n",
      "Epoch: 93, Batch: 661/842, Loss: 0.3403, Time: 0.73s\n",
      "Epoch: 93, Batch: 671/842, Loss: 0.4581, Time: 0.75s\n",
      "Epoch: 93, Batch: 681/842, Loss: 0.3628, Time: 0.75s\n",
      "Epoch: 93, Batch: 691/842, Loss: 0.4622, Time: 0.79s\n",
      "Epoch: 93, Batch: 701/842, Loss: 0.4584, Time: 0.76s\n",
      "Epoch: 93, Batch: 711/842, Loss: 0.4702, Time: 1.36s\n",
      "Epoch: 93, Batch: 721/842, Loss: 0.6255, Time: 0.81s\n",
      "Epoch: 93, Batch: 731/842, Loss: 0.4284, Time: 0.79s\n",
      "Epoch: 93, Batch: 741/842, Loss: 0.5488, Time: 0.73s\n",
      "Epoch: 93, Batch: 751/842, Loss: 0.5563, Time: 0.75s\n",
      "Epoch: 93, Batch: 761/842, Loss: 0.4436, Time: 0.77s\n",
      "Epoch: 93, Batch: 771/842, Loss: 0.4060, Time: 0.79s\n",
      "Epoch: 93, Batch: 781/842, Loss: 0.4588, Time: 0.76s\n",
      "Epoch: 93, Batch: 791/842, Loss: 0.4666, Time: 0.74s\n",
      "Epoch: 93, Batch: 801/842, Loss: 0.4030, Time: 0.82s\n",
      "Epoch: 93, Batch: 811/842, Loss: 0.5758, Time: 0.80s\n",
      "Epoch: 93, Batch: 821/842, Loss: 0.6033, Time: 0.77s\n",
      "Epoch: 93, Batch: 831/842, Loss: 0.4617, Time: 0.81s\n",
      "Epoch: 93, Batch: 841/842, Loss: 0.4072, Time: 0.67s\n",
      "Epoch 93/100: Train Loss: 0.4564, Val Loss: 0.4191, mIoU: 0.6974, F1: 0.8158, OA: 0.9659\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7852, F1=0.8797, OA=0.8773, Precision=0.8342, Recall=0.9304\n",
      "    Class 1: IoU=0.7106, F1=0.8308, OA=0.9965, Precision=0.8572, Recall=0.8060\n",
      "    Class 2: IoU=0.9127, F1=0.9544, OA=0.9816, Precision=0.9596, Recall=0.9492\n",
      "    Class 3: IoU=0.4967, F1=0.6637, OA=0.9650, Precision=0.7981, Recall=0.5681\n",
      "    Class 4: IoU=0.7614, F1=0.8645, OA=0.9820, Precision=0.8865, Recall=0.8436\n",
      "    Class 5: IoU=0.6752, F1=0.8061, OA=0.9965, Precision=0.7881, Recall=0.8250\n",
      "    Class 6: IoU=0.5801, F1=0.7343, OA=0.9591, Precision=0.8408, Recall=0.6517\n",
      "    Class 7: IoU=0.6570, F1=0.7930, OA=0.9695, Precision=0.8689, Recall=0.7293\n",
      "Epoch: 94, Batch: 1/842, Loss: 0.3943, Time: 1.49s\n",
      "Epoch: 94, Batch: 11/842, Loss: 0.4778, Time: 0.73s\n",
      "Epoch: 94, Batch: 21/842, Loss: 0.4925, Time: 0.73s\n",
      "Epoch: 94, Batch: 31/842, Loss: 0.3918, Time: 0.90s\n",
      "Epoch: 94, Batch: 41/842, Loss: 0.4750, Time: 0.78s\n",
      "Epoch: 94, Batch: 51/842, Loss: 0.4024, Time: 0.80s\n",
      "Epoch: 94, Batch: 61/842, Loss: 0.3604, Time: 0.80s\n",
      "Epoch: 94, Batch: 71/842, Loss: 0.4795, Time: 0.75s\n",
      "Epoch: 94, Batch: 81/842, Loss: 0.3318, Time: 0.78s\n",
      "Epoch: 94, Batch: 91/842, Loss: 0.4343, Time: 0.76s\n",
      "Epoch: 94, Batch: 101/842, Loss: 0.3966, Time: 0.75s\n",
      "Epoch: 94, Batch: 111/842, Loss: 0.5578, Time: 0.80s\n",
      "Epoch: 94, Batch: 121/842, Loss: 0.3646, Time: 0.80s\n",
      "Epoch: 94, Batch: 131/842, Loss: 0.4305, Time: 0.82s\n",
      "Epoch: 94, Batch: 141/842, Loss: 0.5214, Time: 0.79s\n",
      "Epoch: 94, Batch: 151/842, Loss: 0.5398, Time: 0.84s\n",
      "Epoch: 94, Batch: 161/842, Loss: 0.4268, Time: 0.76s\n",
      "Epoch: 94, Batch: 171/842, Loss: 0.3020, Time: 0.75s\n",
      "Epoch: 94, Batch: 181/842, Loss: 0.3671, Time: 0.75s\n",
      "Epoch: 94, Batch: 191/842, Loss: 0.4262, Time: 0.79s\n",
      "Epoch: 94, Batch: 201/842, Loss: 0.4532, Time: 0.80s\n",
      "Epoch: 94, Batch: 211/842, Loss: 0.3539, Time: 0.77s\n",
      "Epoch: 94, Batch: 221/842, Loss: 0.3870, Time: 0.76s\n",
      "Epoch: 94, Batch: 231/842, Loss: 0.4901, Time: 0.77s\n",
      "Epoch: 94, Batch: 241/842, Loss: 0.4129, Time: 0.79s\n",
      "Epoch: 94, Batch: 251/842, Loss: 0.4946, Time: 0.75s\n",
      "Epoch: 94, Batch: 261/842, Loss: 0.5229, Time: 0.76s\n",
      "Epoch: 94, Batch: 271/842, Loss: 0.5090, Time: 0.75s\n",
      "Epoch: 94, Batch: 281/842, Loss: 0.4304, Time: 0.75s\n",
      "Epoch: 94, Batch: 291/842, Loss: 0.4135, Time: 0.73s\n",
      "Epoch: 94, Batch: 301/842, Loss: 0.5114, Time: 0.76s\n",
      "Epoch: 94, Batch: 311/842, Loss: 0.6042, Time: 0.75s\n",
      "Epoch: 94, Batch: 321/842, Loss: 0.4644, Time: 0.75s\n",
      "Epoch: 94, Batch: 331/842, Loss: 0.4984, Time: 0.72s\n",
      "Epoch: 94, Batch: 341/842, Loss: 0.3630, Time: 0.74s\n",
      "Epoch: 94, Batch: 351/842, Loss: 0.4669, Time: 0.76s\n",
      "Epoch: 94, Batch: 361/842, Loss: 0.4155, Time: 0.75s\n",
      "Epoch: 94, Batch: 371/842, Loss: 0.4778, Time: 0.76s\n",
      "Epoch: 94, Batch: 381/842, Loss: 0.2309, Time: 0.73s\n",
      "Epoch: 94, Batch: 391/842, Loss: 0.5419, Time: 0.74s\n",
      "Epoch: 94, Batch: 401/842, Loss: 0.3720, Time: 0.73s\n",
      "Epoch: 94, Batch: 411/842, Loss: 0.4586, Time: 0.74s\n",
      "Epoch: 94, Batch: 421/842, Loss: 0.4788, Time: 0.76s\n",
      "Epoch: 94, Batch: 431/842, Loss: 0.6126, Time: 0.74s\n",
      "Epoch: 94, Batch: 441/842, Loss: 0.5117, Time: 0.74s\n",
      "Epoch: 94, Batch: 451/842, Loss: 0.4047, Time: 0.74s\n",
      "Epoch: 94, Batch: 461/842, Loss: 0.3961, Time: 0.74s\n",
      "Epoch: 94, Batch: 471/842, Loss: 0.5076, Time: 0.78s\n",
      "Epoch: 94, Batch: 481/842, Loss: 0.5567, Time: 0.74s\n",
      "Epoch: 94, Batch: 491/842, Loss: 0.4643, Time: 0.74s\n",
      "Epoch: 94, Batch: 501/842, Loss: 0.3303, Time: 0.75s\n",
      "Epoch: 94, Batch: 511/842, Loss: 0.5362, Time: 0.78s\n",
      "Epoch: 94, Batch: 521/842, Loss: 0.3681, Time: 0.74s\n",
      "Epoch: 94, Batch: 531/842, Loss: 0.5410, Time: 0.77s\n",
      "Epoch: 94, Batch: 541/842, Loss: 0.5502, Time: 0.75s\n",
      "Epoch: 94, Batch: 551/842, Loss: 0.6106, Time: 0.74s\n",
      "Epoch: 94, Batch: 561/842, Loss: 0.5078, Time: 0.76s\n",
      "Epoch: 94, Batch: 571/842, Loss: 0.6105, Time: 0.76s\n",
      "Epoch: 94, Batch: 581/842, Loss: 0.4192, Time: 0.81s\n",
      "Epoch: 94, Batch: 591/842, Loss: 0.3277, Time: 0.76s\n",
      "Epoch: 94, Batch: 601/842, Loss: 0.3719, Time: 0.75s\n",
      "Epoch: 94, Batch: 611/842, Loss: 0.5903, Time: 0.76s\n",
      "Epoch: 94, Batch: 621/842, Loss: 0.3202, Time: 0.75s\n",
      "Epoch: 94, Batch: 631/842, Loss: 0.3928, Time: 0.74s\n",
      "Epoch: 94, Batch: 641/842, Loss: 0.4237, Time: 0.81s\n",
      "Epoch: 94, Batch: 651/842, Loss: 0.3552, Time: 0.78s\n",
      "Epoch: 94, Batch: 661/842, Loss: 0.5646, Time: 0.78s\n",
      "Epoch: 94, Batch: 671/842, Loss: 0.3194, Time: 0.82s\n",
      "Epoch: 94, Batch: 681/842, Loss: 0.4501, Time: 0.71s\n",
      "Epoch: 94, Batch: 691/842, Loss: 0.4107, Time: 0.74s\n",
      "Epoch: 94, Batch: 701/842, Loss: 0.5407, Time: 0.81s\n",
      "Epoch: 94, Batch: 711/842, Loss: 0.3354, Time: 0.79s\n",
      "Epoch: 94, Batch: 721/842, Loss: 0.3975, Time: 0.80s\n",
      "Epoch: 94, Batch: 731/842, Loss: 0.4214, Time: 0.80s\n",
      "Epoch: 94, Batch: 741/842, Loss: 0.4455, Time: 0.80s\n",
      "Epoch: 94, Batch: 751/842, Loss: 0.4739, Time: 0.80s\n",
      "Epoch: 94, Batch: 761/842, Loss: 0.3688, Time: 0.80s\n",
      "Epoch: 94, Batch: 771/842, Loss: 0.3621, Time: 0.76s\n",
      "Epoch: 94, Batch: 781/842, Loss: 0.4139, Time: 0.78s\n",
      "Epoch: 94, Batch: 791/842, Loss: 0.4054, Time: 0.78s\n",
      "Epoch: 94, Batch: 801/842, Loss: 0.4699, Time: 0.84s\n",
      "Epoch: 94, Batch: 811/842, Loss: 0.4204, Time: 0.79s\n",
      "Epoch: 94, Batch: 821/842, Loss: 0.4638, Time: 0.78s\n",
      "Epoch: 94, Batch: 831/842, Loss: 0.5987, Time: 0.78s\n",
      "Epoch: 94, Batch: 841/842, Loss: 0.5488, Time: 0.73s\n",
      "Epoch 94/100: Train Loss: 0.4482, Val Loss: 0.4188, mIoU: 0.6994, F1: 0.8172, OA: 0.9660\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7854, F1=0.8798, OA=0.8772, Precision=0.8333, Recall=0.9317\n",
      "    Class 1: IoU=0.7151, F1=0.8339, OA=0.9966, Precision=0.8665, Recall=0.8037\n",
      "    Class 2: IoU=0.9123, F1=0.9542, OA=0.9816, Precision=0.9616, Recall=0.9469\n",
      "    Class 3: IoU=0.4945, F1=0.6618, OA=0.9651, Precision=0.8069, Recall=0.5609\n",
      "    Class 4: IoU=0.7614, F1=0.8645, OA=0.9821, Precision=0.8939, Recall=0.8371\n",
      "    Class 5: IoU=0.6861, F1=0.8138, OA=0.9967, Precision=0.8100, Recall=0.8176\n",
      "    Class 6: IoU=0.5833, F1=0.7368, OA=0.9590, Precision=0.8317, Recall=0.6614\n",
      "    Class 7: IoU=0.6569, F1=0.7929, OA=0.9695, Precision=0.8666, Recall=0.7308\n",
      "Epoch: 95, Batch: 1/842, Loss: 0.3508, Time: 1.56s\n",
      "Epoch: 95, Batch: 11/842, Loss: 0.4271, Time: 0.78s\n",
      "Epoch: 95, Batch: 21/842, Loss: 0.4267, Time: 0.76s\n",
      "Epoch: 95, Batch: 31/842, Loss: 0.4082, Time: 0.78s\n",
      "Epoch: 95, Batch: 41/842, Loss: 0.4049, Time: 0.73s\n",
      "Epoch: 95, Batch: 51/842, Loss: 0.3882, Time: 0.82s\n",
      "Epoch: 95, Batch: 61/842, Loss: 0.5224, Time: 0.75s\n",
      "Epoch: 95, Batch: 71/842, Loss: 0.4910, Time: 0.75s\n",
      "Epoch: 95, Batch: 81/842, Loss: 0.3889, Time: 0.76s\n",
      "Epoch: 95, Batch: 91/842, Loss: 0.4261, Time: 0.75s\n",
      "Epoch: 95, Batch: 101/842, Loss: 0.5834, Time: 0.76s\n",
      "Epoch: 95, Batch: 111/842, Loss: 0.3969, Time: 0.77s\n",
      "Epoch: 95, Batch: 121/842, Loss: 0.5035, Time: 0.78s\n",
      "Epoch: 95, Batch: 131/842, Loss: 0.4704, Time: 0.80s\n",
      "Epoch: 95, Batch: 141/842, Loss: 0.4796, Time: 0.76s\n",
      "Epoch: 95, Batch: 151/842, Loss: 0.4027, Time: 0.82s\n",
      "Epoch: 95, Batch: 161/842, Loss: 0.4680, Time: 0.89s\n",
      "Epoch: 95, Batch: 171/842, Loss: 0.3102, Time: 0.74s\n",
      "Epoch: 95, Batch: 181/842, Loss: 0.3646, Time: 0.79s\n",
      "Epoch: 95, Batch: 191/842, Loss: 0.4584, Time: 0.79s\n",
      "Epoch: 95, Batch: 201/842, Loss: 0.5531, Time: 0.77s\n",
      "Epoch: 95, Batch: 211/842, Loss: 0.4154, Time: 0.77s\n",
      "Epoch: 95, Batch: 221/842, Loss: 0.3573, Time: 0.77s\n",
      "Epoch: 95, Batch: 231/842, Loss: 0.3029, Time: 0.76s\n",
      "Epoch: 95, Batch: 241/842, Loss: 0.3942, Time: 0.77s\n",
      "Epoch: 95, Batch: 251/842, Loss: 0.5348, Time: 0.79s\n",
      "Epoch: 95, Batch: 261/842, Loss: 0.4441, Time: 0.77s\n",
      "Epoch: 95, Batch: 271/842, Loss: 0.5126, Time: 0.77s\n",
      "Epoch: 95, Batch: 281/842, Loss: 0.4795, Time: 0.79s\n",
      "Epoch: 95, Batch: 291/842, Loss: 0.4377, Time: 0.79s\n",
      "Epoch: 95, Batch: 301/842, Loss: 0.4624, Time: 0.73s\n",
      "Epoch: 95, Batch: 311/842, Loss: 0.4736, Time: 0.79s\n",
      "Epoch: 95, Batch: 321/842, Loss: 0.5324, Time: 0.80s\n",
      "Epoch: 95, Batch: 331/842, Loss: 0.3840, Time: 0.80s\n",
      "Epoch: 95, Batch: 341/842, Loss: 0.3768, Time: 0.78s\n",
      "Epoch: 95, Batch: 351/842, Loss: 0.5651, Time: 0.81s\n",
      "Epoch: 95, Batch: 361/842, Loss: 0.4257, Time: 0.78s\n",
      "Epoch: 95, Batch: 371/842, Loss: 0.4138, Time: 0.78s\n",
      "Epoch: 95, Batch: 381/842, Loss: 0.5444, Time: 0.77s\n",
      "Epoch: 95, Batch: 391/842, Loss: 0.4416, Time: 0.76s\n",
      "Epoch: 95, Batch: 401/842, Loss: 0.5479, Time: 0.82s\n",
      "Epoch: 95, Batch: 411/842, Loss: 0.5666, Time: 0.79s\n",
      "Epoch: 95, Batch: 421/842, Loss: 0.5394, Time: 0.76s\n",
      "Epoch: 95, Batch: 431/842, Loss: 0.3523, Time: 0.75s\n",
      "Epoch: 95, Batch: 441/842, Loss: 0.5981, Time: 0.76s\n",
      "Epoch: 95, Batch: 451/842, Loss: 0.2625, Time: 0.74s\n",
      "Epoch: 95, Batch: 461/842, Loss: 0.2976, Time: 0.78s\n",
      "Epoch: 95, Batch: 471/842, Loss: 0.5042, Time: 0.80s\n",
      "Epoch: 95, Batch: 481/842, Loss: 0.3803, Time: 0.79s\n",
      "Epoch: 95, Batch: 491/842, Loss: 0.3756, Time: 0.77s\n",
      "Epoch: 95, Batch: 501/842, Loss: 0.3582, Time: 0.78s\n",
      "Epoch: 95, Batch: 511/842, Loss: 0.3670, Time: 0.76s\n",
      "Epoch: 95, Batch: 521/842, Loss: 0.4747, Time: 0.77s\n",
      "Epoch: 95, Batch: 531/842, Loss: 0.4204, Time: 0.81s\n",
      "Epoch: 95, Batch: 541/842, Loss: 0.4256, Time: 0.77s\n",
      "Epoch: 95, Batch: 551/842, Loss: 0.5726, Time: 0.77s\n",
      "Epoch: 95, Batch: 561/842, Loss: 0.3138, Time: 0.77s\n",
      "Epoch: 95, Batch: 571/842, Loss: 0.5106, Time: 0.76s\n",
      "Epoch: 95, Batch: 581/842, Loss: 0.4270, Time: 0.79s\n",
      "Epoch: 95, Batch: 591/842, Loss: 0.3691, Time: 0.76s\n",
      "Epoch: 95, Batch: 601/842, Loss: 0.4397, Time: 0.74s\n",
      "Epoch: 95, Batch: 611/842, Loss: 0.4527, Time: 0.78s\n",
      "Epoch: 95, Batch: 621/842, Loss: 0.5050, Time: 0.79s\n",
      "Epoch: 95, Batch: 631/842, Loss: 0.6135, Time: 0.76s\n",
      "Epoch: 95, Batch: 641/842, Loss: 0.3128, Time: 0.85s\n",
      "Epoch: 95, Batch: 651/842, Loss: 0.5464, Time: 0.84s\n",
      "Epoch: 95, Batch: 661/842, Loss: 0.4241, Time: 0.82s\n",
      "Epoch: 95, Batch: 671/842, Loss: 0.3497, Time: 0.76s\n",
      "Epoch: 95, Batch: 681/842, Loss: 0.4575, Time: 0.77s\n",
      "Epoch: 95, Batch: 691/842, Loss: 0.3537, Time: 0.75s\n",
      "Epoch: 95, Batch: 701/842, Loss: 0.3734, Time: 0.76s\n",
      "Epoch: 95, Batch: 711/842, Loss: 0.4529, Time: 0.77s\n",
      "Epoch: 95, Batch: 721/842, Loss: 0.5450, Time: 0.73s\n",
      "Epoch: 95, Batch: 731/842, Loss: 0.4568, Time: 0.77s\n",
      "Epoch: 95, Batch: 741/842, Loss: 0.3385, Time: 0.81s\n",
      "Epoch: 95, Batch: 751/842, Loss: 0.5480, Time: 0.77s\n",
      "Epoch: 95, Batch: 761/842, Loss: 0.3955, Time: 0.74s\n",
      "Epoch: 95, Batch: 771/842, Loss: 0.4833, Time: 0.71s\n",
      "Epoch: 95, Batch: 781/842, Loss: 0.3720, Time: 0.69s\n",
      "Epoch: 95, Batch: 791/842, Loss: 0.6047, Time: 0.71s\n",
      "Epoch: 95, Batch: 801/842, Loss: 0.4287, Time: 0.75s\n",
      "Epoch: 95, Batch: 811/842, Loss: 0.4158, Time: 0.76s\n",
      "Epoch: 95, Batch: 821/842, Loss: 0.3389, Time: 0.76s\n",
      "Epoch: 95, Batch: 831/842, Loss: 0.4760, Time: 0.76s\n",
      "Epoch: 95, Batch: 841/842, Loss: 0.3903, Time: 0.72s\n",
      "Epoch 95/100: Train Loss: 0.4463, Val Loss: 0.4159, mIoU: 0.7011, F1: 0.8184, OA: 0.9662\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7861, F1=0.8802, OA=0.8778, Precision=0.8343, Recall=0.9316\n",
      "    Class 1: IoU=0.7215, F1=0.8383, OA=0.9967, Precision=0.8725, Recall=0.8066\n",
      "    Class 2: IoU=0.9140, F1=0.9551, OA=0.9819, Precision=0.9589, Recall=0.9513\n",
      "    Class 3: IoU=0.4953, F1=0.6625, OA=0.9652, Precision=0.8066, Recall=0.5621\n",
      "    Class 4: IoU=0.7627, F1=0.8654, OA=0.9821, Precision=0.8866, Recall=0.8451\n",
      "    Class 5: IoU=0.6872, F1=0.8146, OA=0.9967, Precision=0.8076, Recall=0.8217\n",
      "    Class 6: IoU=0.5819, F1=0.7357, OA=0.9594, Precision=0.8453, Recall=0.6513\n",
      "    Class 7: IoU=0.6600, F1=0.7952, OA=0.9698, Precision=0.8683, Recall=0.7334\n",
      "Epoch: 96, Batch: 1/842, Loss: 0.3933, Time: 1.36s\n",
      "Epoch: 96, Batch: 11/842, Loss: 0.5334, Time: 0.77s\n",
      "Epoch: 96, Batch: 21/842, Loss: 0.4279, Time: 0.77s\n",
      "Epoch: 96, Batch: 31/842, Loss: 0.4504, Time: 0.79s\n",
      "Epoch: 96, Batch: 41/842, Loss: 0.5233, Time: 0.73s\n",
      "Epoch: 96, Batch: 51/842, Loss: 0.5100, Time: 0.75s\n",
      "Epoch: 96, Batch: 61/842, Loss: 0.3903, Time: 0.74s\n",
      "Epoch: 96, Batch: 71/842, Loss: 0.4173, Time: 0.75s\n",
      "Epoch: 96, Batch: 81/842, Loss: 0.3497, Time: 0.78s\n",
      "Epoch: 96, Batch: 91/842, Loss: 0.4214, Time: 0.81s\n",
      "Epoch: 96, Batch: 101/842, Loss: 0.5154, Time: 0.79s\n",
      "Epoch: 96, Batch: 111/842, Loss: 0.3866, Time: 0.78s\n",
      "Epoch: 96, Batch: 121/842, Loss: 0.5134, Time: 0.80s\n",
      "Epoch: 96, Batch: 131/842, Loss: 0.2947, Time: 0.79s\n",
      "Epoch: 96, Batch: 141/842, Loss: 0.3725, Time: 0.81s\n",
      "Epoch: 96, Batch: 151/842, Loss: 0.3922, Time: 0.79s\n",
      "Epoch: 96, Batch: 161/842, Loss: 0.4287, Time: 0.75s\n",
      "Epoch: 96, Batch: 171/842, Loss: 0.5262, Time: 0.74s\n",
      "Epoch: 96, Batch: 181/842, Loss: 0.4790, Time: 0.75s\n",
      "Epoch: 96, Batch: 191/842, Loss: 0.3384, Time: 0.75s\n",
      "Epoch: 96, Batch: 201/842, Loss: 0.5206, Time: 0.75s\n",
      "Epoch: 96, Batch: 211/842, Loss: 0.4104, Time: 0.73s\n",
      "Epoch: 96, Batch: 221/842, Loss: 0.4261, Time: 0.74s\n",
      "Epoch: 96, Batch: 231/842, Loss: 0.5405, Time: 0.80s\n",
      "Epoch: 96, Batch: 241/842, Loss: 0.4500, Time: 0.78s\n",
      "Epoch: 96, Batch: 251/842, Loss: 0.5495, Time: 0.75s\n",
      "Epoch: 96, Batch: 261/842, Loss: 0.3762, Time: 0.72s\n",
      "Epoch: 96, Batch: 271/842, Loss: 0.3492, Time: 0.75s\n",
      "Epoch: 96, Batch: 281/842, Loss: 0.4882, Time: 0.76s\n",
      "Epoch: 96, Batch: 291/842, Loss: 0.4715, Time: 0.79s\n",
      "Epoch: 96, Batch: 301/842, Loss: 0.4700, Time: 0.80s\n",
      "Epoch: 96, Batch: 311/842, Loss: 0.3059, Time: 0.80s\n",
      "Epoch: 96, Batch: 321/842, Loss: 0.5155, Time: 0.76s\n",
      "Epoch: 96, Batch: 331/842, Loss: 0.6121, Time: 0.78s\n",
      "Epoch: 96, Batch: 341/842, Loss: 0.4508, Time: 0.76s\n",
      "Epoch: 96, Batch: 351/842, Loss: 0.4099, Time: 0.76s\n",
      "Epoch: 96, Batch: 361/842, Loss: 0.3817, Time: 0.75s\n",
      "Epoch: 96, Batch: 371/842, Loss: 0.5798, Time: 0.75s\n",
      "Epoch: 96, Batch: 381/842, Loss: 0.6215, Time: 0.76s\n",
      "Epoch: 96, Batch: 391/842, Loss: 0.4544, Time: 0.78s\n",
      "Epoch: 96, Batch: 401/842, Loss: 0.4038, Time: 0.79s\n",
      "Epoch: 96, Batch: 411/842, Loss: 0.4307, Time: 0.80s\n",
      "Epoch: 96, Batch: 421/842, Loss: 0.5437, Time: 0.76s\n",
      "Epoch: 96, Batch: 431/842, Loss: 0.3650, Time: 0.79s\n",
      "Epoch: 96, Batch: 441/842, Loss: 0.4399, Time: 0.78s\n",
      "Epoch: 96, Batch: 451/842, Loss: 0.4357, Time: 0.78s\n",
      "Epoch: 96, Batch: 461/842, Loss: 0.5001, Time: 0.80s\n",
      "Epoch: 96, Batch: 471/842, Loss: 0.4199, Time: 0.81s\n",
      "Epoch: 96, Batch: 481/842, Loss: 0.4408, Time: 0.80s\n",
      "Epoch: 96, Batch: 491/842, Loss: 0.4843, Time: 0.78s\n",
      "Epoch: 96, Batch: 501/842, Loss: 0.4395, Time: 0.77s\n",
      "Epoch: 96, Batch: 511/842, Loss: 0.3274, Time: 0.75s\n",
      "Epoch: 96, Batch: 521/842, Loss: 0.3480, Time: 0.76s\n",
      "Epoch: 96, Batch: 531/842, Loss: 0.4249, Time: 0.77s\n",
      "Epoch: 96, Batch: 541/842, Loss: 0.3799, Time: 0.77s\n",
      "Epoch: 96, Batch: 551/842, Loss: 0.4062, Time: 0.80s\n",
      "Epoch: 96, Batch: 561/842, Loss: 0.3666, Time: 0.79s\n",
      "Epoch: 96, Batch: 571/842, Loss: 0.3616, Time: 0.78s\n",
      "Epoch: 96, Batch: 581/842, Loss: 0.5522, Time: 0.84s\n",
      "Epoch: 96, Batch: 591/842, Loss: 0.5046, Time: 0.78s\n",
      "Epoch: 96, Batch: 601/842, Loss: 0.4867, Time: 0.77s\n",
      "Epoch: 96, Batch: 611/842, Loss: 0.3178, Time: 0.75s\n",
      "Epoch: 96, Batch: 621/842, Loss: 0.3772, Time: 0.76s\n",
      "Epoch: 96, Batch: 631/842, Loss: 0.4134, Time: 0.77s\n",
      "Epoch: 96, Batch: 641/842, Loss: 0.3224, Time: 0.75s\n",
      "Epoch: 96, Batch: 651/842, Loss: 0.5170, Time: 0.75s\n",
      "Epoch: 96, Batch: 661/842, Loss: 0.4451, Time: 0.74s\n",
      "Epoch: 96, Batch: 671/842, Loss: 0.4792, Time: 0.75s\n",
      "Epoch: 96, Batch: 681/842, Loss: 0.4454, Time: 0.75s\n",
      "Epoch: 96, Batch: 691/842, Loss: 0.6214, Time: 0.78s\n",
      "Epoch: 96, Batch: 701/842, Loss: 0.4205, Time: 0.78s\n",
      "Epoch: 96, Batch: 711/842, Loss: 0.3781, Time: 0.78s\n",
      "Epoch: 96, Batch: 721/842, Loss: 0.4907, Time: 0.76s\n",
      "Epoch: 96, Batch: 731/842, Loss: 0.4082, Time: 0.78s\n",
      "Epoch: 96, Batch: 741/842, Loss: 0.4068, Time: 0.84s\n",
      "Epoch: 96, Batch: 751/842, Loss: 0.4659, Time: 0.83s\n",
      "Epoch: 96, Batch: 761/842, Loss: 0.6263, Time: 0.77s\n",
      "Epoch: 96, Batch: 771/842, Loss: 0.5458, Time: 0.79s\n",
      "Epoch: 96, Batch: 781/842, Loss: 0.6131, Time: 0.79s\n",
      "Epoch: 96, Batch: 791/842, Loss: 0.3965, Time: 0.81s\n",
      "Epoch: 96, Batch: 801/842, Loss: 0.5067, Time: 0.75s\n",
      "Epoch: 96, Batch: 811/842, Loss: 0.3498, Time: 0.77s\n",
      "Epoch: 96, Batch: 821/842, Loss: 0.4917, Time: 0.77s\n",
      "Epoch: 96, Batch: 831/842, Loss: 0.4613, Time: 0.72s\n",
      "Epoch: 96, Batch: 841/842, Loss: 0.5680, Time: 0.72s\n",
      "Epoch 96/100: Train Loss: 0.4482, Val Loss: 0.4184, mIoU: 0.7003, F1: 0.8179, OA: 0.9660\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7857, F1=0.8800, OA=0.8779, Precision=0.8363, Recall=0.9285\n",
      "    Class 1: IoU=0.7169, F1=0.8351, OA=0.9966, Precision=0.8670, Recall=0.8054\n",
      "    Class 2: IoU=0.9129, F1=0.9545, OA=0.9817, Precision=0.9603, Recall=0.9487\n",
      "    Class 3: IoU=0.4956, F1=0.6628, OA=0.9651, Precision=0.8048, Recall=0.5634\n",
      "    Class 4: IoU=0.7620, F1=0.8649, OA=0.9820, Precision=0.8870, Recall=0.8439\n",
      "    Class 5: IoU=0.6884, F1=0.8155, OA=0.9967, Precision=0.8098, Recall=0.8212\n",
      "    Class 6: IoU=0.5824, F1=0.7361, OA=0.9588, Precision=0.8273, Recall=0.6631\n",
      "    Class 7: IoU=0.6584, F1=0.7941, OA=0.9694, Precision=0.8594, Recall=0.7379\n",
      "Epoch: 97, Batch: 1/842, Loss: 0.4605, Time: 1.55s\n",
      "Epoch: 97, Batch: 11/842, Loss: 0.4142, Time: 0.85s\n",
      "Epoch: 97, Batch: 21/842, Loss: 0.5567, Time: 0.78s\n",
      "Epoch: 97, Batch: 31/842, Loss: 0.5202, Time: 0.74s\n",
      "Epoch: 97, Batch: 41/842, Loss: 0.5125, Time: 0.76s\n",
      "Epoch: 97, Batch: 51/842, Loss: 0.4503, Time: 0.80s\n",
      "Epoch: 97, Batch: 61/842, Loss: 0.3038, Time: 0.80s\n",
      "Epoch: 97, Batch: 71/842, Loss: 0.3909, Time: 0.82s\n",
      "Epoch: 97, Batch: 81/842, Loss: 0.5169, Time: 0.75s\n",
      "Epoch: 97, Batch: 91/842, Loss: 0.3613, Time: 0.75s\n",
      "Epoch: 97, Batch: 101/842, Loss: 0.5020, Time: 0.78s\n",
      "Epoch: 97, Batch: 111/842, Loss: 0.5710, Time: 0.79s\n",
      "Epoch: 97, Batch: 121/842, Loss: 0.4923, Time: 0.87s\n",
      "Epoch: 97, Batch: 131/842, Loss: 0.4463, Time: 0.91s\n",
      "Epoch: 97, Batch: 141/842, Loss: 0.5500, Time: 0.86s\n",
      "Epoch: 97, Batch: 151/842, Loss: 0.4280, Time: 0.81s\n",
      "Epoch: 97, Batch: 161/842, Loss: 0.4183, Time: 0.81s\n",
      "Epoch: 97, Batch: 171/842, Loss: 0.4469, Time: 0.89s\n",
      "Epoch: 97, Batch: 181/842, Loss: 0.5526, Time: 0.97s\n",
      "Epoch: 97, Batch: 191/842, Loss: 0.6115, Time: 0.91s\n",
      "Epoch: 97, Batch: 201/842, Loss: 0.4305, Time: 0.87s\n",
      "Epoch: 97, Batch: 211/842, Loss: 0.3877, Time: 0.83s\n",
      "Epoch: 97, Batch: 221/842, Loss: 0.4542, Time: 0.79s\n",
      "Epoch: 97, Batch: 231/842, Loss: 0.4729, Time: 0.80s\n",
      "Epoch: 97, Batch: 241/842, Loss: 0.3519, Time: 0.76s\n",
      "Epoch: 97, Batch: 251/842, Loss: 0.4123, Time: 0.82s\n",
      "Epoch: 97, Batch: 261/842, Loss: 0.3909, Time: 1.31s\n",
      "Epoch: 97, Batch: 271/842, Loss: 0.4068, Time: 0.79s\n",
      "Epoch: 97, Batch: 281/842, Loss: 0.3833, Time: 0.78s\n",
      "Epoch: 97, Batch: 291/842, Loss: 0.3969, Time: 0.76s\n",
      "Epoch: 97, Batch: 301/842, Loss: 0.5023, Time: 0.75s\n",
      "Epoch: 97, Batch: 311/842, Loss: 0.3996, Time: 0.77s\n",
      "Epoch: 97, Batch: 321/842, Loss: 0.5773, Time: 0.77s\n",
      "Epoch: 97, Batch: 331/842, Loss: 0.5552, Time: 0.76s\n",
      "Epoch: 97, Batch: 341/842, Loss: 0.4609, Time: 0.74s\n",
      "Epoch: 97, Batch: 351/842, Loss: 0.4608, Time: 0.75s\n",
      "Epoch: 97, Batch: 361/842, Loss: 0.5307, Time: 0.78s\n",
      "Epoch: 97, Batch: 371/842, Loss: 0.6081, Time: 0.86s\n",
      "Epoch: 97, Batch: 381/842, Loss: 0.5203, Time: 0.78s\n",
      "Epoch: 97, Batch: 391/842, Loss: 0.4592, Time: 0.75s\n",
      "Epoch: 97, Batch: 401/842, Loss: 0.4617, Time: 0.77s\n",
      "Epoch: 97, Batch: 411/842, Loss: 0.3986, Time: 0.79s\n",
      "Epoch: 97, Batch: 421/842, Loss: 0.4983, Time: 0.78s\n",
      "Epoch: 97, Batch: 431/842, Loss: 0.3616, Time: 0.77s\n",
      "Epoch: 97, Batch: 441/842, Loss: 0.5258, Time: 0.76s\n",
      "Epoch: 97, Batch: 451/842, Loss: 0.4991, Time: 0.77s\n",
      "Epoch: 97, Batch: 461/842, Loss: 0.5171, Time: 0.74s\n",
      "Epoch: 97, Batch: 471/842, Loss: 0.3118, Time: 0.73s\n",
      "Epoch: 97, Batch: 481/842, Loss: 0.5744, Time: 0.77s\n",
      "Epoch: 97, Batch: 491/842, Loss: 0.4673, Time: 0.87s\n",
      "Epoch: 97, Batch: 501/842, Loss: 0.3610, Time: 0.83s\n",
      "Epoch: 97, Batch: 511/842, Loss: 0.5145, Time: 0.84s\n",
      "Epoch: 97, Batch: 521/842, Loss: 0.5143, Time: 0.90s\n",
      "Epoch: 97, Batch: 531/842, Loss: 0.4463, Time: 0.86s\n",
      "Epoch: 97, Batch: 541/842, Loss: 0.4132, Time: 0.91s\n",
      "Epoch: 97, Batch: 551/842, Loss: 0.4634, Time: 0.85s\n",
      "Epoch: 97, Batch: 561/842, Loss: 0.5128, Time: 0.84s\n",
      "Epoch: 97, Batch: 571/842, Loss: 0.3650, Time: 0.82s\n",
      "Epoch: 97, Batch: 581/842, Loss: 0.4537, Time: 0.85s\n",
      "Epoch: 97, Batch: 591/842, Loss: 0.3734, Time: 0.80s\n",
      "Epoch: 97, Batch: 601/842, Loss: 0.4232, Time: 0.81s\n",
      "Epoch: 97, Batch: 611/842, Loss: 0.4069, Time: 0.86s\n",
      "Epoch: 97, Batch: 621/842, Loss: 0.4109, Time: 0.76s\n",
      "Epoch: 97, Batch: 631/842, Loss: 0.5707, Time: 0.76s\n",
      "Epoch: 97, Batch: 641/842, Loss: 0.4619, Time: 0.79s\n",
      "Epoch: 97, Batch: 651/842, Loss: 0.4163, Time: 0.78s\n",
      "Epoch: 97, Batch: 661/842, Loss: 0.4471, Time: 0.78s\n",
      "Epoch: 97, Batch: 671/842, Loss: 0.5100, Time: 0.82s\n",
      "Epoch: 97, Batch: 681/842, Loss: 0.4021, Time: 0.87s\n",
      "Epoch: 97, Batch: 691/842, Loss: 0.4495, Time: 0.78s\n",
      "Epoch: 97, Batch: 701/842, Loss: 0.3789, Time: 0.75s\n",
      "Epoch: 97, Batch: 711/842, Loss: 0.5472, Time: 0.76s\n",
      "Epoch: 97, Batch: 721/842, Loss: 0.3901, Time: 0.80s\n",
      "Epoch: 97, Batch: 731/842, Loss: 0.4413, Time: 0.78s\n",
      "Epoch: 97, Batch: 741/842, Loss: 0.3822, Time: 0.76s\n",
      "Epoch: 97, Batch: 751/842, Loss: 0.4540, Time: 0.77s\n",
      "Epoch: 97, Batch: 761/842, Loss: 0.4591, Time: 0.84s\n",
      "Epoch: 97, Batch: 771/842, Loss: 0.4684, Time: 0.80s\n",
      "Epoch: 97, Batch: 781/842, Loss: 0.4416, Time: 0.84s\n",
      "Epoch: 97, Batch: 791/842, Loss: 0.4546, Time: 0.79s\n",
      "Epoch: 97, Batch: 801/842, Loss: 0.3706, Time: 0.78s\n",
      "Epoch: 97, Batch: 811/842, Loss: 0.4200, Time: 0.81s\n",
      "Epoch: 97, Batch: 821/842, Loss: 0.3242, Time: 0.76s\n",
      "Epoch: 97, Batch: 831/842, Loss: 0.4345, Time: 0.77s\n",
      "Epoch: 97, Batch: 841/842, Loss: 0.4768, Time: 0.75s\n",
      "Epoch 97/100: Train Loss: 0.4559, Val Loss: 0.4162, mIoU: 0.7009, F1: 0.8183, OA: 0.9662\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7865, F1=0.8805, OA=0.8780, Precision=0.8345, Recall=0.9318\n",
      "    Class 1: IoU=0.7187, F1=0.8363, OA=0.9966, Precision=0.8658, Recall=0.8088\n",
      "    Class 2: IoU=0.9135, F1=0.9548, OA=0.9818, Precision=0.9610, Recall=0.9487\n",
      "    Class 3: IoU=0.4942, F1=0.6615, OA=0.9655, Precision=0.8190, Recall=0.5548\n",
      "    Class 4: IoU=0.7630, F1=0.8656, OA=0.9822, Precision=0.8944, Recall=0.8385\n",
      "    Class 5: IoU=0.6846, F1=0.8128, OA=0.9966, Precision=0.8013, Recall=0.8245\n",
      "    Class 6: IoU=0.5840, F1=0.7373, OA=0.9594, Precision=0.8385, Recall=0.6579\n",
      "    Class 7: IoU=0.6631, F1=0.7974, OA=0.9697, Precision=0.8588, Recall=0.7443\n",
      "Epoch: 98, Batch: 1/842, Loss: 0.5189, Time: 1.61s\n",
      "Epoch: 98, Batch: 11/842, Loss: 0.3949, Time: 0.75s\n",
      "Epoch: 98, Batch: 21/842, Loss: 0.4061, Time: 0.77s\n",
      "Epoch: 98, Batch: 31/842, Loss: 0.4764, Time: 0.76s\n",
      "Epoch: 98, Batch: 41/842, Loss: 0.4770, Time: 0.74s\n",
      "Epoch: 98, Batch: 51/842, Loss: 0.3893, Time: 0.73s\n",
      "Epoch: 98, Batch: 61/842, Loss: 0.3341, Time: 0.78s\n",
      "Epoch: 98, Batch: 71/842, Loss: 0.4613, Time: 0.81s\n",
      "Epoch: 98, Batch: 81/842, Loss: 0.4645, Time: 0.77s\n",
      "Epoch: 98, Batch: 91/842, Loss: 0.5222, Time: 0.76s\n",
      "Epoch: 98, Batch: 101/842, Loss: 0.4221, Time: 0.75s\n",
      "Epoch: 98, Batch: 111/842, Loss: 0.4315, Time: 0.80s\n",
      "Epoch: 98, Batch: 121/842, Loss: 0.4685, Time: 0.80s\n",
      "Epoch: 98, Batch: 131/842, Loss: 0.3441, Time: 0.80s\n",
      "Epoch: 98, Batch: 141/842, Loss: 0.4125, Time: 0.77s\n",
      "Epoch: 98, Batch: 151/842, Loss: 0.4679, Time: 0.78s\n",
      "Epoch: 98, Batch: 161/842, Loss: 0.4158, Time: 0.76s\n",
      "Epoch: 98, Batch: 171/842, Loss: 0.6266, Time: 0.76s\n",
      "Epoch: 98, Batch: 181/842, Loss: 0.4222, Time: 0.79s\n",
      "Epoch: 98, Batch: 191/842, Loss: 0.4502, Time: 0.77s\n",
      "Epoch: 98, Batch: 201/842, Loss: 0.3681, Time: 0.76s\n",
      "Epoch: 98, Batch: 211/842, Loss: 0.3854, Time: 0.81s\n",
      "Epoch: 98, Batch: 221/842, Loss: 0.2495, Time: 0.82s\n",
      "Epoch: 98, Batch: 231/842, Loss: 0.4628, Time: 0.80s\n",
      "Epoch: 98, Batch: 241/842, Loss: 0.5217, Time: 0.78s\n",
      "Epoch: 98, Batch: 251/842, Loss: 0.3702, Time: 0.76s\n",
      "Epoch: 98, Batch: 261/842, Loss: 0.4971, Time: 0.76s\n",
      "Epoch: 98, Batch: 271/842, Loss: 0.3662, Time: 0.81s\n",
      "Epoch: 98, Batch: 281/842, Loss: 0.4695, Time: 0.76s\n",
      "Epoch: 98, Batch: 291/842, Loss: 0.4564, Time: 0.77s\n",
      "Epoch: 98, Batch: 301/842, Loss: 0.4317, Time: 0.78s\n",
      "Epoch: 98, Batch: 311/842, Loss: 0.4664, Time: 0.76s\n",
      "Epoch: 98, Batch: 321/842, Loss: 0.3746, Time: 0.80s\n",
      "Epoch: 98, Batch: 331/842, Loss: 0.4020, Time: 0.83s\n",
      "Epoch: 98, Batch: 341/842, Loss: 0.3376, Time: 0.82s\n",
      "Epoch: 98, Batch: 351/842, Loss: 0.5545, Time: 0.84s\n",
      "Epoch: 98, Batch: 361/842, Loss: 0.4949, Time: 0.80s\n",
      "Epoch: 98, Batch: 371/842, Loss: 0.4210, Time: 0.78s\n",
      "Epoch: 98, Batch: 381/842, Loss: 0.4335, Time: 0.77s\n",
      "Epoch: 98, Batch: 391/842, Loss: 0.5316, Time: 0.76s\n",
      "Epoch: 98, Batch: 401/842, Loss: 0.4999, Time: 0.74s\n",
      "Epoch: 98, Batch: 411/842, Loss: 0.3105, Time: 0.76s\n",
      "Epoch: 98, Batch: 421/842, Loss: 0.4200, Time: 0.74s\n",
      "Epoch: 98, Batch: 431/842, Loss: 0.4038, Time: 0.75s\n",
      "Epoch: 98, Batch: 441/842, Loss: 0.3889, Time: 0.76s\n",
      "Epoch: 98, Batch: 451/842, Loss: 0.4102, Time: 0.74s\n",
      "Epoch: 98, Batch: 461/842, Loss: 0.4125, Time: 0.75s\n",
      "Epoch: 98, Batch: 471/842, Loss: 0.3385, Time: 0.69s\n",
      "Epoch: 98, Batch: 481/842, Loss: 0.5404, Time: 0.74s\n",
      "Epoch: 98, Batch: 491/842, Loss: 0.5369, Time: 0.81s\n",
      "Epoch: 98, Batch: 501/842, Loss: 0.4127, Time: 0.77s\n",
      "Epoch: 98, Batch: 511/842, Loss: 0.3865, Time: 0.79s\n",
      "Epoch: 98, Batch: 521/842, Loss: 0.5254, Time: 0.76s\n",
      "Epoch: 98, Batch: 531/842, Loss: 0.5036, Time: 0.71s\n",
      "Epoch: 98, Batch: 541/842, Loss: 0.3110, Time: 0.71s\n",
      "Epoch: 98, Batch: 551/842, Loss: 0.3937, Time: 0.78s\n",
      "Epoch: 98, Batch: 561/842, Loss: 0.3712, Time: 0.76s\n",
      "Epoch: 98, Batch: 571/842, Loss: 0.4501, Time: 0.76s\n",
      "Epoch: 98, Batch: 581/842, Loss: 0.4402, Time: 0.76s\n",
      "Epoch: 98, Batch: 591/842, Loss: 0.3980, Time: 0.74s\n",
      "Epoch: 98, Batch: 601/842, Loss: 0.4335, Time: 0.73s\n",
      "Epoch: 98, Batch: 611/842, Loss: 0.3985, Time: 0.79s\n",
      "Epoch: 98, Batch: 621/842, Loss: 0.3841, Time: 0.87s\n",
      "Epoch: 98, Batch: 631/842, Loss: 0.4232, Time: 0.80s\n",
      "Epoch: 98, Batch: 641/842, Loss: 0.4272, Time: 0.75s\n",
      "Epoch: 98, Batch: 651/842, Loss: 0.4762, Time: 0.80s\n",
      "Epoch: 98, Batch: 661/842, Loss: 0.3351, Time: 0.78s\n",
      "Epoch: 98, Batch: 671/842, Loss: 0.4173, Time: 0.76s\n",
      "Epoch: 98, Batch: 681/842, Loss: 0.4370, Time: 0.73s\n",
      "Epoch: 98, Batch: 691/842, Loss: 0.6065, Time: 0.78s\n",
      "Epoch: 98, Batch: 701/842, Loss: 0.4993, Time: 0.81s\n",
      "Epoch: 98, Batch: 711/842, Loss: 0.4485, Time: 0.82s\n",
      "Epoch: 98, Batch: 721/842, Loss: 0.4492, Time: 0.80s\n",
      "Epoch: 98, Batch: 731/842, Loss: 0.4388, Time: 0.82s\n",
      "Epoch: 98, Batch: 741/842, Loss: 0.5931, Time: 0.78s\n",
      "Epoch: 98, Batch: 751/842, Loss: 0.4962, Time: 0.71s\n",
      "Epoch: 98, Batch: 761/842, Loss: 0.3611, Time: 0.68s\n",
      "Epoch: 98, Batch: 771/842, Loss: 0.3446, Time: 0.75s\n",
      "Epoch: 98, Batch: 781/842, Loss: 0.4733, Time: 0.75s\n",
      "Epoch: 98, Batch: 791/842, Loss: 0.4670, Time: 0.77s\n",
      "Epoch: 98, Batch: 801/842, Loss: 0.4069, Time: 0.75s\n",
      "Epoch: 98, Batch: 811/842, Loss: 0.3679, Time: 0.77s\n",
      "Epoch: 98, Batch: 821/842, Loss: 0.4097, Time: 0.75s\n",
      "Epoch: 98, Batch: 831/842, Loss: 0.4066, Time: 0.70s\n",
      "Epoch: 98, Batch: 841/842, Loss: 0.4789, Time: 0.69s\n",
      "Epoch 98/100: Train Loss: 0.4435, Val Loss: 0.4171, mIoU: 0.7011, F1: 0.8183, OA: 0.9661\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7856, F1=0.8799, OA=0.8772, Precision=0.8323, Recall=0.9334\n",
      "    Class 1: IoU=0.7175, F1=0.8355, OA=0.9966, Precision=0.8681, Recall=0.8053\n",
      "    Class 2: IoU=0.9136, F1=0.9549, OA=0.9818, Precision=0.9602, Recall=0.9496\n",
      "    Class 3: IoU=0.4930, F1=0.6604, OA=0.9651, Precision=0.8072, Recall=0.5588\n",
      "    Class 4: IoU=0.7629, F1=0.8655, OA=0.9822, Precision=0.8928, Recall=0.8398\n",
      "    Class 5: IoU=0.6974, F1=0.8217, OA=0.9969, Precision=0.8300, Recall=0.8137\n",
      "    Class 6: IoU=0.5818, F1=0.7356, OA=0.9593, Precision=0.8414, Recall=0.6535\n",
      "    Class 7: IoU=0.6568, F1=0.7929, OA=0.9696, Precision=0.8722, Recall=0.7268\n",
      "Epoch: 99, Batch: 1/842, Loss: 0.3927, Time: 1.42s\n",
      "Epoch: 99, Batch: 11/842, Loss: 0.4127, Time: 0.77s\n",
      "Epoch: 99, Batch: 21/842, Loss: 0.4189, Time: 0.80s\n",
      "Epoch: 99, Batch: 31/842, Loss: 0.4684, Time: 0.77s\n",
      "Epoch: 99, Batch: 41/842, Loss: 0.4148, Time: 0.76s\n",
      "Epoch: 99, Batch: 51/842, Loss: 0.4217, Time: 0.75s\n",
      "Epoch: 99, Batch: 61/842, Loss: 0.3819, Time: 0.77s\n",
      "Epoch: 99, Batch: 71/842, Loss: 0.3917, Time: 0.76s\n",
      "Epoch: 99, Batch: 81/842, Loss: 0.3368, Time: 0.74s\n",
      "Epoch: 99, Batch: 91/842, Loss: 0.5116, Time: 0.75s\n",
      "Epoch: 99, Batch: 101/842, Loss: 0.4142, Time: 0.76s\n",
      "Epoch: 99, Batch: 111/842, Loss: 0.5424, Time: 0.79s\n",
      "Epoch: 99, Batch: 121/842, Loss: 0.3705, Time: 0.82s\n",
      "Epoch: 99, Batch: 131/842, Loss: 0.3929, Time: 0.85s\n",
      "Epoch: 99, Batch: 141/842, Loss: 0.4495, Time: 0.84s\n",
      "Epoch: 99, Batch: 151/842, Loss: 0.3705, Time: 0.78s\n",
      "Epoch: 99, Batch: 161/842, Loss: 0.4931, Time: 0.73s\n",
      "Epoch: 99, Batch: 171/842, Loss: 0.3818, Time: 0.76s\n",
      "Epoch: 99, Batch: 181/842, Loss: 0.3557, Time: 0.81s\n",
      "Epoch: 99, Batch: 191/842, Loss: 0.4996, Time: 0.81s\n",
      "Epoch: 99, Batch: 201/842, Loss: 0.4174, Time: 0.76s\n",
      "Epoch: 99, Batch: 211/842, Loss: 0.4329, Time: 0.81s\n",
      "Epoch: 99, Batch: 221/842, Loss: 0.5393, Time: 0.80s\n",
      "Epoch: 99, Batch: 231/842, Loss: 0.4837, Time: 0.85s\n",
      "Epoch: 99, Batch: 241/842, Loss: 0.4154, Time: 0.80s\n",
      "Epoch: 99, Batch: 251/842, Loss: 0.3616, Time: 0.75s\n",
      "Epoch: 99, Batch: 261/842, Loss: 0.3687, Time: 0.73s\n",
      "Epoch: 99, Batch: 271/842, Loss: 0.3375, Time: 0.77s\n",
      "Epoch: 99, Batch: 281/842, Loss: 0.4924, Time: 0.77s\n",
      "Epoch: 99, Batch: 291/842, Loss: 0.5380, Time: 0.79s\n",
      "Epoch: 99, Batch: 301/842, Loss: 0.4837, Time: 0.75s\n",
      "Epoch: 99, Batch: 311/842, Loss: 0.4674, Time: 0.76s\n",
      "Epoch: 99, Batch: 321/842, Loss: 0.2725, Time: 0.74s\n",
      "Epoch: 99, Batch: 331/842, Loss: 0.3633, Time: 0.76s\n",
      "Epoch: 99, Batch: 341/842, Loss: 0.6678, Time: 0.80s\n",
      "Epoch: 99, Batch: 351/842, Loss: 0.5126, Time: 0.76s\n",
      "Epoch: 99, Batch: 361/842, Loss: 0.4084, Time: 0.80s\n",
      "Epoch: 99, Batch: 371/842, Loss: 0.4765, Time: 0.73s\n",
      "Epoch: 99, Batch: 381/842, Loss: 0.4833, Time: 0.74s\n",
      "Epoch: 99, Batch: 391/842, Loss: 0.2771, Time: 0.76s\n",
      "Epoch: 99, Batch: 401/842, Loss: 0.5114, Time: 0.75s\n",
      "Epoch: 99, Batch: 411/842, Loss: 0.5587, Time: 0.78s\n",
      "Epoch: 99, Batch: 421/842, Loss: 0.4569, Time: 0.83s\n",
      "Epoch: 99, Batch: 431/842, Loss: 0.3856, Time: 0.84s\n",
      "Epoch: 99, Batch: 441/842, Loss: 0.4794, Time: 0.89s\n",
      "Epoch: 99, Batch: 451/842, Loss: 0.3507, Time: 0.85s\n",
      "Epoch: 99, Batch: 461/842, Loss: 0.5868, Time: 0.81s\n",
      "Epoch: 99, Batch: 471/842, Loss: 0.3754, Time: 0.75s\n",
      "Epoch: 99, Batch: 481/842, Loss: 0.4386, Time: 0.76s\n",
      "Epoch: 99, Batch: 491/842, Loss: 0.4477, Time: 0.75s\n",
      "Epoch: 99, Batch: 501/842, Loss: 0.4550, Time: 0.78s\n",
      "Epoch: 99, Batch: 511/842, Loss: 0.4394, Time: 0.78s\n",
      "Epoch: 99, Batch: 521/842, Loss: 0.2994, Time: 0.75s\n",
      "Epoch: 99, Batch: 531/842, Loss: 0.4656, Time: 0.76s\n",
      "Epoch: 99, Batch: 541/842, Loss: 0.4860, Time: 0.79s\n",
      "Epoch: 99, Batch: 551/842, Loss: 0.4797, Time: 0.76s\n",
      "Epoch: 99, Batch: 561/842, Loss: 0.3348, Time: 0.76s\n",
      "Epoch: 99, Batch: 571/842, Loss: 0.4660, Time: 0.80s\n",
      "Epoch: 99, Batch: 581/842, Loss: 0.3690, Time: 0.76s\n",
      "Epoch: 99, Batch: 591/842, Loss: 0.5236, Time: 0.71s\n",
      "Epoch: 99, Batch: 601/842, Loss: 0.3893, Time: 0.74s\n",
      "Epoch: 99, Batch: 611/842, Loss: 0.5134, Time: 0.73s\n",
      "Epoch: 99, Batch: 621/842, Loss: 0.4482, Time: 0.73s\n",
      "Epoch: 99, Batch: 631/842, Loss: 0.4899, Time: 0.67s\n",
      "Epoch: 99, Batch: 641/842, Loss: 0.3180, Time: 0.69s\n",
      "Epoch: 99, Batch: 651/842, Loss: 0.4992, Time: 0.73s\n",
      "Epoch: 99, Batch: 661/842, Loss: 0.5191, Time: 0.74s\n",
      "Epoch: 99, Batch: 671/842, Loss: 0.5149, Time: 0.71s\n",
      "Epoch: 99, Batch: 681/842, Loss: 0.4764, Time: 0.76s\n",
      "Epoch: 99, Batch: 691/842, Loss: 0.4427, Time: 0.76s\n",
      "Epoch: 99, Batch: 701/842, Loss: 0.4043, Time: 0.76s\n",
      "Epoch: 99, Batch: 711/842, Loss: 0.5101, Time: 0.75s\n",
      "Epoch: 99, Batch: 721/842, Loss: 0.5392, Time: 0.77s\n",
      "Epoch: 99, Batch: 731/842, Loss: 0.4661, Time: 0.76s\n",
      "Epoch: 99, Batch: 741/842, Loss: 0.4821, Time: 0.71s\n",
      "Epoch: 99, Batch: 751/842, Loss: 0.4927, Time: 0.75s\n",
      "Epoch: 99, Batch: 761/842, Loss: 0.3461, Time: 0.78s\n",
      "Epoch: 99, Batch: 771/842, Loss: 0.4570, Time: 0.74s\n",
      "Epoch: 99, Batch: 781/842, Loss: 0.4536, Time: 0.75s\n",
      "Epoch: 99, Batch: 791/842, Loss: 0.4871, Time: 0.77s\n",
      "Epoch: 99, Batch: 801/842, Loss: 0.4763, Time: 0.82s\n",
      "Epoch: 99, Batch: 811/842, Loss: 0.4298, Time: 0.79s\n",
      "Epoch: 99, Batch: 821/842, Loss: 0.4811, Time: 0.80s\n",
      "Epoch: 99, Batch: 831/842, Loss: 0.4094, Time: 0.74s\n",
      "Epoch: 99, Batch: 841/842, Loss: 0.4379, Time: 0.73s\n",
      "Epoch 99/100: Train Loss: 0.4506, Val Loss: 0.4169, mIoU: 0.7012, F1: 0.8184, OA: 0.9661\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7859, F1=0.8801, OA=0.8777, Precision=0.8345, Recall=0.9310\n",
      "    Class 1: IoU=0.7171, F1=0.8353, OA=0.9966, Precision=0.8676, Recall=0.8053\n",
      "    Class 2: IoU=0.9134, F1=0.9547, OA=0.9818, Precision=0.9605, Recall=0.9491\n",
      "    Class 3: IoU=0.4953, F1=0.6625, OA=0.9651, Precision=0.8054, Recall=0.5626\n",
      "    Class 4: IoU=0.7628, F1=0.8654, OA=0.9821, Precision=0.8875, Recall=0.8444\n",
      "    Class 5: IoU=0.6942, F1=0.8195, OA=0.9968, Precision=0.8212, Recall=0.8178\n",
      "    Class 6: IoU=0.5827, F1=0.7364, OA=0.9592, Precision=0.8360, Recall=0.6580\n",
      "    Class 7: IoU=0.6580, F1=0.7937, OA=0.9696, Precision=0.8666, Recall=0.7321\n",
      "Epoch: 100, Batch: 1/842, Loss: 0.4008, Time: 1.57s\n",
      "Epoch: 100, Batch: 11/842, Loss: 0.4971, Time: 0.76s\n",
      "Epoch: 100, Batch: 21/842, Loss: 0.4912, Time: 0.85s\n",
      "Epoch: 100, Batch: 31/842, Loss: 0.4172, Time: 0.81s\n",
      "Epoch: 100, Batch: 41/842, Loss: 0.4103, Time: 0.75s\n",
      "Epoch: 100, Batch: 51/842, Loss: 0.3657, Time: 0.78s\n",
      "Epoch: 100, Batch: 61/842, Loss: 0.3688, Time: 0.78s\n",
      "Epoch: 100, Batch: 71/842, Loss: 0.4423, Time: 0.86s\n",
      "Epoch: 100, Batch: 81/842, Loss: 0.4575, Time: 0.86s\n",
      "Epoch: 100, Batch: 91/842, Loss: 0.5468, Time: 0.80s\n",
      "Epoch: 100, Batch: 101/842, Loss: 0.5601, Time: 0.77s\n",
      "Epoch: 100, Batch: 111/842, Loss: 0.5328, Time: 0.79s\n",
      "Epoch: 100, Batch: 121/842, Loss: 0.4973, Time: 0.82s\n",
      "Epoch: 100, Batch: 131/842, Loss: 0.3970, Time: 0.80s\n",
      "Epoch: 100, Batch: 141/842, Loss: 0.3839, Time: 0.78s\n",
      "Epoch: 100, Batch: 151/842, Loss: 0.4425, Time: 0.79s\n",
      "Epoch: 100, Batch: 161/842, Loss: 0.4201, Time: 0.74s\n",
      "Epoch: 100, Batch: 171/842, Loss: 0.4177, Time: 0.75s\n",
      "Epoch: 100, Batch: 181/842, Loss: 0.4699, Time: 0.74s\n",
      "Epoch: 100, Batch: 191/842, Loss: 0.5593, Time: 0.73s\n",
      "Epoch: 100, Batch: 201/842, Loss: 0.4458, Time: 0.75s\n",
      "Epoch: 100, Batch: 211/842, Loss: 0.4412, Time: 0.76s\n",
      "Epoch: 100, Batch: 221/842, Loss: 0.3799, Time: 0.71s\n",
      "Epoch: 100, Batch: 231/842, Loss: 0.4805, Time: 0.74s\n",
      "Epoch: 100, Batch: 241/842, Loss: 0.4972, Time: 0.77s\n",
      "Epoch: 100, Batch: 251/842, Loss: 0.4153, Time: 0.80s\n",
      "Epoch: 100, Batch: 261/842, Loss: 0.3235, Time: 0.74s\n",
      "Epoch: 100, Batch: 271/842, Loss: 0.3608, Time: 0.76s\n",
      "Epoch: 100, Batch: 281/842, Loss: 0.4143, Time: 0.79s\n",
      "Epoch: 100, Batch: 291/842, Loss: 0.3536, Time: 0.76s\n",
      "Epoch: 100, Batch: 301/842, Loss: 0.5870, Time: 0.78s\n",
      "Epoch: 100, Batch: 311/842, Loss: 0.3663, Time: 0.78s\n",
      "Epoch: 100, Batch: 321/842, Loss: 0.5656, Time: 0.76s\n",
      "Epoch: 100, Batch: 331/842, Loss: 0.4633, Time: 0.82s\n",
      "Epoch: 100, Batch: 341/842, Loss: 0.3494, Time: 0.82s\n",
      "Epoch: 100, Batch: 351/842, Loss: 0.4597, Time: 0.82s\n",
      "Epoch: 100, Batch: 361/842, Loss: 0.4182, Time: 0.78s\n",
      "Epoch: 100, Batch: 371/842, Loss: 0.3900, Time: 0.76s\n",
      "Epoch: 100, Batch: 381/842, Loss: 0.4198, Time: 0.79s\n",
      "Epoch: 100, Batch: 391/842, Loss: 0.5269, Time: 0.83s\n",
      "Epoch: 100, Batch: 401/842, Loss: 0.4184, Time: 0.78s\n",
      "Epoch: 100, Batch: 411/842, Loss: 0.5694, Time: 0.78s\n",
      "Epoch: 100, Batch: 421/842, Loss: 0.5400, Time: 0.78s\n",
      "Epoch: 100, Batch: 431/842, Loss: 0.5212, Time: 0.75s\n",
      "Epoch: 100, Batch: 441/842, Loss: 0.3306, Time: 0.77s\n",
      "Epoch: 100, Batch: 451/842, Loss: 0.3748, Time: 0.78s\n",
      "Epoch: 100, Batch: 461/842, Loss: 0.4920, Time: 0.79s\n",
      "Epoch: 100, Batch: 471/842, Loss: 0.5429, Time: 0.78s\n",
      "Epoch: 100, Batch: 481/842, Loss: 0.2890, Time: 0.78s\n",
      "Epoch: 100, Batch: 491/842, Loss: 0.4790, Time: 0.78s\n",
      "Epoch: 100, Batch: 501/842, Loss: 0.3361, Time: 0.82s\n",
      "Epoch: 100, Batch: 511/842, Loss: 0.4273, Time: 0.82s\n",
      "Epoch: 100, Batch: 521/842, Loss: 0.2529, Time: 0.78s\n",
      "Epoch: 100, Batch: 531/842, Loss: 0.4838, Time: 0.81s\n",
      "Epoch: 100, Batch: 541/842, Loss: 0.3591, Time: 0.80s\n",
      "Epoch: 100, Batch: 551/842, Loss: 0.5340, Time: 0.80s\n",
      "Epoch: 100, Batch: 561/842, Loss: 0.4959, Time: 0.82s\n",
      "Epoch: 100, Batch: 571/842, Loss: 0.6215, Time: 0.78s\n",
      "Epoch: 100, Batch: 581/842, Loss: 0.4772, Time: 0.79s\n",
      "Epoch: 100, Batch: 591/842, Loss: 0.5274, Time: 0.83s\n",
      "Epoch: 100, Batch: 601/842, Loss: 0.4996, Time: 0.80s\n",
      "Epoch: 100, Batch: 611/842, Loss: 0.4343, Time: 0.80s\n",
      "Epoch: 100, Batch: 621/842, Loss: 0.3792, Time: 0.81s\n",
      "Epoch: 100, Batch: 631/842, Loss: 0.4610, Time: 0.80s\n",
      "Epoch: 100, Batch: 641/842, Loss: 0.5217, Time: 0.78s\n",
      "Epoch: 100, Batch: 651/842, Loss: 0.4330, Time: 0.82s\n",
      "Epoch: 100, Batch: 661/842, Loss: 0.3774, Time: 0.84s\n",
      "Epoch: 100, Batch: 671/842, Loss: 0.4910, Time: 0.82s\n",
      "Epoch: 100, Batch: 681/842, Loss: 0.6156, Time: 0.80s\n",
      "Epoch: 100, Batch: 691/842, Loss: 0.5988, Time: 0.82s\n",
      "Epoch: 100, Batch: 701/842, Loss: 0.4656, Time: 0.77s\n",
      "Epoch: 100, Batch: 711/842, Loss: 0.5191, Time: 0.79s\n",
      "Epoch: 100, Batch: 721/842, Loss: 0.4310, Time: 0.80s\n",
      "Epoch: 100, Batch: 731/842, Loss: 0.5423, Time: 1.35s\n",
      "Epoch: 100, Batch: 741/842, Loss: 0.5445, Time: 0.79s\n",
      "Epoch: 100, Batch: 751/842, Loss: 0.4809, Time: 0.85s\n",
      "Epoch: 100, Batch: 761/842, Loss: 0.4419, Time: 0.81s\n",
      "Epoch: 100, Batch: 771/842, Loss: 0.4850, Time: 0.76s\n",
      "Epoch: 100, Batch: 781/842, Loss: 0.4143, Time: 0.80s\n",
      "Epoch: 100, Batch: 791/842, Loss: 0.4160, Time: 0.82s\n",
      "Epoch: 100, Batch: 801/842, Loss: 0.4066, Time: 0.81s\n",
      "Epoch: 100, Batch: 811/842, Loss: 0.7126, Time: 0.82s\n",
      "Epoch: 100, Batch: 821/842, Loss: 0.4862, Time: 0.79s\n",
      "Epoch: 100, Batch: 831/842, Loss: 0.4497, Time: 0.78s\n",
      "Epoch: 100, Batch: 841/842, Loss: 0.4265, Time: 0.78s\n",
      "Epoch 100/100: Train Loss: 0.4501, Val Loss: 0.4176, mIoU: 0.7002, F1: 0.8178, OA: 0.9660\n",
      "  Per-class metrics:\n",
      "    Class 0: IoU=0.7855, F1=0.8799, OA=0.8781, Precision=0.8379, Recall=0.9264\n",
      "    Class 1: IoU=0.7173, F1=0.8354, OA=0.9966, Precision=0.8616, Recall=0.8107\n",
      "    Class 2: IoU=0.9130, F1=0.9545, OA=0.9816, Precision=0.9568, Recall=0.9522\n",
      "    Class 3: IoU=0.4969, F1=0.6639, OA=0.9648, Precision=0.7909, Recall=0.5721\n",
      "    Class 4: IoU=0.7624, F1=0.8652, OA=0.9820, Precision=0.8841, Recall=0.8471\n",
      "    Class 5: IoU=0.6834, F1=0.8119, OA=0.9966, Precision=0.8013, Recall=0.8228\n",
      "    Class 6: IoU=0.5829, F1=0.7365, OA=0.9591, Precision=0.8331, Recall=0.6600\n",
      "    Class 7: IoU=0.6599, F1=0.7951, OA=0.9697, Precision=0.8646, Recall=0.7360\n",
      "Training complete! Best mIoU: 0.7012\n",
      "Models saved to ../models/single_7c_2023_06_30_subset[10,11,5,4,6]/deeplabv3plus_efb5\n",
      "Training curves saved to ../models/single_7c_2023_06_30_subset[10,11,5,4,6]/deeplabv3plus_efb5/training_curves.png\n",
      " View run single_7c_2023_06_30_subset[10,11,5,4,6]-deeplabv3plus-20251022-182455 at: https://mlflow.stelarea.com/#/experiments/2/runs/88faef243ae047c4b5167c602c5134c5\n",
      " View experiment at: https://mlflow.stelarea.com/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\") \n",
    "\n",
    "run_name = f\"{out_name}-{model_params['architecture']}-{timestamp}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # Log parameters (avoid logging large/None weights field)\n",
    "    params_to_log = {k: v for k, v in model_params.items() if k != \"encoder_weights\"}\n",
    "    params_to_log.update({\n",
    "        \"image_path\": image_path,\n",
    "        \"label_path\": label_path,\n",
    "        \"architecture_alias\": \"deeplabv3plus\",\n",
    "        \"dataloader_tile_size\": dataloader_params[\"tile_size\"],\n",
    "        \"dataloader_stride\": dataloader_params[\"stride\"],\n",
    "        \"dataloader_skip_empty_tiles\": dataloader_params[\"skip_empty_tiles\"],\n",
    "        \"dataloader_stats\": dataloader_params[\"stats\"],\n",
    "    })\n",
    "    mlflow.log_params(params_to_log)\n",
    "\n",
    "    try:\n",
    "        import mlflow.pytorch\n",
    "        mlflow.pytorch.autolog(log_models=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Train the segmentation model\n",
    "    geoai.train_segmentation_model(\n",
    "        images_dir=model_params[\"images_dir\"],\n",
    "        labels_dir=model_params[\"labels_dir\"],\n",
    "        output_dir=model_params[\"output_dir\"],\n",
    "        architecture=model_params[\"architecture\"],\n",
    "        encoder_name=model_params[\"encoder_name\"],\n",
    "        encoder_weights=model_params[\"encoder_weights\"],\n",
    "        num_channels=model_params[\"num_channels\"],\n",
    "        num_classes=model_params[\"num_classes\"],\n",
    "        batch_size=model_params[\"batch_size\"],\n",
    "        num_epochs=model_params[\"num_epochs\"],\n",
    "        learning_rate=model_params[\"learning_rate\"],\n",
    "        val_split=model_params[\"val_split\"],\n",
    "        # criterion=criterion,\n",
    "        verbose=True,\n",
    "        class_balanced=model_params[\"class_balanced\"],\n",
    "        save_best_only=False,\n",
    "        checkpoint_interval=model_params[\"checkpoint_interval\"],\n",
    "        plot_curves=True\n",
    "    )\n",
    "\n",
    "    # Find the training output folder and history file robustly\n",
    "    history_path = None\n",
    "    output_dirs = set()\n",
    "    # 1) user-specified output_dir\n",
    "    output_dirs.add(Path(model_params[\"output_dir\"]))\n",
    "    # 2) any folder under model_folder that contains training_history.pth\n",
    "    for p in Path(model_folder).rglob(\"training_history.pth\"):\n",
    "        history_path = str(p)\n",
    "        output_dirs.add(p.parent)\n",
    "        break\n",
    "\n",
    "    # Log metrics from training_history.pth if present\n",
    "    if history_path and Path(history_path).exists():\n",
    "        try:\n",
    "            history = torch.load(history_path, map_location=\"cpu\")\n",
    "            keys = [\n",
    "                'train_losses', 'val_losses', 'val_ious', 'val_f1s',\n",
    "            ]\n",
    "            max_len = max(len(history.get(k, [])) for k in keys if isinstance(history.get(k, []), (list, tuple)) ) if history else 0\n",
    "            for epoch in range(max_len):\n",
    "                metrics = {}\n",
    "                for k in keys:\n",
    "                    seq = history.get(k, [])\n",
    "                    if isinstance(seq, (list, tuple)) and epoch < len(seq):\n",
    "                        try:\n",
    "                            metrics[k] = float(seq[epoch])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                if metrics:\n",
    "                    mlflow.log_metrics(metrics, step=epoch + 1)\n",
    "        except Exception as e:\n",
    "            print(\"Could not parse training history for MLflow metrics:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804befb",
   "metadata": {},
   "source": [
    "#### Plot model perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "112d75ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best IoU: 0.2537\n",
      "Final IoU: 0.2537\n",
      "Best Dice: 0.3000\n",
      "Final Dice: 0.3000\n"
     ]
    }
   ],
   "source": [
    "geoai.plot_performance_metrics(\n",
    "    history_path=f\"{model_params['output_dir']}/training_history.pth\",\n",
    "    figsize=(15, 5),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a13dcd",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932198f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file format: GeoTIFF (.tif)\n",
      "Processing 1271 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1344it [00:24, 55.31it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: 6 classes, Background: 51.4%\n",
      "Inference completed in 31.30 seconds\n",
      "Saved prediction to ../data/predicted/single_7c_2023_06_30/S2H_2024_2024_07_29_pred.tif\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "model_path = f\"{model_params['output_dir']}/best_model.pth\"\n",
    "test_raster_path = \"../data/raw/images/S2H_2024_2024_07_29_nodata.tif\"\n",
    "predicted_path = f\"../data/predicted/{out_name}/S2H_2024_2024_07_29_pred.tif\"\n",
    "\n",
    "# Run semantic segmentation inference\n",
    "geoai.semantic_segmentation(\n",
    "    input_path=test_raster_path,\n",
    "    output_path=predicted_path,\n",
    "    model_path=model_path,\n",
    "    architecture=model_params[\"architecture\"],\n",
    "    encoder_name=model_params[\"encoder_name\"],\n",
    "    num_channels=model_params[\"num_channels\"],\n",
    "    num_classes=model_params[\"num_classes\"],\n",
    "    window_size=dataloader_params[\"tile_size\"],\n",
    "    overlap=dataloader_params[\"stride\"],\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b072f87",
   "metadata": {},
   "source": [
    "#### Remap class pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique compact values in prediction: [0 1 2 3 4 5 6]\n",
      "Saved restored raster to ../data/predicted/single_6c_2023_07_30/S2H_2024_2024_07_29_pred_remap.tif\n",
      "Unique values: [ 0  1  3 24 54 75 76]\n"
     ]
    }
   ],
   "source": [
    "from geoai import label_utils\n",
    "\n",
    "# Map them into compact range\n",
    "mapping = {\n",
    "    0: 0,   # Background\n",
    "    1: 1,   # Corn\n",
    "    3: 2,   # Rice\n",
    "    24: 3,  # Winter Wheat\n",
    "    54: 4,  # Tomatoes\n",
    "    69: 5,  # Grapes\n",
    "    75: 5,  # Almonds\n",
    "    76: 6,  # Walnuts\n",
    "}\n",
    "\n",
    "# Remap back to original class codes\n",
    "label_utils.remap_back_raster(\n",
    "    in_path=predicted_path,\n",
    "    out_path=predicted_path.replace(\".tif\", \"_remap.tif\"),\n",
    "    mapping=mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1001d2",
   "metadata": {},
   "source": [
    "#### Visualize pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25acfc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24e3260f02940a29b6c51c889654adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[38.929975, -121.751141], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.constants import USDA_CDL_COLORS\n",
    "\n",
    "geoai.view_raster_with_labels(\n",
    "    raster_image=test_raster_path,\n",
    "    label_raster=predicted_path,\n",
    "    class_mapping={\n",
    "        0: \"Background\",\n",
    "        1: \"Corn\",\n",
    "        3: \"Rice\",\n",
    "        24: \"Winter Wheat\",\n",
    "        54: \"Tomatoes\",\n",
    "        69: \"Grapes\",\n",
    "        75: \"Almonds\",\n",
    "        76: \"Walnuts\",\n",
    "    },\n",
    "    class_colors={\n",
    "        0: USDA_CDL_COLORS.get(0),\n",
    "        1: USDA_CDL_COLORS.get(1),\n",
    "        3: USDA_CDL_COLORS.get(3),\n",
    "        24: USDA_CDL_COLORS.get(24),\n",
    "        54: USDA_CDL_COLORS.get(54),\n",
    "        69: USDA_CDL_COLORS.get(69),\n",
    "        75: USDA_CDL_COLORS.get(75),\n",
    "        76: USDA_CDL_COLORS.get(76),\n",
    "    },\n",
    "    raster_indexes=(3, 2, 1),\n",
    "    raster_layer_name=\"Satellite\",\n",
    "    label_layer_name=\"Labels\",\n",
    "    legend_title=\"Crop Classes\",\n",
    "    opacity=0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
