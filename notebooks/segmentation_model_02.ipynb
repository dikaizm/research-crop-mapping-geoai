{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ../../geoai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84684855",
   "metadata": {},
   "source": [
    "### Reload Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08c623d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'geoai.utils' from '/Users/dikaizm/Documents/PROGRAMMING/machine-learning/research-crop-mapping-thesis/geoai/geoai/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geoai\n",
    "import geoai.label_utils\n",
    "import geoai.utils\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import importlib\n",
    "importlib.reload(geoai)\n",
    "importlib.reload(geoai.label_utils)\n",
    "importlib.reload(geoai.utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e252c",
   "metadata": {},
   "source": [
    "## Train single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51114b2",
   "metadata": {},
   "source": [
    "### Build dataset [stride 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "066c1e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected in_class_data as raster: ../data/raw/cdl/2023_30m_cdls_10m_remap.tif\n",
      "Raster CRS: EPSG:4326\n",
      "Raster dimensions: 7217 x 5495\n",
      "\n",
      "Raster info for ../data/raw/images/S2H_2023_2023_07_30_nodata.tif:\n",
      "  CRS: EPSG:4326\n",
      "  Dimensions: 7217 x 5495\n",
      "  Resolution: (8.983152841195215e-05, 8.983152841195215e-05)\n",
      "  Bands: 9\n",
      "  Bounds: BoundingBox(left=-122.0752978940246, bottom=38.68316293322643, right=-121.42698375347554, top=39.1767871818501)\n",
      "Found 10 unique classes in raster: [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generated: 9112, With features: 9112: 100%|██████████| 9130/9130 [04:10<00:00, 36.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- Export Summary -------\n",
      "Total tiles exported: 9112\n",
      "Tiles with features: 9112 (100.0%)\n",
      "Average feature pixels per tile: 77361.1\n",
      "Output saved to: ../data/processed/crop_mapping_2023_07_30\n",
      "\n",
      "------- Georeference Verification -------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_path = \"../data/raw/images/S2H_2023_2023_07_30_nodata.tif\"\n",
    "label_path = \"../data/raw/cdl/2023_30m_cdls_10m_remap.tif\"\n",
    "\n",
    "out_name = \"crop_mapping_2023_07_30\"\n",
    "data_folder = f\"../data/processed/{out_name}\"\n",
    "\n",
    "model_folder = f\"../models/{out_name}\"\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# this function will create folders (annotations, images, labels)\n",
    "tiles = geoai.export_geotiff_tiles(\n",
    "    in_raster=image_path,\n",
    "    out_folder=data_folder,\n",
    "    in_class_data=label_path,\n",
    "    tile_size=256,\n",
    "    stride=64,\n",
    "    buffer_radius=0,\n",
    "    skip_empty_tiles=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76dade",
   "metadata": {},
   "source": [
    "### Train DeepLabV3+ [oversampling]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32560d6",
   "metadata": {},
   "source": [
    "#### Setup loss criterios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7964bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from geoai import losses\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "criterion = losses.DECBLoss(\n",
    "    num_classes=11,\n",
    "    beta=0.9999,          # smoothing factor (as in paper)\n",
    "    loss_type=\"ce\",       # or \"focal\"\n",
    "    gamma=2.0,            # focal gamma\n",
    "    ignore_index=255,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f778e71",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85787be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Found 9112 image files and 9112 label files\n",
      "Training on 7289 images, validating on 1823 images\n",
      "Checking image sizes for compatibility...\n",
      "All sampled images have the same size: (256, 256)\n",
      "No resizing needed.\n",
      "Building class-balanced sampler (strategy='presence') ...\n",
      "Class-balanced sampler ready. Non-empty classes: 11/11.\n",
      "Using class balanced sampler\n",
      "Testing data loader...\n",
      "Data loader test passed.\n",
      "Starting training with segformer + resnet152\n",
      "Model parameters: 59,494,347\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgeoai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_segmentation_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/images\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/labels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/segformer_models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msegformer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresnet152\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# background and 10 crop classes\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# criterion=criterion,\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_balanced\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/machine-learning/research-crop-mapping-thesis/geoai/geoai/train.py:2649\u001b[39m, in \u001b[36mtrain_segmentation_model\u001b[39m\u001b[34m(images_dir, labels_dir, output_dir, architecture, encoder_name, encoder_weights, num_channels, num_classes, batch_size, num_epochs, learning_rate, weight_decay, seed, val_split, print_freq, verbose, save_best_only, plot_curves, device, checkpoint_path, resume_training, target_size, resize_mode, num_workers, criterion, class_balanced, balance_strategy, ignore_index, sampler_num_samples, checkpoint_interval, **kwargs)\u001b[39m\n\u001b[32m   2646\u001b[39m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, num_epochs):\n\u001b[32m   2648\u001b[39m     \u001b[38;5;66;03m# Train one epoch\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2649\u001b[39m     train_loss = \u001b[43mtrain_semantic_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2657\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2659\u001b[39m     train_losses.append(train_loss)\n\u001b[32m   2661\u001b[39m     \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/machine-learning/research-crop-mapping-thesis/geoai/geoai/train.py:2055\u001b[39m, in \u001b[36mtrain_semantic_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, device, epoch, criterion, print_freq, verbose)\u001b[39m\n\u001b[32m   2053\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m   2054\u001b[39m optimizer.zero_grad()\n\u001b[32m-> \u001b[39m\u001b[32m2055\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2056\u001b[39m optimizer.step()\n\u001b[32m   2058\u001b[39m \u001b[38;5;66;03m# Track loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/machine-learning/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/machine-learning/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PROGRAMMING/machine-learning/research-crop-mapping-thesis/research-crop-mapping-geoai/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "geoai.train_segmentation_model(\n",
    "    images_dir=f\"{data_folder}/images\",\n",
    "    labels_dir=f\"{data_folder}/labels\",\n",
    "    output_dir=f\"{model_folder}/segformer_models\",\n",
    "    architecture=\"segformer\",\n",
    "    encoder_name=\"resnet152\",\n",
    "    encoder_weights=None,\n",
    "    num_channels=9,\n",
    "    num_classes=11, # background and 10 crop classes\n",
    "    batch_size=8,\n",
    "    num_epochs=10,\n",
    "    learning_rate=0.0005,\n",
    "    val_split=0.2,\n",
    "    # criterion=criterion,\n",
    "    verbose=True,\n",
    "    class_balanced=True,\n",
    "    save_best_only=False,\n",
    "    checkpoint_interval=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804befb",
   "metadata": {},
   "source": [
    "#### Plot model perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoai.plot_performance_metrics(\n",
    "    history_path=f\"{model_folder}/segformer_models/training_history.pth\",\n",
    "    figsize=(15, 5),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a13dcd",
   "metadata": {},
   "source": [
    "#### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932198f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file format: GeoTIFF (.tif)\n",
      "Processing 1102 windows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1170it [00:16, 70.30it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: 11 classes, Background: 47.4%\n",
      "Inference completed in 26.37 seconds\n",
      "Saved prediction to ../data/predicted/crop_mapping_2023_07_30/S2H_2024_2024_07_29_pred.tif\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "model_path = f\"{model_folder}/segformer_models/best_model.pth\"\n",
    "test_raster_path = \"../data/raw/images/S2H_2024_2024_07_29_nodata.tif\"\n",
    "predicted_path = f\"../data/predicted/{out_name}/S2H_2024_2024_07_29_pred.tif\"\n",
    "\n",
    "# Run semantic segmentation inference\n",
    "geoai.semantic_segmentation(\n",
    "    input_path=test_raster_path,\n",
    "    output_path=predicted_path,\n",
    "    model_path=model_path,\n",
    "    architecture=\"deeplabv3plus\",\n",
    "    encoder_name=\"efficientnet-b4\",\n",
    "    num_channels=9,\n",
    "    num_classes=11,\n",
    "    window_size=256,\n",
    "    overlap=64,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b072f87",
   "metadata": {},
   "source": [
    "#### Remap class pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191b779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique compact values in prediction: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Saved restored raster to ../data/predicted/crop_mapping_2023_07_30/S2H_2024_2024_07_29_pred_remap.tif\n",
      "Unique values: [  0   1   3  24  36  54  69  75  76 210 211]\n"
     ]
    }
   ],
   "source": [
    "from geoai import label_utils\n",
    "\n",
    "# Map them into compact range\n",
    "mapping = {\n",
    "    0: 0,   # Background\n",
    "    1: 1,   # Corn\n",
    "    3: 2,   # Rice\n",
    "    24: 3,  # Winter Wheat\n",
    "    36: 4,  # Alfalfa\n",
    "    54: 5,  # Tomatoes\n",
    "    69: 6,  # Grapes\n",
    "    75: 7,  # Almonds\n",
    "    76: 8,  # Walnuts\n",
    "    210: 9, # Prunes\n",
    "    211: 10 # Olives\n",
    "}\n",
    "\n",
    "# Remap back to original class codes\n",
    "label_utils.remap_back_raster(\n",
    "    in_path=predicted_path,\n",
    "    out_path=predicted_path.replace(\".tif\", \"_remap.tif\"),\n",
    "    mapping=mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1001d2",
   "metadata": {},
   "source": [
    "#### Visualize pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25acfc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a12686ff2bb41e2bc546e1cd351de7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[38.929975, -121.751141], controls=(ZoomControl(options=['position', 'zoom_in_text', 'zoom_in_title…"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.constants import USDA_CDL_COLORS\n",
    "\n",
    "geoai.view_raster_with_labels(\n",
    "    raster_image=test_raster_path,\n",
    "    label_raster=predicted_path,\n",
    "    class_mapping={\n",
    "        0: \"Background\",\n",
    "        1: \"Corn\",\n",
    "        3: \"Rice\",\n",
    "        24: \"Winter Wheat\",\n",
    "        36: \"Alfalfa\",\n",
    "        54: \"Tomatoes\",\n",
    "        69: \"Grapes\",\n",
    "        75: \"Almonds\",\n",
    "        76: \"Walnuts\",\n",
    "        210: \"Prunes\",\n",
    "        211: \"Olives\"\n",
    "    },\n",
    "    class_colors={\n",
    "        0: USDA_CDL_COLORS.get(0),\n",
    "        1: USDA_CDL_COLORS.get(1),\n",
    "        3: USDA_CDL_COLORS.get(3),\n",
    "        24: USDA_CDL_COLORS.get(24),\n",
    "        36: USDA_CDL_COLORS.get(36),\n",
    "        54: USDA_CDL_COLORS.get(54),\n",
    "        69: USDA_CDL_COLORS.get(69),\n",
    "        75: USDA_CDL_COLORS.get(75),\n",
    "        76: USDA_CDL_COLORS.get(76),\n",
    "        210: USDA_CDL_COLORS.get(210),\n",
    "        211: USDA_CDL_COLORS.get(211)\n",
    "    },\n",
    "    raster_indexes=(3, 2, 1),\n",
    "    raster_layer_name=\"Satellite\",\n",
    "    label_layer_name=\"Labels\",\n",
    "    legend_title=\"Crop Classes\",\n",
    "    opacity=0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
