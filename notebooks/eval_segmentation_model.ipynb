{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfd38cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79bcb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Dataset for semantic segmentation from GeoTIFF, PNG, JPG, and other image formats.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        label_paths: List[str],\n",
    "        transforms: Optional[Callable] = None,\n",
    "        num_channels: Optional[int] = None,\n",
    "        target_size: Optional[Tuple[int, int]] = None,\n",
    "        resize_mode: str = \"resize\",\n",
    "        num_classes: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize dataset for semantic segmentation.\n",
    "\n",
    "        Args:\n",
    "            image_paths (list): List of paths to image files (GeoTIFF, PNG, JPG, etc.).\n",
    "            label_paths (list): List of paths to label files (GeoTIFF, PNG, JPG, etc.).\n",
    "            transforms (callable, optional): Transformations to apply to images and masks.\n",
    "            num_channels (int, optional): Number of channels to use from images. If None,\n",
    "                auto-detected from the first image.\n",
    "            target_size (tuple, optional): Target size (height, width) for standardizing images.\n",
    "                If None, images will keep their original sizes.\n",
    "            resize_mode (str): How to handle size standardization. Options:\n",
    "                'resize' - Resize images to target_size (may change aspect ratio)\n",
    "                'pad' - Pad images to target_size (preserves aspect ratio)\n",
    "            num_classes (int): Number of classes for segmentation. Used for mask normalization.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transforms = transforms\n",
    "        self.target_size = target_size\n",
    "        self.resize_mode = resize_mode\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Auto-detect the number of channels if not specified\n",
    "        if num_channels is None:\n",
    "            self.num_channels = self._get_num_channels(self.image_paths[0])\n",
    "        else:\n",
    "            self.num_channels = num_channels\n",
    "\n",
    "    def _is_geotiff(self, file_path: str) -> bool:\n",
    "        \"\"\"Check if file is a GeoTIFF based on extension.\"\"\"\n",
    "        return file_path.lower().endswith((\".tif\", \".tiff\"))\n",
    "\n",
    "    def _get_num_channels(self, image_path: str) -> int:\n",
    "        \"\"\"Get number of channels from an image file.\"\"\"\n",
    "        if self._is_geotiff(image_path):\n",
    "            with rasterio.open(image_path) as src:\n",
    "                return src.count\n",
    "        else:\n",
    "            # For standard image formats, use PIL\n",
    "            with Image.open(image_path) as img:\n",
    "                if img.mode == \"RGB\":\n",
    "                    return 3\n",
    "                elif img.mode == \"RGBA\":\n",
    "                    return 4\n",
    "                elif img.mode == \"L\":\n",
    "                    return 1\n",
    "                else:\n",
    "                    # Convert to RGB and return 3 channels\n",
    "                    return 3\n",
    "\n",
    "    def _resize_image_and_mask(\n",
    "        self, image: np.ndarray, mask: np.ndarray\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Resize image and mask to target size.\"\"\"\n",
    "        if self.target_size is None:\n",
    "            return image, mask\n",
    "\n",
    "        target_h, target_w = self.target_size\n",
    "\n",
    "        if self.resize_mode == \"resize\":\n",
    "            # Direct resize (may change aspect ratio)\n",
    "            image = F.interpolate(\n",
    "                image.unsqueeze(0),\n",
    "                size=(target_h, target_w),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            ).squeeze(0)\n",
    "\n",
    "            mask = (\n",
    "                F.interpolate(\n",
    "                    mask.unsqueeze(0).unsqueeze(0).float(),\n",
    "                    size=(target_h, target_w),\n",
    "                    mode=\"nearest\",\n",
    "                )\n",
    "                .squeeze(0)\n",
    "                .squeeze(0)\n",
    "                .long()\n",
    "            )\n",
    "            # Clamp mask values to ensure they're within valid range [0, num_classes-1]\n",
    "            mask = torch.clamp(mask, 0, self.num_classes - 1)\n",
    "\n",
    "        elif self.resize_mode == \"pad\":\n",
    "            # Pad to target size (preserves aspect ratio)\n",
    "            image = self._pad_to_size(image, (target_h, target_w))\n",
    "            mask = self._pad_to_size(mask.unsqueeze(0), (target_h, target_w)).squeeze(0)\n",
    "            # Clamp mask values to ensure they're within valid range [0, num_classes-1]\n",
    "            mask = torch.clamp(mask, 0, self.num_classes - 1)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def _pad_to_size(\n",
    "        self, tensor: torch.Tensor, target_size: Tuple[int, int]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Pad tensor to target size with zeros.\"\"\"\n",
    "        target_h, target_w = target_size\n",
    "\n",
    "        if tensor.dim() == 3:  # Image [C, H, W]\n",
    "            _, h, w = tensor.shape\n",
    "        elif tensor.dim() == 2:  # Mask [H, W]\n",
    "            h, w = tensor.shape\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected tensor dimensions: {tensor.shape}\")\n",
    "\n",
    "        # Calculate padding\n",
    "        pad_h = max(0, target_h - h)\n",
    "        pad_w = max(0, target_w - w)\n",
    "\n",
    "        # Pad equally on both sides\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        # Apply padding (left, right, top, bottom)\n",
    "        padded = F.pad(tensor, (pad_left, pad_right, pad_top, pad_bottom), value=0)\n",
    "\n",
    "        # Crop if tensor is larger than target\n",
    "        if tensor.dim() == 3:\n",
    "            padded = padded[:, :target_h, :target_w]\n",
    "        else:\n",
    "            padded = padded[:target_h, :target_w]\n",
    "\n",
    "        return padded\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Load image\n",
    "        image_path = self.image_paths[idx]\n",
    "        if self._is_geotiff(image_path):\n",
    "            # Load GeoTIFF using rasterio\n",
    "            with rasterio.open(image_path) as src:\n",
    "                # Read as [C, H, W] format\n",
    "                image = src.read().astype(np.float32)\n",
    "                # Normalize image to [0, 1] range\n",
    "                image = image / 255.0\n",
    "        else:\n",
    "            # Load standard image formats using PIL\n",
    "            with Image.open(image_path) as img:\n",
    "                # Convert to RGB if needed\n",
    "                if img.mode != \"RGB\":\n",
    "                    img = img.convert(\"RGB\")\n",
    "                # Convert to numpy array [H, W, C]\n",
    "                image = np.array(img, dtype=np.float32)\n",
    "                # Normalize to [0, 1] range\n",
    "                image = image / 255.0\n",
    "                # Convert to [C, H, W] format\n",
    "                image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # Handle different number of channels\n",
    "        if image.shape[0] > self.num_channels:\n",
    "            image = image[: self.num_channels]  # Keep only specified bands\n",
    "        elif image.shape[0] < self.num_channels:\n",
    "            # Pad with zeros if less than specified bands\n",
    "            padded = np.zeros(\n",
    "                (self.num_channels, image.shape[1], image.shape[2]),\n",
    "                dtype=np.float32,\n",
    "            )\n",
    "            padded[: image.shape[0]] = image\n",
    "            image = padded\n",
    "\n",
    "        # Convert to CHW tensor\n",
    "        image = torch.as_tensor(image, dtype=torch.float32)\n",
    "\n",
    "        # Load label mask\n",
    "        label_path = self.label_paths[idx]\n",
    "        if self._is_geotiff(label_path):\n",
    "            # Load GeoTIFF label using rasterio\n",
    "            with rasterio.open(label_path) as src:\n",
    "                label_mask = src.read(1).astype(np.int64)\n",
    "        else:\n",
    "            # Load standard image format label using PIL\n",
    "            with Image.open(label_path) as img:\n",
    "                # Convert to grayscale if needed\n",
    "                if img.mode != \"L\":\n",
    "                    img = img.convert(\"L\")\n",
    "                label_mask = np.array(img, dtype=np.int64)\n",
    "\n",
    "        # Normalize mask values to expected class range [0, num_classes-1]\n",
    "        # This handles cases where masks contain pixel values outside the expected range\n",
    "        unique_vals = np.unique(label_mask)\n",
    "        if len(unique_vals) > 2:\n",
    "            # For multi-class case, we need to map values to proper class indices\n",
    "            # For now, we'll use a simple thresholding approach for binary segmentation\n",
    "            if self.num_classes == 2:\n",
    "                # Binary segmentation: convert to 0 (background) and 1 (foreground)\n",
    "                label_mask = (label_mask > 0).astype(np.int64)\n",
    "            else:\n",
    "                # For multi-class, we could implement more sophisticated mapping\n",
    "                # For now, just ensure values are in valid range\n",
    "                label_mask = np.clip(label_mask, 0, self.num_classes - 1)\n",
    "        elif len(unique_vals) == 2 and unique_vals.max() > 1:\n",
    "            # Binary mask with values not in [0,1] range - normalize to [0,1]\n",
    "            label_mask = (label_mask > 0).astype(np.int64)\n",
    "\n",
    "        # Convert to tensor\n",
    "        mask = torch.as_tensor(label_mask, dtype=torch.long)\n",
    "\n",
    "        # Resize image and mask to target size if specified\n",
    "        image, mask = self._resize_image_and_mask(image, mask)\n",
    "\n",
    "        # Apply transforms if specified\n",
    "        if self.transforms is not None:\n",
    "            image, mask = self.transforms(image, mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f27eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from typing import Callable, List, Optional, Tuple\n",
    "\n",
    "class SemanticTransforms:\n",
    "    \"\"\"Custom transforms for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, transforms: List[Callable]) -> None:\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(\n",
    "        self, image: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        for t in self.transforms:\n",
    "            image, mask = t(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class SemanticToTensor:\n",
    "    \"\"\"Convert numpy.ndarray to tensor for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self, image: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "class SemanticRandomHorizontalFlip:\n",
    "    \"\"\"Random horizontal flip transform for semantic segmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, prob: float = 0.5) -> None:\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(\n",
    "        self, image: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if random.random() < self.prob:\n",
    "            # Flip image and mask along width dimension\n",
    "            image = torch.flip(image, dims=[2])\n",
    "            mask = torch.flip(mask, dims=[1])\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be6c8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def get_semantic_transform(train: bool) -> Any:\n",
    "    \"\"\"\n",
    "    Get transforms for semantic segmentation data augmentation.\n",
    "\n",
    "    Args:\n",
    "        train (bool): Whether to include training-specific transforms.\n",
    "\n",
    "    Returns:\n",
    "        SemanticTransforms: Composed transforms.\n",
    "    \"\"\"\n",
    "    transforms = []\n",
    "    transforms.append(SemanticToTensor())\n",
    "\n",
    "    if train:\n",
    "        transforms.append(SemanticRandomHorizontalFlip(0.5))\n",
    "\n",
    "    return SemanticTransforms(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f6eb1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"../models/crop_mapping_subset_3\"\n",
    "images_dir = \"../data/processed/crop_mapping_subset_3/images\"\n",
    "labels_dir = \"../data/processed/crop_mapping_subset_3/labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "807feaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepLabV3Plus(\n",
       "  (encoder): EfficientNetEncoder(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      4, 40, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_expand_conv): Identity()\n",
       "        (_bn0): Identity()\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          40, 40, kernel_size=(3, 3), stride=[1, 1], groups=40, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          40, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Identity()\n",
       "        (_bn0): Identity()\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (3-4): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (6-7): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          288, 288, kernel_size=(5, 5), stride=(1, 1), groups=288, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          288, 288, kernel_size=(3, 3), stride=[2, 2], groups=288, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (9-12): 4 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          576, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          24, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          576, 576, kernel_size=(5, 5), stride=[1, 1], groups=576, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          576, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          24, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(136, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (14-17): 4 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          816, 816, kernel_size=(5, 5), stride=(1, 1), groups=816, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          816, 34, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          34, 816, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(136, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (18): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          136, 816, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=816, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          816, 34, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          34, 816, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          816, 232, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (19-23): 5 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          232, 1392, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(4, 4), dilation=(2, 2), groups=1392, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1392, 58, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          58, 1392, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1392, 232, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (24): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          232, 1392, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=1392, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1392, 58, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          58, 1392, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1392, 384, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (25): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          384, 2304, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          2304, 2304, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), groups=2304, bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          2304, 96, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          96, 2304, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          2304, 384, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1536, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.3, inplace=False)\n",
       "    (_swish): SiLU()\n",
       "  )\n",
       "  (decoder): DeepLabV3PlusDecoder(\n",
       "    (aspp): Sequential(\n",
       "      (0): ASPP(\n",
       "        (convs): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (1): ASPPSeparableConv(\n",
       "            (0): SeparableConv2d(\n",
       "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=384, bias=False)\n",
       "              (1): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (2): ASPPSeparableConv(\n",
       "            (0): SeparableConv2d(\n",
       "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), groups=384, bias=False)\n",
       "              (1): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (3): ASPPSeparableConv(\n",
       "            (0): SeparableConv2d(\n",
       "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), groups=384, bias=False)\n",
       "              (1): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): ReLU()\n",
       "          )\n",
       "          (4): ASPPPooling(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (3): ReLU()\n",
       "          )\n",
       "        )\n",
       "        (project): Sequential(\n",
       "          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SeparableConv2d(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (up): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(32, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): SeparableConv2d(\n",
       "        (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n",
       "        (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(256, 11, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model_path = f\"{model_folder}/deeplabv3plus_models/best_model.pth\"\n",
    "\n",
    "# Rebuild the model architecture the same way as training\n",
    "model = smp.create_model(\n",
    "    arch=\"deeplabv3plus\",\n",
    "    encoder_name=\"efficientnet-b3\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=4,\n",
    "    classes=11,\n",
    ")\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ed768d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get all image and label files\n",
    "# Support multiple image formats: GeoTIFF, PNG, JPG, JPEG, TIF, TIFF\n",
    "image_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n",
    "label_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n",
    "\n",
    "image_files = sorted(\n",
    "    [\n",
    "        os.path.join(images_dir, f)\n",
    "        for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith(image_extensions)\n",
    "    ]\n",
    ")\n",
    "label_files = sorted(\n",
    "    [\n",
    "        os.path.join(labels_dir, f)\n",
    "        for f in os.listdir(labels_dir)\n",
    "        if f.lower().endswith(label_extensions)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c43c766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n",
    "    image_files, label_files, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5d77d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from geoai import train\n",
    "\n",
    "val_dataset = train.SemanticSegmentationDataset(\n",
    "    val_imgs,             # from your earlier train/val split\n",
    "    val_labels,\n",
    "    transforms=train.get_semantic_transform(train=False),\n",
    "    num_channels=4,\n",
    "    target_size=(256, 256),  # must match training\n",
    "    resize_mode=\"resize\",\n",
    "    num_classes=11,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "769fba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Any, Dict\n",
    "\n",
    "def evaluate_semantic(\n",
    "    model: torch.nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    criterion: Any,\n",
    "    num_classes: int = 2,\n",
    "    ignore_index: int = 255,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the semantic segmentation model with per-class IoU and macro IoU.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        data_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
    "        device (torch.device): Device to evaluate on.\n",
    "        criterion: Loss function.\n",
    "        num_classes (int): Number of classes.\n",
    "        ignore_index (int): Value to ignore in evaluation (e.g., 255).\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics including loss, Dice, IoU, Macro IoU, Per-class IoU.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(data_loader)\n",
    "\n",
    "    # confusion matrix for IoU\n",
    "    confusion = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            for p, t in zip(preds, targets):\n",
    "                mask = t != ignore_index\n",
    "                p = p[mask]\n",
    "                t = t[mask]\n",
    "                cm = np.bincount(\n",
    "                    num_classes * t.astype(int) + p.astype(int),\n",
    "                    minlength=num_classes**2,\n",
    "                ).reshape(num_classes, num_classes)\n",
    "                confusion += cm\n",
    "\n",
    "    # per-class IoU\n",
    "    TP = np.diag(confusion)\n",
    "    FP = confusion.sum(axis=0) - TP\n",
    "    FN = confusion.sum(axis=1) - TP\n",
    "    per_class_iou = TP / (TP + FP + FN + 1e-6)\n",
    "\n",
    "    # metrics\n",
    "    macro_iou = np.nanmean(per_class_iou)\n",
    "    mean_loss = total_loss / num_batches\n",
    "    dice = 2 * TP.sum() / (2 * TP.sum() + FP.sum() + FN.sum())\n",
    "\n",
    "    return {\n",
    "        \"loss\": mean_loss,\n",
    "        \"Dice\": float(dice),\n",
    "        \"IoU\": float(per_class_iou.mean()),   # mean IoU across classes\n",
    "        \"MacroIoU\": float(macro_iou),\n",
    "        \"PerClassIoU\": per_class_iou.tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f2c6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_detail = evaluate_semantic(\n",
    "    model,\n",
    "    val_loader,\n",
    "    device,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    num_classes=11,\n",
    "    ignore_index=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "79865426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.603565348174748\n",
      "Global IoU: 0.4800350773148956\n",
      "Macro IoU: 0.4800350773148956\n",
      "Dice: 0.7388469457714606\n",
      "Per-class IoU:\n",
      "  Background: 0.0000\n",
      "  Corn: 0.4391\n",
      "  Rice: 0.8917\n",
      "  Winter Wheat: 0.4047\n",
      "  Alfalfa: 0.5111\n",
      "  Tomatoes: 0.5722\n",
      "  Grapes: 0.4012\n",
      "  Almonds: 0.5950\n",
      "  Walnuts: 0.6419\n",
      "  Prunes: 0.3566\n",
      "  Olives: 0.4668\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Loss:\", metrics_detail[\"loss\"])\n",
    "print(\"Global IoU:\", metrics_detail[\"IoU\"])\n",
    "print(\"Macro IoU:\", metrics_detail[\"MacroIoU\"])\n",
    "print(\"Dice:\", metrics_detail[\"Dice\"])\n",
    "print(\"Per-class IoU:\")\n",
    "\n",
    "class_mapping = {\n",
    "    0: \"Background\",\n",
    "    1: \"Corn\",\n",
    "    2: \"Rice\",\n",
    "    3: \"Winter Wheat\",\n",
    "    4: \"Alfalfa\",\n",
    "    5: \"Tomatoes\",\n",
    "    6: \"Grapes\",\n",
    "    7: \"Almonds\",\n",
    "    8: \"Walnuts\",\n",
    "    9: \"Prunes\",\n",
    "    10: \"Olives\"\n",
    "}\n",
    "for i, iou in enumerate(metrics_detail[\"PerClassIoU\"]):\n",
    "    print(f\"  {class_mapping.get(i, str(i))}: {iou:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
